Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00001-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00001-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00002-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00003-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00004-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00005-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00006-of-00007.safetensors
Loading model file LLaMA-Factory/models/llama2-7b-sft/model-00007-of-00007.safetensors
params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=4096, n_ff=11008, n_head=32, n_head_kv=32, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=10000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyQ8_0: 7>, path_model=PosixPath('LLaMA-Factory/models/llama2-7b-sft'))
Loaded vocab file PosixPath('LLaMA-Factory/models/llama2-7b-sft/tokenizer.model'), type 'spm'
Vocab info: <SentencePieceVocab with 32000 base tokens and 0 added tokens>
Special vocab info: <SpecialVocab with 0 merges, special tokens {'bos': 1, 'eos': 2}, add special tokens {'bos': True, 'eos': False}>
Permuting layer 0
Permuting layer 1
Permuting layer 2
Permuting layer 3
Permuting layer 4
Permuting layer 5
Permuting layer 6
Permuting layer 7
Permuting layer 8
Permuting layer 9
Permuting layer 10
Permuting layer 11
Permuting layer 12
Permuting layer 13
Permuting layer 14
Permuting layer 15
Permuting layer 16
Permuting layer 17
Permuting layer 18
Permuting layer 19
Permuting layer 20
Permuting layer 21
Permuting layer 22
Permuting layer 23
Permuting layer 24
Permuting layer 25
Permuting layer 26
Permuting layer 27
Permuting layer 28
Permuting layer 29
Permuting layer 30
Permuting layer 31
model.embed_tokens.weight                        -> token_embd.weight                        | F16    | [32000, 4096]
model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | F16    | [4096]
model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | F16    | [4096]
model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F16    | [4096, 4096]
model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F16    | [4096, 4096]
model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F16    | [4096, 4096]
model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | F16    | [4096]
model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | F16    | [4096]
model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F16    | [4096, 4096]
model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F16    | [4096, 4096]
model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F16    | [4096, 4096]
model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | F16    | [4096]
model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | F16    | [4096]
model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F16    | [4096, 4096]
model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F16    | [4096, 4096]
model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F16    | [4096, 4096]
model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | F16    | [4096]
model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | F16    | [4096]
model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F16    | [4096, 4096]
model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F16    | [4096, 4096]
model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F16    | [4096, 4096]
model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F16    | [4096, 4096]
model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | F16    | [4096]
model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | F16    | [4096]
model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F16    | [4096, 4096]
model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | F16    | [4096]
model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | F16    | [4096]
model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F16    | [4096, 4096]
model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F16    | [4096, 4096]
model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F16    | [4096, 4096]
model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | F16    | [4096]
model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | F16    | [4096]
model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F16    | [4096, 4096]
model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F16    | [4096, 4096]
model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F16    | [4096, 4096]
model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | F16    | [4096]
model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | F16    | [4096]
model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F16    | [4096, 4096]
model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F16    | [4096, 4096]
model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F16    | [4096, 4096]
model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | F16    | [4096]
model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | F16    | [4096]
model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F16    | [4096, 4096]
model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F16    | [4096, 4096]
model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F16    | [4096, 4096]
model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F16    | [4096, 4096]
model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | F16    | [4096]
model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | F16    | [4096]
model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F16    | [4096, 4096]
model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F16    | [4096, 4096]
model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F16    | [4096, 4096]
model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | F16    | [4096]
model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | F16    | [4096]
model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F16    | [4096, 4096]
model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F16    | [4096, 4096]
model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F16    | [4096, 4096]
model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | F16    | [4096]
model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | F16    | [4096]
model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F16    | [4096, 4096]
model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F16    | [4096, 4096]
model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F16    | [4096, 4096]
model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | F16    | [4096]
model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | F16    | [4096]
model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F16    | [4096, 4096]
model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F16    | [4096, 4096]
model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F16    | [4096, 4096]
model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | F16    | [4096]
model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F16    | [4096, 11008]
model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F16    | [11008, 4096]
model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F16    | [11008, 4096]
model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | F16    | [4096]
model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F16    | [4096, 4096]
model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F16    | [4096, 4096]
model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | F16    | [4096]
model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | F16    | [4096]
model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F16    | [4096, 4096]
model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F16    | [4096, 4096]
model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F16    | [4096, 4096]
model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | F16    | [4096]
model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | F16    | [4096]
model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F16    | [4096, 4096]
model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F16    | [4096, 4096]
model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F16    | [4096, 4096]
model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | F16    | [4096]
model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | F16    | [4096]
model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F16    | [4096, 4096]
model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F16    | [4096, 4096]
model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F16    | [4096, 4096]
model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | F16    | [4096]
model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | F16    | [4096]
model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F16    | [4096, 4096]
model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F16    | [4096, 4096]
model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F16    | [4096, 4096]
model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | F16    | [4096]
model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | F16    | [4096]
model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F16    | [4096, 4096]
model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F16    | [4096, 4096]
model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F16    | [4096, 4096]
model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | F16    | [4096]
model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | F16    | [4096]
model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F16    | [4096, 4096]
model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F16    | [4096, 4096]
model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F16    | [4096, 4096]
model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | F16    | [4096]
model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | F16    | [4096]
model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F16    | [4096, 4096]
model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F16    | [4096, 4096]
model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F16    | [4096, 4096]
model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | F16    | [4096]
model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | F16    | [4096]
model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F16    | [4096, 4096]
model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F16    | [4096, 4096]
model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F16    | [4096, 4096]
model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | F16    | [4096]
model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | F16    | [4096]
model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F16    | [4096, 4096]
model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F16    | [4096, 4096]
model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F16    | [4096, 4096]
model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F16    | [4096, 4096]
model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F16    | [4096, 4096]
model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | F16    | [4096]
model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | F16    | [4096]
model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | F16    | [4096]
model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | F16    | [4096]
model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F16    | [4096, 4096]
model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F16    | [4096, 4096]
model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F16    | [4096, 4096]
model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | F16    | [4096]
model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | F16    | [4096]
model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F16    | [4096, 4096]
model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F16    | [4096, 4096]
model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F16    | [4096, 4096]
model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | F16    | [4096]
model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | F16    | [4096]
model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F16    | [4096, 4096]
model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F16    | [4096, 4096]
model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F16    | [4096, 4096]
model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | F16    | [4096]
model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | F16    | [4096]
model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F16    | [4096, 4096]
model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F16    | [4096, 4096]
model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F16    | [4096, 4096]
model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F16    | [4096, 4096]
model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F16    | [4096, 4096]
model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F16    | [4096, 4096]
lm_head.weight                                   -> output.weight                            | F16    | [32000, 4096]
model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | F16    | [4096]
model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | F16    | [4096]
model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | F16    | [4096]
model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | F16    | [4096]
model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F16    | [4096, 4096]
model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F16    | [4096, 4096]
model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F16    | [4096, 4096]
model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | F16    | [4096]
model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | F16    | [4096]
model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F16    | [4096, 4096]
model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F16    | [4096, 4096]
model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F16    | [4096, 4096]
model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | F16    | [4096]
model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F16    | [4096, 11008]
model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F16    | [11008, 4096]
model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F16    | [11008, 4096]
model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | F16    | [4096]
model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F16    | [4096, 4096]
model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F16    | [4096, 4096]
model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F16    | [4096, 4096]
model.norm.weight                                -> output_norm.weight                       | F16    | [4096]
Writing LLaMA-Factory/models/llama2-7b-sft_q2.gguf, format 7
Ignoring added_tokens.json since model matches vocab size without it.
gguf: This GGUF file is for Little Endian only
gguf: Setting special token type bos to 1
gguf: Setting special token type eos to 2
gguf: Setting add_bos_token to True
gguf: Setting add_eos_token to False
gguf: Setting chat_template to {% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message + '\n' }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ 'Human: ' + content + '\nAssistant: ' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\n' }}{% endif %}{% endfor %}
[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type Q8_0 | T+   5
[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   5
[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+   5
[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+   6
[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+   6
[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   6
[  7/291] Writing tensor blk.0.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+   6
[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   6
[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   6
[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+   6
[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   6
[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+   7
[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+   8
[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+   8
[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   8
[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+   8
[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+   8
[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+   8
[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+   8
[ 20/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   8
[ 21/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+   9
[ 22/291] Writing tensor blk.2.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  10
[ 23/291] Writing tensor blk.2.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  10
[ 24/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  11
[ 25/291] Writing tensor blk.2.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  11
[ 26/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  11
[ 27/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  11
[ 28/291] Writing tensor blk.2.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  11
[ 29/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  11
[ 30/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  12
[ 31/291] Writing tensor blk.3.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  13
[ 32/291] Writing tensor blk.3.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  13
[ 33/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  13
[ 34/291] Writing tensor blk.3.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  13
[ 35/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  13
[ 36/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  13
[ 37/291] Writing tensor blk.3.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  13
[ 38/291] Writing tensor blk.4.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  13
[ 39/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  13
[ 40/291] Writing tensor blk.4.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  14
[ 41/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  14
[ 42/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  15
[ 43/291] Writing tensor blk.4.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  16
[ 44/291] Writing tensor blk.4.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  16
[ 45/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  16
[ 46/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  16
[ 47/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  16
[ 48/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  16
[ 49/291] Writing tensor blk.5.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  17
[ 50/291] Writing tensor blk.5.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  17
[ 51/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  17
[ 52/291] Writing tensor blk.5.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  17
[ 53/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  17
[ 54/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  17
[ 55/291] Writing tensor blk.5.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  17
[ 56/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  17
[ 57/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  18
[ 58/291] Writing tensor blk.6.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  19
[ 59/291] Writing tensor blk.6.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  20
[ 60/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  20
[ 61/291] Writing tensor blk.6.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  20
[ 62/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  20
[ 63/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  20
[ 64/291] Writing tensor blk.6.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  20
[ 65/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  20
[ 66/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  21
[ 67/291] Writing tensor blk.7.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  21
[ 68/291] Writing tensor blk.7.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  22
[ 69/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  22
[ 70/291] Writing tensor blk.7.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  22
[ 71/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  22
[ 72/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  22
[ 73/291] Writing tensor blk.7.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  22
[ 74/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  22
[ 75/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  23
[ 76/291] Writing tensor blk.8.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  24
[ 77/291] Writing tensor blk.8.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  24
[ 78/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  24
[ 79/291] Writing tensor blk.8.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  24
[ 80/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  25
[ 81/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25
[ 82/291] Writing tensor blk.8.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  25
[ 83/291] Writing tensor blk.9.attn_k.weight                    | size   4096 x   4096  | type Q8_0 | T+  25
[ 84/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type Q8_0 | T+  25
[ 85/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  25
[ 86/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  26
[ 87/291] Writing tensor blk.10.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  27
[ 88/291] Writing tensor blk.10.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  27
[ 89/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  27
[ 90/291] Writing tensor blk.10.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  27
[ 91/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  27
[ 92/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  27
[ 93/291] Writing tensor blk.10.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  27
[ 94/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  27
[ 95/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  28
[ 96/291] Writing tensor blk.11.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  29
[ 97/291] Writing tensor blk.11.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  29
[ 98/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  29
[ 99/291] Writing tensor blk.11.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  29
[100/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  29
[101/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  29
[102/291] Writing tensor blk.11.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  30
[103/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  30
[104/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  31
[105/291] Writing tensor blk.12.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  31
[106/291] Writing tensor blk.12.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  32
[107/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  32
[108/291] Writing tensor blk.12.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  32
[109/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  32
[110/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  32
[111/291] Writing tensor blk.12.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  32
[112/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  32
[113/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  33
[114/291] Writing tensor blk.13.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  34
[115/291] Writing tensor blk.13.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  34
[116/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  35
[117/291] Writing tensor blk.13.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  35
[118/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  35
[119/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  35
[120/291] Writing tensor blk.13.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  35
[121/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  35
[122/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  35
[123/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  11008  | type Q8_0 | T+  36
[124/291] Writing tensor blk.9.ffn_gate.weight                  | size  11008 x   4096  | type Q8_0 | T+  37
[125/291] Writing tensor blk.9.ffn_up.weight                    | size  11008 x   4096  | type Q8_0 | T+  37
[126/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  37
[127/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type Q8_0 | T+  37
[128/291] Writing tensor blk.9.attn_v.weight                    | size   4096 x   4096  | type Q8_0 | T+  37
[129/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  37
[130/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  38
[131/291] Writing tensor blk.14.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  38
[132/291] Writing tensor blk.14.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  39
[133/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  39
[134/291] Writing tensor blk.14.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  39
[135/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  39
[136/291] Writing tensor blk.14.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  39
[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  39
[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  40
[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  40
[140/291] Writing tensor blk.15.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  41
[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  41
[142/291] Writing tensor blk.15.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  41
[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  41
[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  41
[145/291] Writing tensor blk.15.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  41
[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  41
[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  42
[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  43
[149/291] Writing tensor blk.16.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  43
[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  43
[151/291] Writing tensor blk.16.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  43
[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  43
[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  43
[154/291] Writing tensor blk.16.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  43
[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  43
[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  45
[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  45
[158/291] Writing tensor blk.17.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  46
[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  46
[160/291] Writing tensor blk.17.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  46
[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  46
[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  46
[163/291] Writing tensor blk.17.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  46
[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  46
[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  47
[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  47
[167/291] Writing tensor blk.18.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  48
[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  48
[169/291] Writing tensor blk.18.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  48
[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  48
[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  48
[172/291] Writing tensor blk.18.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  48
[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  48
[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  49
[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  50
[176/291] Writing tensor blk.19.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  50
[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  50
[178/291] Writing tensor blk.19.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  50
[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  50
[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  51
[181/291] Writing tensor blk.19.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  51
[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  51
[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  51
[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  52
[185/291] Writing tensor blk.20.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  53
[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  53
[187/291] Writing tensor blk.20.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  53
[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  53
[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  53
[190/291] Writing tensor blk.20.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  53
[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  53
[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  54
[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  54
[194/291] Writing tensor blk.21.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  55
[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  55
[196/291] Writing tensor blk.21.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  55
[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  55
[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  55
[199/291] Writing tensor blk.21.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  55
[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  56
[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  56
[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  57
[203/291] Writing tensor blk.22.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  58
[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  58
[205/291] Writing tensor blk.22.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  58
[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  58
[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  58
[208/291] Writing tensor blk.22.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  58
[209/291] Writing tensor blk.23.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  59
[210/291] Writing tensor blk.23.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  59
[211/291] Writing tensor blk.23.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  59
[212/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  59
[213/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  59
[214/291] Writing tensor blk.23.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  59
[215/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  59
[216/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  60
[217/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  61
[218/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  61
[219/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  61
[220/291] Writing tensor blk.24.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  61
[221/291] Writing tensor blk.24.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  62
[222/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  62
[223/291] Writing tensor blk.24.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  62
[224/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  62
[225/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  62
[226/291] Writing tensor blk.24.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  62
[227/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  62
[228/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  63
[229/291] Writing tensor blk.25.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  64
[230/291] Writing tensor blk.25.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  64
[231/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  65
[232/291] Writing tensor blk.25.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  65
[233/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  65
[234/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  65
[235/291] Writing tensor blk.25.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  65
[236/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  65
[237/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  65
[238/291] Writing tensor blk.26.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  66
[239/291] Writing tensor blk.26.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  67
[240/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  67
[241/291] Writing tensor blk.26.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  67
[242/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  67
[243/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  67
[244/291] Writing tensor blk.26.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  67
[245/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  67
[246/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  68
[247/291] Writing tensor blk.27.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  69
[248/291] Writing tensor blk.27.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  69
[249/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  69
[250/291] Writing tensor blk.27.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  69
[251/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  69
[252/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  69
[253/291] Writing tensor blk.27.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  70
[254/291] Writing tensor blk.28.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  71
[255/291] Writing tensor blk.28.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  71
[256/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  71
[257/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  71
[258/291] Writing tensor blk.28.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  71
[259/291] Writing tensor output.weight                          | size  32000 x   4096  | type Q8_0 | T+  75
[260/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  75
[261/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  75
[262/291] Writing tensor blk.28.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  75
[263/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  76
[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  76
[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  76
[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  76
[267/291] Writing tensor blk.29.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  77
[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  77
[269/291] Writing tensor blk.29.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  77
[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  77
[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  77
[272/291] Writing tensor blk.29.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  77
[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  77
[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  78
[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  79
[276/291] Writing tensor blk.30.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  79
[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  79
[278/291] Writing tensor blk.30.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  79
[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  79
[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  79
[281/291] Writing tensor blk.30.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  79
[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  79
[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  11008  | type Q8_0 | T+  81
[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  11008 x   4096  | type Q8_0 | T+  81
[285/291] Writing tensor blk.31.ffn_up.weight                   | size  11008 x   4096  | type Q8_0 | T+  81
[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  81
[287/291] Writing tensor blk.31.attn_k.weight                   | size   4096 x   4096  | type Q8_0 | T+  81
[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type Q8_0 | T+  81
[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type Q8_0 | T+  81
[290/291] Writing tensor blk.31.attn_v.weight                   | size   4096 x   4096  | type Q8_0 | T+  81
[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  81
Wrote LLaMA-Factory/models/llama2-7b-sft_q2.gguf
