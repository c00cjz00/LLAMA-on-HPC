[2024-04-07 18:20:51,708] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 18:20:52,364] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-04-07 18:20:52,390] [INFO] [runner.py:568:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None ../../src/train_bash.py --deepspeed ../deepspeed/ds_z2_offload_config.json --stage sft --do_train --model_name_or_path meta-llama/Llama-2-7b-hf --dataset demo_sft --dataset_dir ../../data --template default --finetuning_type full --output_dir ../../saves/LLaMA2-7B/full/04_sft --overwrite_cache --overwrite_output_dir --cutoff_len 1024 --preprocessing_num_workers 16 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --gradient_accumulation_steps 2 --lr_scheduler_type cosine --logging_steps 10 --warmup_steps 20 --save_steps 100 --eval_steps 100 --evaluation_strategy steps --learning_rate 5e-5 --num_train_epochs 3.0 --max_samples 3000 --val_size 0.1 --ddp_timeout 180000000 --plot_loss --fp16
[2024-04-07 18:20:54,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.16.2-1+cuda11.8
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.16.2-1
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NCCL_VERSION=2.16.2-1
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.16.2-1+cuda11.8
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2
[2024-04-07 18:20:55,198] [INFO] [launch.py:138:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.16.2-1
[2024-04-07 18:20:55,198] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-04-07 18:20:55,198] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-04-07 18:20:55,198] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-04-07 18:20:55,198] [INFO] [launch.py:163:main] dist_world_size=4
[2024-04-07 18:20:55,198] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-04-07 18:20:55,199] [INFO] [launch.py:253:main] process 10148 spawned with command: ['/opt/conda/bin/python', '-u', '../../src/train_bash.py', '--local_rank=0', '--deepspeed', '../deepspeed/ds_z2_offload_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--dataset', 'demo_sft', '--dataset_dir', '../../data', '--template', 'default', '--finetuning_type', 'full', '--output_dir', '../../saves/LLaMA2-7B/full/04_sft', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--preprocessing_num_workers', '16', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--save_steps', '100', '--eval_steps', '100', '--evaluation_strategy', 'steps', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--max_samples', '3000', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
[2024-04-07 18:20:55,200] [INFO] [launch.py:253:main] process 10149 spawned with command: ['/opt/conda/bin/python', '-u', '../../src/train_bash.py', '--local_rank=1', '--deepspeed', '../deepspeed/ds_z2_offload_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--dataset', 'demo_sft', '--dataset_dir', '../../data', '--template', 'default', '--finetuning_type', 'full', '--output_dir', '../../saves/LLaMA2-7B/full/04_sft', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--preprocessing_num_workers', '16', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--save_steps', '100', '--eval_steps', '100', '--evaluation_strategy', 'steps', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--max_samples', '3000', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
[2024-04-07 18:20:55,201] [INFO] [launch.py:253:main] process 10150 spawned with command: ['/opt/conda/bin/python', '-u', '../../src/train_bash.py', '--local_rank=2', '--deepspeed', '../deepspeed/ds_z2_offload_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--dataset', 'demo_sft', '--dataset_dir', '../../data', '--template', 'default', '--finetuning_type', 'full', '--output_dir', '../../saves/LLaMA2-7B/full/04_sft', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--preprocessing_num_workers', '16', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--save_steps', '100', '--eval_steps', '100', '--evaluation_strategy', 'steps', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--max_samples', '3000', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
[2024-04-07 18:20:55,201] [INFO] [launch.py:253:main] process 10151 spawned with command: ['/opt/conda/bin/python', '-u', '../../src/train_bash.py', '--local_rank=3', '--deepspeed', '../deepspeed/ds_z2_offload_config.json', '--stage', 'sft', '--do_train', '--model_name_or_path', 'meta-llama/Llama-2-7b-hf', '--dataset', 'demo_sft', '--dataset_dir', '../../data', '--template', 'default', '--finetuning_type', 'full', '--output_dir', '../../saves/LLaMA2-7B/full/04_sft', '--overwrite_cache', '--overwrite_output_dir', '--cutoff_len', '1024', '--preprocessing_num_workers', '16', '--per_device_train_batch_size', '1', '--per_device_eval_batch_size', '1', '--gradient_accumulation_steps', '2', '--lr_scheduler_type', 'cosine', '--logging_steps', '10', '--warmup_steps', '20', '--save_steps', '100', '--eval_steps', '100', '--evaluation_strategy', 'steps', '--learning_rate', '5e-5', '--num_train_epochs', '3.0', '--max_samples', '3000', '--val_size', '0.1', '--ddp_timeout', '180000000', '--plot_loss', '--fp16']
[2024-04-07 18:20:59,674] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 18:20:59,774] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 18:20:59,849] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 18:20:59,851] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-04-07 18:21:01,732] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-07 18:21:01,732] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-04-07 18:21:01,733] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-07 18:21:01,749] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-04-07 18:21:01,755] [INFO] [comm.py:637:init_distributed] cdb=None
04/07/2024 18:21:01 - INFO - llmtuner.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
04/07/2024 18:21:01 - INFO - llmtuner.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
04/07/2024 18:21:01 - INFO - llmtuner.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
04/07/2024 18:21:01 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
04/07/2024 18:21:02 - INFO - llmtuner.data.template - Add pad token: </s>
04/07/2024 18:21:02 - INFO - llmtuner.data.template - Add pad token: </s>
04/07/2024 18:21:02 - INFO - llmtuner.data.loader - Loading dataset demo_sft.json...
04/07/2024 18:21:02 - INFO - llmtuner.data.template - Add pad token: </s>
04/07/2024 18:21:02 - INFO - llmtuner.data.template - Add pad token: </s>
04/07/2024 18:21:11 - INFO - llmtuner.data.loader - Loading dataset demo_sft.json...
04/07/2024 18:21:11 - INFO - llmtuner.data.loader - Loading dataset demo_sft.json...
04/07/2024 18:21:11 - INFO - llmtuner.data.loader - Loading dataset demo_sft.json...
input_ids:
[887, 526, 263, 8444, 319, 29902, 20255, 4240, 491, 405, 3210, 29907, 29889, 450, 1404, 366, 526, 19912, 7726, 29879, 18375, 3245, 10013, 322, 5304, 515, 27807, 29889, 13, 12968, 29901, 29871, 30446, 232, 136, 149, 235, 133, 168, 235, 134, 153, 31480, 30908, 235, 172, 181, 30847, 31502, 31032, 234, 156, 133, 13, 30647, 232, 178, 185, 232, 178, 185, 30214, 232, 140, 158, 29955, 233, 176, 181, 30214, 236, 131, 156, 30287, 30470, 30214, 232, 178, 162, 235, 169, 189, 30780, 30214, 30672, 30613, 232, 176, 172, 30319, 31687, 30429, 235, 133, 140, 232, 193, 139, 30923, 30214, 31325, 231, 187, 151, 30214, 31855, 31180, 31838, 31190, 30210, 30257, 30214, 30606, 30974, 30769, 30413, 31823, 233, 176, 164, 232, 147, 134, 31475, 234, 145, 172, 30214, 235, 174, 142, 232, 152, 146, 30383, 30446, 232, 136, 149, 235, 133, 168, 235, 134, 153, 31480, 30908, 235, 172, 181, 30847, 31502, 31032, 234, 156, 133, 30267, 13, 7900, 22137, 29901, 29871, 29871, 232, 176, 172, 30319, 30544, 31928, 235, 133, 168, 235, 134, 153, 234, 154, 138, 30210, 30993, 233, 182, 132, 30267, 30613, 30899, 30698, 236, 131, 146, 236, 132, 145, 232, 176, 172, 30319, 236, 132, 142, 31134, 30503, 31863, 31577, 30210, 236, 166, 181, 31855, 231, 193, 137, 234, 186, 172, 31201, 31221, 30210, 234, 154, 138, 234, 142, 131, 30214, 30682, 30651, 31244, 235, 177, 150, 31221, 232, 132, 157, 30287, 31959, 30417, 233, 179, 170, 236, 132, 142, 31124, 30214, 31419, 30847, 233, 136, 165, 235, 186, 148, 30214, 234, 139, 175, 232, 160, 164, 30214, 233, 187, 187, 233, 182, 182, 31184, 30214, 231, 187, 169, 231, 187, 151, 236, 166, 181, 31855, 30429, 232, 176, 172, 30319, 30923, 232, 147, 134, 31995, 234, 150, 159, 30214, 235, 134, 164, 235, 155, 194, 235, 151, 151, 30214, 235, 146, 163, 31854, 31184, 30214, 234, 169, 132, 31981, 232, 176, 172, 30319, 232, 147, 134, 30287, 31959, 233, 181, 188, 234, 133, 187, 31855, 31399, 30503, 231, 188, 193, 30801, 236, 164, 161, 31855, 30834, 30214, 236, 131, 156, 31959, 30769, 30392, 231, 188, 193, 234, 137, 180, 31180, 30528, 235, 135, 133, 235, 133, 173, 30210, 31855, 30834, 30214, 31325, 231, 187, 151, 30413, 30698, 235, 177, 150, 232, 176, 172, 30319, 234, 187, 192, 30392, 232, 147, 134, 31366, 31238, 235, 189, 189, 30505, 232, 189, 141, 30429, 30413, 31124, 30214, 30613, 30899, 30505, 31032, 234, 156, 133, 30446, 232, 136, 149, 235, 133, 168, 235, 134, 153, 31117, 31069, 30847, 30801, 232, 176, 172, 30319, 30993, 233, 182, 132, 232, 157, 183, 30908, 31238, 30698, 31436, 30974, 31475, 236, 137, 174, 30963, 30505, 236, 137, 174, 30486, 30210, 31084, 232, 179, 145, 30557, 234, 184, 169, 232, 176, 172, 30319, 31032, 234, 156, 133, 30267, 2]
inputs:
You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.
 Human: 小兒肥胖超重該如何治療
女寶寶，剛7歲，這一年，察覺到，我家孩子身上肉很多，而且，食量非常的大，平時都不喜歡吃去玩，請問：小兒肥胖超重該如何治療。
Assistant:  孩子出現肥胖症的情況。家長要透過孩子運功和健康的飲食來緩解他的症狀，可以先讓他做一些有氧運動，比如慢跑，爬坡，游泳等，並且飲食上孩子多吃黃瓜，胡蘿蔔，菠菜等，禁止孩子吃一些油炸食品和乾果類食物，這些都是乾熱量高脂肪的食物，而且不要讓孩子總是吃完就躺在床上不動，家長在治療小兒肥胖期間如果孩子情況嚴重就要及時去醫院在醫生的指導下給孩子治療。</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 232, 176, 172, 30319, 30544, 31928, 235, 133, 168, 235, 134, 153, 234, 154, 138, 30210, 30993, 233, 182, 132, 30267, 30613, 30899, 30698, 236, 131, 146, 236, 132, 145, 232, 176, 172, 30319, 236, 132, 142, 31134, 30503, 31863, 31577, 30210, 236, 166, 181, 31855, 231, 193, 137, 234, 186, 172, 31201, 31221, 30210, 234, 154, 138, 234, 142, 131, 30214, 30682, 30651, 31244, 235, 177, 150, 31221, 232, 132, 157, 30287, 31959, 30417, 233, 179, 170, 236, 132, 142, 31124, 30214, 31419, 30847, 233, 136, 165, 235, 186, 148, 30214, 234, 139, 175, 232, 160, 164, 30214, 233, 187, 187, 233, 182, 182, 31184, 30214, 231, 187, 169, 231, 187, 151, 236, 166, 181, 31855, 30429, 232, 176, 172, 30319, 30923, 232, 147, 134, 31995, 234, 150, 159, 30214, 235, 134, 164, 235, 155, 194, 235, 151, 151, 30214, 235, 146, 163, 31854, 31184, 30214, 234, 169, 132, 31981, 232, 176, 172, 30319, 232, 147, 134, 30287, 31959, 233, 181, 188, 234, 133, 187, 31855, 31399, 30503, 231, 188, 193, 30801, 236, 164, 161, 31855, 30834, 30214, 236, 131, 156, 31959, 30769, 30392, 231, 188, 193, 234, 137, 180, 31180, 30528, 235, 135, 133, 235, 133, 173, 30210, 31855, 30834, 30214, 31325, 231, 187, 151, 30413, 30698, 235, 177, 150, 232, 176, 172, 30319, 234, 187, 192, 30392, 232, 147, 134, 31366, 31238, 235, 189, 189, 30505, 232, 189, 141, 30429, 30413, 31124, 30214, 30613, 30899, 30505, 31032, 234, 156, 133, 30446, 232, 136, 149, 235, 133, 168, 235, 134, 153, 31117, 31069, 30847, 30801, 232, 176, 172, 30319, 30993, 233, 182, 132, 232, 157, 183, 30908, 31238, 30698, 31436, 30974, 31475, 236, 137, 174, 30963, 30505, 236, 137, 174, 30486, 30210, 31084, 232, 179, 145, 30557, 234, 184, 169, 232, 176, 172, 30319, 31032, 234, 156, 133, 30267, 2]
labels:
孩子出現肥胖症的情況。家長要透過孩子運功和健康的飲食來緩解他的症狀，可以先讓他做一些有氧運動，比如慢跑，爬坡，游泳等，並且飲食上孩子多吃黃瓜，胡蘿蔔，菠菜等，禁止孩子吃一些油炸食品和乾果類食物，這些都是乾熱量高脂肪的食物，而且不要讓孩子總是吃完就躺在床上不動，家長在治療小兒肥胖期間如果孩子情況嚴重就要及時去醫院在醫生的指導下給孩子治療。</s>
04/07/2024 18:21:52 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
04/07/2024 18:21:52 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
04/07/2024 18:21:53 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
04/07/2024 18:21:53 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
04/07/2024 18:21:53 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
04/07/2024 18:21:53 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
04/07/2024 18:21:53 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
04/07/2024 18:21:53 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
04/07/2024 18:21:53 - INFO - llmtuner.model.adapter - Fine-tuning method: Full
04/07/2024 18:21:53 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
04/07/2024 18:21:53 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
04/07/2024 18:21:53 - INFO - llmtuner.model.loader - trainable params: 6738415616 || all params: 6738415616 || trainable%: 100.0000
[1/4] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wenning1/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_70,code=compute_70 -gencode=arch=compute_70,code=sm_70 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ --threads=8 -gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_70,code=compute_70 -c /home/wenning1/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o 
[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wenning1/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /home/wenning1/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/home/wenning1/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /home/wenning1/.local/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so
Time to load cpu_adam op: 35.919880867004395 seconds
Time to load cpu_adam op: 35.9565589427948 seconds
Time to load cpu_adam op: 35.95771932601929 secondsTime to load cpu_adam op: 35.957728147506714 seconds

Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000050, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-04-07 18:22:36,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown
[2024-04-07 18:22:36,786] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-04-07 18:22:36,787] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-04-07 18:22:36,787] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-04-07 18:22:36,796] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-04-07 18:22:36,796] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-04-07 18:22:36,796] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[2024-04-07 18:22:36,797] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000
[2024-04-07 18:22:36,797] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000
[2024-04-07 18:22:36,797] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: True
[2024-04-07 18:22:36,797] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: True
[2024-04-07 18:22:52,009] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states
[2024-04-07 18:22:52,010] [INFO] [utils.py:801:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-04-07 18:22:52,011] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 102.67 GB, percent = 13.6%
[2024-04-07 18:22:56,952] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states
[2024-04-07 18:22:56,952] [INFO] [utils.py:801:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-04-07 18:22:56,953] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 137.19 GB, percent = 18.2%
[2024-04-07 18:22:56,953] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized
[2024-04-07 18:22:57,382] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer
[2024-04-07 18:22:57,383] [INFO] [utils.py:801:see_memory_usage] MA 12.86 GB         Max_MA 12.86 GB         CA 12.86 GB         Max_CA 13 GB 
[2024-04-07 18:22:57,383] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 137.68 GB, percent = 18.3%
[2024-04-07 18:22:57,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam
[2024-04-07 18:22:57,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-04-07 18:22:57,386] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-04-07 18:22:57,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]
[2024-04-07 18:22:57,387] [INFO] [config.py:996:print] DeepSpeedEngine configuration:
[2024-04-07 18:22:57,387] [INFO] [config.py:1000:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-04-07 18:22:57,387] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-04-07 18:22:57,387] [INFO] [config.py:1000:print]   amp_enabled .................. False
[2024-04-07 18:22:57,387] [INFO] [config.py:1000:print]   amp_params ................... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b61647ec610>
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   communication_data_type ...... None
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   disable_allgather ............ False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   dump_state ................... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   elasticity_enabled ........... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   fp16_enabled ................. True
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   global_rank .................. 0
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 2
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   gradient_clipping ............ 1.0
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   graph_harvesting ............. False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   loss_scale ................... 0
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   memory_breakdown ............. False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   mics_shard_size .............. -1
[2024-04-07 18:22:57,388] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   optimizer_name ............... None
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   optimizer_params ............. None
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   pld_enabled .................. False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   pld_params ................... False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   prescale_gradients ........... False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   scheduler_name ............... None
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   scheduler_params ............. None
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   sparse_attention ............. None
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   steps_per_print .............. inf
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   train_batch_size ............. 8
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   use_node_local_storage ....... False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   weight_quantization_config ... None
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   world_size ................... 4
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   zero_enabled ................. True
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True
[2024-04-07 18:22:57,389] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2
[2024-04-07 18:22:57,389] [INFO] [config.py:986:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 2, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": true, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": false
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 5.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 5.000000e+08, 
        "contiguous_gradients": true, 
        "round_robin_gradients": true
    }, 
    "steps_per_print": inf
}
{'loss': 1.0556, 'grad_norm': 3.609891891479492, 'learning_rate': 2.5e-05, 'epoch': 0.09}
{'loss': 0.5183, 'grad_norm': 3.621852397918701, 'learning_rate': 5e-05, 'epoch': 0.18}
[2024-04-07 18:28:13,828] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
{'loss': 0.3793, 'grad_norm': 2.0763423442840576, 'learning_rate': 4.989999289644992e-05, 'epoch': 0.27}
[2024-04-07 18:28:18,903] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
{'loss': 0.3819, 'grad_norm': 2.428572654724121, 'learning_rate': 4.960077169946052e-05, 'epoch': 0.36}
{'loss': 0.3088, 'grad_norm': 1.5957456827163696, 'learning_rate': 4.903762260818551e-05, 'epoch': 0.44}
{'loss': 0.2243, 'grad_norm': 1.3841285705566406, 'learning_rate': 4.8237085192434676e-05, 'epoch': 0.53}
{'loss': 0.1583, 'grad_norm': 1.2400556802749634, 'learning_rate': 4.72070653187283e-05, 'epoch': 0.62}
{'loss': 0.3092, 'grad_norm': 1.5679696798324585, 'learning_rate': 4.5957735153256915e-05, 'epoch': 0.71}
{'loss': 0.2617, 'grad_norm': 1.2036892175674438, 'learning_rate': 4.4501432704630305e-05, 'epoch': 0.8}
[2024-04-07 18:39:38,252] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
{'loss': 0.2288, 'grad_norm': 0.6071386337280273, 'learning_rate': 4.3025645923732926e-05, 'epoch': 0.89}
{'eval_loss': 0.1572735607624054, 'eval_runtime': 19.5295, 'eval_samples_per_second': 5.12, 'eval_steps_per_second': 1.28, 'epoch': 0.89}
[2024-04-07 18:40:58,724] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-04-07 18:40:58,732] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2024-04-07 18:40:58,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/global_step100/mp_rank_00_model_states.pt...
[2024-04-07 18:41:11,467] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/global_step100/mp_rank_00_model_states.pt.
[2024-04-07 18:41:11,469] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-07 18:41:24,739] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-07 18:41:24,752] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-07 18:41:24,752] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
{'loss': 0.1892, 'grad_norm': 1.2876948118209839, 'learning_rate': 4.121729377463285e-05, 'epoch': 0.98}
{'loss': 0.1908, 'grad_norm': 2.3122951984405518, 'learning_rate': 3.924878451434735e-05, 'epoch': 1.07}
{'loss': 0.1747, 'grad_norm': 0.6451461911201477, 'learning_rate': 3.713955854772144e-05, 'epoch': 1.16}
{'loss': 0.1392, 'grad_norm': 0.764187753200531, 'learning_rate': 3.4910445955429854e-05, 'epoch': 1.24}
{'loss': 0.0912, 'grad_norm': 0.4580959975719452, 'learning_rate': 3.258346078237122e-05, 'epoch': 1.33}
{'loss': 0.1245, 'grad_norm': 1.1413966417312622, 'learning_rate': 3.018158363358773e-05, 'epoch': 1.42}
{'loss': 0.1786, 'grad_norm': 2.0842511653900146, 'learning_rate': 2.7728534724728027e-05, 'epoch': 1.51}
{'loss': 0.1066, 'grad_norm': 0.9558092355728149, 'learning_rate': 2.5248539628338246e-05, 'epoch': 1.6}
{'loss': 0.0752, 'grad_norm': 0.4178353250026703, 'learning_rate': 2.2766090029400573e-05, 'epoch': 1.69}
{'loss': 0.0767, 'grad_norm': 0.8839486241340637, 'learning_rate': 2.030570185282544e-05, 'epoch': 1.78}
{'eval_loss': 0.07695190608501434, 'eval_runtime': 19.4979, 'eval_samples_per_second': 5.129, 'eval_steps_per_second': 1.282, 'epoch': 1.78}
[2024-04-07 18:59:39,351] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-04-07 18:59:39,359] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2024-04-07 18:59:39,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2024-04-07 18:59:51,946] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2024-04-07 18:59:51,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-07 19:00:05,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-07 19:00:05,001] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-07 19:00:05,001] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
{'loss': 0.1027, 'grad_norm': 0.343330442905426, 'learning_rate': 1.789167315155749e-05, 'epoch': 1.87}
{'loss': 0.0978, 'grad_norm': 0.36534950137138367, 'learning_rate': 1.5547844146319545e-05, 'epoch': 1.96}
{'loss': 0.0759, 'grad_norm': 0.44134166836738586, 'learning_rate': 1.3297361786769652e-05, 'epoch': 2.04}
{'loss': 0.0642, 'grad_norm': 0.8097285032272339, 'learning_rate': 1.1162451159194614e-05, 'epoch': 2.13}
{'loss': 0.0913, 'grad_norm': 0.2763434052467346, 'learning_rate': 9.16419599824847e-06, 'epoch': 2.22}
{'loss': 0.0731, 'grad_norm': 0.3974161744117737, 'learning_rate': 7.3223304703363135e-06, 'epoch': 2.31}
{'loss': 0.0939, 'grad_norm': 0.4505806863307953, 'learning_rate': 5.655044284927657e-06, 'epoch': 2.4}
{'loss': 0.0599, 'grad_norm': 0.2641710340976715, 'learning_rate': 4.178803058461664e-06, 'epoch': 2.49}
{'loss': 0.0467, 'grad_norm': 0.3212595283985138, 'learning_rate': 2.908185704876101e-06, 'epoch': 2.58}
{'loss': 0.0657, 'grad_norm': 0.24682360887527466, 'learning_rate': 1.8557404586421413e-06, 'epoch': 2.67}
{'eval_loss': 0.05014587938785553, 'eval_runtime': 19.5193, 'eval_samples_per_second': 5.123, 'eval_steps_per_second': 1.281, 'epoch': 2.67}
[2024-04-07 19:18:08,329] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2024-04-07 19:18:08,338] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/global_step300/mp_rank_00_model_states.pt
[2024-04-07 19:18:08,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/global_step300/mp_rank_00_model_states.pt...
[2024-04-07 19:18:21,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/global_step300/mp_rank_00_model_states.pt.
[2024-04-07 19:18:21,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-04-07 19:18:34,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-04-07 19:18:34,574] [INFO] [engine.py:3488:_save_zero_checkpoint] zero checkpoint saved ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-04-07 19:18:34,574] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
{'loss': 0.0449, 'grad_norm': 0.31564924120903015, 'learning_rate': 1.0318609521783818e-06, 'epoch': 2.76}
{'loss': 0.0418, 'grad_norm': 0.3149891793727875, 'learning_rate': 4.4468357146596475e-07, 'epoch': 2.84}
{'loss': 0.083, 'grad_norm': 0.3270021080970764, 'learning_rate': 1.0000710355008159e-07, 'epoch': 2.93}
{'train_runtime': 3715.7814, 'train_samples_per_second': 0.727, 'train_steps_per_second': 0.09, 'train_loss': 0.1826572036814122, 'epoch': 2.99}
***** train metrics *****
  epoch                    =       2.99
  train_loss               =     0.1827
  train_runtime            = 1:01:55.78
  train_samples_per_second =      0.727
  train_steps_per_second   =       0.09
Figure saved at: ../../saves/LLaMA2-7B/full/04_sft/training_loss.png
Figure saved at: ../../saves/LLaMA2-7B/full/04_sft/training_eval_loss.png
***** eval metrics *****
  epoch                   =       2.99
  eval_loss               =     0.0496
  eval_runtime            = 0:00:19.27
  eval_samples_per_second =      5.189
  eval_steps_per_second   =      1.297
[2024-04-07 19:25:51,196] [INFO] [launch.py:348:main] Process 10151 exits successfully.
[2024-04-07 19:25:52,197] [INFO] [launch.py:348:main] Process 10149 exits successfully.
[2024-04-07 19:25:55,200] [INFO] [launch.py:348:main] Process 10150 exits successfully.
[2024-04-07 19:25:56,201] [INFO] [launch.py:348:main] Process 10148 exits successfully.
