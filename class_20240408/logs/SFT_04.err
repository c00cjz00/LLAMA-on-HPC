[INFO|tokenization_utils_base.py:2084] 2024-04-07 18:21:02,098 >> loading file tokenizer.model from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-04-07 18:21:02,098 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-04-07 18:21:02,098 >> loading file special_tokens_map.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 18:21:02,098 >> loading file tokenizer_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer_config.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 18:21:02,098 >> loading file tokenizer.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.json
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:01, 577.38 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4600.03 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:03, 292.11 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:02<00:15, 54.80 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 251/1000 [00:02<00:05, 135.17 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:02<00:00, 767.90 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:02<00:00, 424.56 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:02<00:39, 23.93 examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:02<00:38, 24.41 examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:02<00:40, 23.37 examples/s]Converting format of dataset (num_proc=16):  19%|█▉        | 188/1000 [00:02<00:09, 84.67 examples/s]Converting format of dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:02<00:00, 373.07 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:02<00:00, 353.15 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:02<00:00, 349.34 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:02<00:00, 339.68 examples/s]
[INFO|configuration_utils.py:726] 2024-04-07 18:21:17,978 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 18:21:17,981 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3283] 2024-04-07 18:21:18,014 >> loading weights file model.safetensors from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-04-07 18:21:18,015 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-07 18:21:18,016 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:04, 190.85 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:07, 133.61 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:00<00:03, 228.68 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 439/1000 [00:00<00:00, 973.56 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:00<00:01, 552.39 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 690/1000 [00:00<00:00, 1065.51 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:11, 81.03 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 441/1000 [00:00<00:00, 647.86 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 314/1000 [00:00<00:01, 434.72 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:00<00:00, 1068.06 examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 752/1000 [00:01<00:00, 1037.89 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 814/1000 [00:01<00:00, 1210.60 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:01<00:00, 875.35 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:01<00:00, 863.96 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:01<00:00, 840.12 examples/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.09s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:24<00:24, 24.60s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:25<00:25, 25.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 14.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.21s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 14.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.29s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 14.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:32<00:00, 16.34s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 15.28s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 16.84s/it]
[INFO|modeling_utils.py:4024] 2024-04-07 18:21:52,819 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-04-07 18:21:52,819 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 18:21:53,046 >> loading configuration file generation_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 18:21:53,047 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-04-07 18:21:53,522 >> Using auto half precision backend
[INFO|deepspeed.py:325] 2024-04-07 18:21:53,792 >> Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
Using /home/wenning1/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Creating extension directory /home/wenning1/.cache/torch_extensions/py310_cu118/cpu_adam...
Using /home/wenning1/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Using /home/wenning1/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Using /home/wenning1/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/wenning1/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...
Building extension module cpu_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Loading extension module cpu_adam...
Loading extension module cpu_adam...
Loading extension module cpu_adam...Loading extension module cpu_adam...

[INFO|trainer.py:1969] 2024-04-07 18:22:57,389 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-04-07 18:22:57,389 >>   Num examples = 900
[INFO|trainer.py:1971] 2024-04-07 18:22:57,389 >>   Num Epochs = 3
[INFO|trainer.py:1972] 2024-04-07 18:22:57,389 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-04-07 18:22:57,389 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-04-07 18:22:57,389 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-04-07 18:22:57,389 >>   Total optimization steps = 336
[INFO|trainer.py:1978] 2024-04-07 18:22:57,390 >>   Number of trainable parameters = 6,738,415,616
  0%|          | 0/336 [00:00<?, ?it/s]  0%|          | 1/336 [00:17<1:37:18, 17.43s/it]  1%|          | 2/336 [00:27<1:13:54, 13.28s/it]  1%|          | 3/336 [00:38<1:06:04, 11.91s/it]  1%|          | 4/336 [00:48<1:02:40, 11.33s/it]  1%|▏         | 5/336 [00:59<1:01:05, 11.07s/it]  2%|▏         | 6/336 [01:10<1:00:46, 11.05s/it]  2%|▏         | 7/336 [01:20<1:00:09, 10.97s/it]  2%|▏         | 8/336 [01:31<59:41, 10.92s/it]    3%|▎         | 9/336 [01:41<58:05, 10.66s/it]  3%|▎         | 10/336 [01:52<58:30, 10.77s/it]                                                  3%|▎         | 10/336 [01:52<58:30, 10.77s/it]  3%|▎         | 11/336 [02:03<57:46, 10.67s/it]  4%|▎         | 12/336 [02:14<57:48, 10.71s/it]  4%|▍         | 13/336 [02:23<56:08, 10.43s/it]  4%|▍         | 14/336 [02:33<55:26, 10.33s/it]  4%|▍         | 15/336 [02:44<55:35, 10.39s/it]  5%|▍         | 16/336 [02:54<55:19, 10.37s/it]  5%|▌         | 17/336 [03:05<55:39, 10.47s/it]  5%|▌         | 18/336 [03:16<55:36, 10.49s/it]  6%|▌         | 19/336 [03:26<54:58, 10.41s/it]  6%|▌         | 20/336 [03:37<55:27, 10.53s/it]                                                  6%|▌         | 20/336 [03:37<55:27, 10.53s/it]  6%|▋         | 21/336 [03:47<55:18, 10.54s/it]  7%|▋         | 22/336 [03:57<54:45, 10.46s/it]  7%|▋         | 23/336 [04:08<55:19, 10.60s/it]  7%|▋         | 24/336 [04:19<55:20, 10.64s/it]  7%|▋         | 25/336 [04:29<54:33, 10.53s/it]  8%|▊         | 26/336 [04:40<54:15, 10.50s/it]  8%|▊         | 27/336 [04:51<54:35, 10.60s/it]  8%|▊         | 28/336 [05:01<54:07, 10.54s/it]  9%|▊         | 29/336 [05:11<52:51, 10.33s/it]  9%|▉         | 30/336 [05:16<44:35,  8.74s/it]                                                  9%|▉         | 30/336 [05:16<44:35,  8.74s/it]  9%|▉         | 31/336 [05:21<38:51,  7.64s/it] 10%|▉         | 32/336 [05:32<43:58,  8.68s/it] 10%|▉         | 33/336 [05:43<46:34,  9.22s/it] 10%|█         | 34/336 [05:53<48:45,  9.69s/it] 10%|█         | 35/336 [06:04<50:37, 10.09s/it] 11%|█         | 36/336 [06:15<51:10, 10.23s/it] 11%|█         | 37/336 [06:26<51:39, 10.37s/it] 11%|█▏        | 38/336 [06:36<51:54, 10.45s/it] 12%|█▏        | 39/336 [06:47<52:14, 10.55s/it] 12%|█▏        | 40/336 [06:58<52:19, 10.61s/it]                                                 12%|█▏        | 40/336 [06:58<52:19, 10.61s/it] 12%|█▏        | 41/336 [07:09<52:19, 10.64s/it] 12%|█▎        | 42/336 [07:20<52:56, 10.80s/it] 13%|█▎        | 43/336 [07:31<52:50, 10.82s/it] 13%|█▎        | 44/336 [07:41<52:26, 10.78s/it] 13%|█▎        | 45/336 [07:51<51:06, 10.54s/it] 14%|█▎        | 46/336 [08:02<50:40, 10.48s/it] 14%|█▍        | 47/336 [08:12<49:53, 10.36s/it] 14%|█▍        | 48/336 [08:22<50:02, 10.43s/it] 15%|█▍        | 49/336 [08:32<49:19, 10.31s/it] 15%|█▍        | 50/336 [08:43<49:05, 10.30s/it]                                                 15%|█▍        | 50/336 [08:43<49:05, 10.30s/it] 15%|█▌        | 51/336 [08:53<49:26, 10.41s/it] 15%|█▌        | 52/336 [09:04<49:25, 10.44s/it] 16%|█▌        | 53/336 [09:14<48:57, 10.38s/it] 16%|█▌        | 54/336 [09:25<49:41, 10.57s/it] 16%|█▋        | 55/336 [09:36<49:46, 10.63s/it] 17%|█▋        | 56/336 [09:46<49:15, 10.55s/it] 17%|█▋        | 57/336 [09:57<48:53, 10.51s/it] 17%|█▋        | 58/336 [10:07<48:10, 10.40s/it] 18%|█▊        | 59/336 [10:17<47:35, 10.31s/it] 18%|█▊        | 60/336 [10:27<47:37, 10.35s/it]                                                 18%|█▊        | 60/336 [10:27<47:37, 10.35s/it] 18%|█▊        | 61/336 [10:38<47:22, 10.34s/it] 18%|█▊        | 62/336 [10:48<47:12, 10.34s/it] 19%|█▉        | 63/336 [10:58<47:12, 10.37s/it] 19%|█▉        | 64/336 [11:09<46:55, 10.35s/it] 19%|█▉        | 65/336 [11:19<47:18, 10.47s/it] 20%|█▉        | 66/336 [11:29<46:34, 10.35s/it] 20%|█▉        | 67/336 [11:40<46:25, 10.35s/it] 20%|██        | 68/336 [11:50<45:52, 10.27s/it] 21%|██        | 69/336 [12:01<46:10, 10.38s/it] 21%|██        | 70/336 [12:10<45:24, 10.24s/it]                                                 21%|██        | 70/336 [12:10<45:24, 10.24s/it] 21%|██        | 71/336 [12:21<46:16, 10.48s/it] 21%|██▏       | 72/336 [12:32<45:55, 10.44s/it] 22%|██▏       | 73/336 [12:42<46:01, 10.50s/it] 22%|██▏       | 74/336 [12:52<44:44, 10.25s/it] 22%|██▏       | 75/336 [13:02<44:40, 10.27s/it] 23%|██▎       | 76/336 [13:13<44:26, 10.26s/it] 23%|██▎       | 77/336 [13:23<44:27, 10.30s/it] 23%|██▎       | 78/336 [13:34<44:45, 10.41s/it] 24%|██▎       | 79/336 [13:44<44:41, 10.43s/it] 24%|██▍       | 80/336 [13:55<44:43, 10.48s/it]                                                 24%|██▍       | 80/336 [13:55<44:43, 10.48s/it] 24%|██▍       | 81/336 [14:06<45:10, 10.63s/it] 24%|██▍       | 82/336 [14:16<45:00, 10.63s/it] 25%|██▍       | 83/336 [14:27<44:57, 10.66s/it] 25%|██▌       | 84/336 [14:38<45:23, 10.81s/it] 25%|██▌       | 85/336 [14:49<44:44, 10.69s/it] 26%|██▌       | 86/336 [15:00<44:42, 10.73s/it] 26%|██▌       | 87/336 [15:10<44:08, 10.64s/it] 26%|██▌       | 88/336 [15:21<44:10, 10.69s/it] 26%|██▋       | 89/336 [15:31<43:48, 10.64s/it] 27%|██▋       | 90/336 [15:42<43:21, 10.57s/it]                                                 27%|██▋       | 90/336 [15:42<43:21, 10.57s/it] 27%|██▋       | 91/336 [15:52<43:23, 10.63s/it] 27%|██▋       | 92/336 [16:03<43:33, 10.71s/it] 28%|██▊       | 93/336 [16:14<43:13, 10.67s/it] 28%|██▊       | 94/336 [16:24<42:43, 10.59s/it] 28%|██▊       | 95/336 [16:35<42:48, 10.66s/it] 29%|██▊       | 96/336 [16:40<36:02,  9.01s/it] 29%|██▉       | 97/336 [16:51<37:44,  9.47s/it] 29%|██▉       | 98/336 [17:02<39:06,  9.86s/it] 29%|██▉       | 99/336 [17:12<39:41, 10.05s/it] 30%|██▉       | 100/336 [17:23<40:00, 10.17s/it]                                                  30%|██▉       | 100/336 [17:23<40:00, 10.17s/it][INFO|trainer.py:3512] 2024-04-07 18:40:20,530 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 18:40:20,530 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 18:40:20,530 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:09,  2.55it/s][A
 12%|█▏        | 3/25 [00:01<00:11,  1.85it/s][A
 16%|█▌        | 4/25 [00:02<00:13,  1.60it/s][A
 20%|██        | 5/25 [00:03<00:13,  1.47it/s][A
 24%|██▍       | 6/25 [00:03<00:13,  1.43it/s][A
 28%|██▊       | 7/25 [00:04<00:12,  1.39it/s][A
 32%|███▏      | 8/25 [00:05<00:12,  1.34it/s][A
 36%|███▌      | 9/25 [00:06<00:12,  1.32it/s][A
 40%|████      | 10/25 [00:06<00:11,  1.30it/s][A
 44%|████▍     | 11/25 [00:07<00:10,  1.30it/s][A
 48%|████▊     | 12/25 [00:08<00:10,  1.29it/s][A
 52%|█████▏    | 13/25 [00:09<00:09,  1.29it/s][A
 56%|█████▌    | 14/25 [00:10<00:08,  1.29it/s][A
 60%|██████    | 15/25 [00:10<00:07,  1.27it/s][A
 64%|██████▍   | 16/25 [00:11<00:07,  1.28it/s][A
 68%|██████▊   | 17/25 [00:12<00:06,  1.29it/s][A
 72%|███████▏  | 18/25 [00:13<00:05,  1.26it/s][A
 76%|███████▌  | 19/25 [00:14<00:04,  1.28it/s][A
 80%|████████  | 20/25 [00:14<00:03,  1.28it/s][A
 84%|████████▍ | 21/25 [00:15<00:03,  1.26it/s][A
 88%|████████▊ | 22/25 [00:16<00:02,  1.28it/s][A
 92%|█████████▏| 23/25 [00:17<00:01,  1.26it/s][A
 96%|█████████▌| 24/25 [00:17<00:00,  1.27it/s][A
100%|██████████| 25/25 [00:18<00:00,  1.28it/s][A                                                 
                                               [A 30%|██▉       | 100/336 [17:42<40:00, 10.17s/it]
100%|██████████| 25/25 [00:18<00:00,  1.28it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 18:40:45,677 >> Saving model checkpoint to ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100
[INFO|configuration_utils.py:471] 2024-04-07 18:40:45,678 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/config.json
[INFO|configuration_utils.py:697] 2024-04-07 18:40:45,679 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:2482] 2024-04-07 18:40:58,633 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2502] 2024-04-07 18:40:58,633 >> tokenizer config file saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 18:40:58,634 >> Special tokens file saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-100/special_tokens_map.json
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 30%|███       | 101/336 [18:38<1:56:16, 29.69s/it] 30%|███       | 102/336 [18:48<1:33:02, 23.86s/it] 31%|███       | 103/336 [18:59<1:17:25, 19.94s/it] 31%|███       | 104/336 [19:09<1:05:56, 17.05s/it] 31%|███▏      | 105/336 [19:20<58:37, 15.23s/it]   32%|███▏      | 106/336 [19:31<53:24, 13.93s/it] 32%|███▏      | 107/336 [19:42<49:26, 12.95s/it] 32%|███▏      | 108/336 [19:53<47:08, 12.41s/it] 32%|███▏      | 109/336 [20:03<44:36, 11.79s/it] 33%|███▎      | 110/336 [20:13<42:25, 11.26s/it]                                                  33%|███▎      | 110/336 [20:13<42:25, 11.26s/it] 33%|███▎      | 111/336 [20:24<41:41, 11.12s/it] 33%|███▎      | 112/336 [20:34<40:38, 10.89s/it] 34%|███▎      | 113/336 [20:45<40:18, 10.85s/it] 34%|███▍      | 114/336 [20:56<39:48, 10.76s/it] 34%|███▍      | 115/336 [21:07<39:39, 10.77s/it] 35%|███▍      | 116/336 [21:17<39:29, 10.77s/it] 35%|███▍      | 117/336 [21:27<38:23, 10.52s/it] 35%|███▌      | 118/336 [21:37<37:39, 10.36s/it] 35%|███▌      | 119/336 [21:47<37:11, 10.28s/it] 36%|███▌      | 120/336 [21:58<37:38, 10.45s/it]                                                  36%|███▌      | 120/336 [21:58<37:38, 10.45s/it] 36%|███▌      | 121/336 [22:09<37:20, 10.42s/it] 36%|███▋      | 122/336 [22:19<37:39, 10.56s/it] 37%|███▋      | 123/336 [22:30<37:32, 10.57s/it] 37%|███▋      | 124/336 [22:41<37:20, 10.57s/it] 37%|███▋      | 125/336 [22:51<37:15, 10.59s/it] 38%|███▊      | 126/336 [23:02<36:52, 10.54s/it] 38%|███▊      | 127/336 [23:12<36:44, 10.55s/it] 38%|███▊      | 128/336 [23:23<37:01, 10.68s/it] 38%|███▊      | 129/336 [23:34<36:30, 10.58s/it] 39%|███▊      | 130/336 [23:44<36:35, 10.66s/it]                                                  39%|███▊      | 130/336 [23:44<36:35, 10.66s/it] 39%|███▉      | 131/336 [23:55<36:14, 10.61s/it] 39%|███▉      | 132/336 [24:06<36:17, 10.67s/it] 40%|███▉      | 133/336 [24:16<35:29, 10.49s/it] 40%|███▉      | 134/336 [24:27<35:47, 10.63s/it] 40%|████      | 135/336 [24:37<35:22, 10.56s/it] 40%|████      | 136/336 [24:47<35:01, 10.51s/it] 41%|████      | 137/336 [24:58<34:58, 10.55s/it] 41%|████      | 138/336 [25:09<34:45, 10.53s/it] 41%|████▏     | 139/336 [25:19<34:22, 10.47s/it] 42%|████▏     | 140/336 [25:29<34:13, 10.48s/it]                                                  42%|████▏     | 140/336 [25:29<34:13, 10.48s/it] 42%|████▏     | 141/336 [25:40<34:22, 10.58s/it] 42%|████▏     | 142/336 [25:51<34:06, 10.55s/it] 43%|████▎     | 143/336 [26:02<34:10, 10.63s/it] 43%|████▎     | 144/336 [26:12<33:53, 10.59s/it] 43%|████▎     | 145/336 [26:22<33:04, 10.39s/it] 43%|████▎     | 146/336 [26:32<32:46, 10.35s/it] 44%|████▍     | 147/336 [26:43<32:52, 10.43s/it] 44%|████▍     | 148/336 [26:54<32:54, 10.50s/it] 44%|████▍     | 149/336 [27:04<32:56, 10.57s/it] 45%|████▍     | 150/336 [27:15<32:35, 10.51s/it]                                                  45%|████▍     | 150/336 [27:15<32:35, 10.51s/it] 45%|████▍     | 151/336 [27:25<32:18, 10.48s/it] 45%|████▌     | 152/336 [27:36<32:30, 10.60s/it] 46%|████▌     | 153/336 [27:47<32:34, 10.68s/it] 46%|████▌     | 154/336 [27:57<31:48, 10.49s/it] 46%|████▌     | 155/336 [28:07<31:39, 10.50s/it] 46%|████▋     | 156/336 [28:18<31:40, 10.56s/it] 47%|████▋     | 157/336 [28:29<31:31, 10.57s/it] 47%|████▋     | 158/336 [28:40<31:37, 10.66s/it] 47%|████▋     | 159/336 [28:50<31:34, 10.70s/it] 48%|████▊     | 160/336 [29:01<31:07, 10.61s/it]                                                  48%|████▊     | 160/336 [29:01<31:07, 10.61s/it] 48%|████▊     | 161/336 [29:11<30:48, 10.56s/it] 48%|████▊     | 162/336 [29:22<30:37, 10.56s/it] 49%|████▊     | 163/336 [29:32<30:21, 10.53s/it] 49%|████▉     | 164/336 [29:42<30:01, 10.47s/it] 49%|████▉     | 165/336 [29:54<30:33, 10.72s/it] 49%|████▉     | 166/336 [30:04<29:43, 10.49s/it] 50%|████▉     | 167/336 [30:14<29:19, 10.41s/it] 50%|█████     | 168/336 [30:25<29:29, 10.54s/it] 50%|█████     | 169/336 [30:35<28:40, 10.30s/it] 51%|█████     | 170/336 [30:45<28:39, 10.36s/it]                                                  51%|█████     | 170/336 [30:45<28:39, 10.36s/it] 51%|█████     | 171/336 [30:55<28:28, 10.35s/it] 51%|█████     | 172/336 [31:07<28:56, 10.59s/it] 51%|█████▏    | 173/336 [31:17<28:36, 10.53s/it] 52%|█████▏    | 174/336 [31:27<28:06, 10.41s/it] 52%|█████▏    | 175/336 [31:38<28:13, 10.52s/it] 52%|█████▏    | 176/336 [31:48<28:09, 10.56s/it] 53%|█████▎    | 177/336 [31:59<28:06, 10.60s/it] 53%|█████▎    | 178/336 [32:10<27:47, 10.55s/it] 53%|█████▎    | 179/336 [32:20<27:38, 10.56s/it] 54%|█████▎    | 180/336 [32:31<27:39, 10.64s/it]                                                  54%|█████▎    | 180/336 [32:31<27:39, 10.64s/it] 54%|█████▍    | 181/336 [32:42<27:40, 10.71s/it] 54%|█████▍    | 182/336 [32:53<27:24, 10.68s/it] 54%|█████▍    | 183/336 [33:03<27:05, 10.63s/it] 55%|█████▍    | 184/336 [33:13<26:45, 10.56s/it] 55%|█████▌    | 185/336 [33:24<26:38, 10.58s/it] 55%|█████▌    | 186/336 [33:35<26:23, 10.56s/it] 56%|█████▌    | 187/336 [33:45<26:28, 10.66s/it] 56%|█████▌    | 188/336 [33:56<26:14, 10.64s/it] 56%|█████▋    | 189/336 [34:07<26:19, 10.75s/it] 57%|█████▋    | 190/336 [34:18<26:24, 10.85s/it]                                                  57%|█████▋    | 190/336 [34:18<26:24, 10.85s/it] 57%|█████▋    | 191/336 [34:28<25:45, 10.66s/it] 57%|█████▋    | 192/336 [34:39<25:44, 10.73s/it] 57%|█████▋    | 193/336 [34:49<25:12, 10.58s/it] 58%|█████▊    | 194/336 [35:00<25:01, 10.58s/it] 58%|█████▊    | 195/336 [35:10<24:38, 10.49s/it] 58%|█████▊    | 196/336 [35:21<24:53, 10.66s/it] 59%|█████▊    | 197/336 [35:32<24:26, 10.55s/it] 59%|█████▉    | 198/336 [35:43<24:36, 10.70s/it] 59%|█████▉    | 199/336 [35:53<24:20, 10.66s/it] 60%|█████▉    | 200/336 [36:04<24:15, 10.70s/it]                                                  60%|█████▉    | 200/336 [36:04<24:15, 10.70s/it][INFO|trainer.py:3512] 2024-04-07 18:59:01,993 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 18:59:01,993 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 18:59:01,993 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:09,  2.49it/s][A
 12%|█▏        | 3/25 [00:01<00:11,  1.88it/s][A
 16%|█▌        | 4/25 [00:02<00:13,  1.61it/s][A
 20%|██        | 5/25 [00:03<00:13,  1.49it/s][A
 24%|██▍       | 6/25 [00:03<00:13,  1.41it/s][A
 28%|██▊       | 7/25 [00:04<00:12,  1.40it/s][A
 32%|███▏      | 8/25 [00:05<00:12,  1.35it/s][A
 36%|███▌      | 9/25 [00:06<00:11,  1.34it/s][A
 40%|████      | 10/25 [00:06<00:11,  1.33it/s][A
 44%|████▍     | 11/25 [00:07<00:10,  1.29it/s][A
 48%|████▊     | 12/25 [00:08<00:10,  1.28it/s][A
 52%|█████▏    | 13/25 [00:09<00:09,  1.30it/s][A
 56%|█████▌    | 14/25 [00:10<00:08,  1.28it/s][A
 60%|██████    | 15/25 [00:10<00:08,  1.22it/s][A
 64%|██████▍   | 16/25 [00:11<00:07,  1.24it/s][A
 68%|██████▊   | 17/25 [00:12<00:06,  1.24it/s][A
 72%|███████▏  | 18/25 [00:13<00:05,  1.25it/s][A
 76%|███████▌  | 19/25 [00:14<00:04,  1.27it/s][A
 80%|████████  | 20/25 [00:14<00:03,  1.27it/s][A
 84%|████████▍ | 21/25 [00:15<00:03,  1.28it/s][A
 88%|████████▊ | 22/25 [00:16<00:02,  1.28it/s][A
 92%|█████████▏| 23/25 [00:17<00:01,  1.28it/s][A
 96%|█████████▌| 24/25 [00:18<00:00,  1.28it/s][A
100%|██████████| 25/25 [00:18<00:00,  1.30it/s][A                                                 
                                               [A 60%|█████▉    | 200/336 [36:24<24:15, 10.70s/it]
100%|██████████| 25/25 [00:18<00:00,  1.30it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 18:59:27,128 >> Saving model checkpoint to ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200
[INFO|configuration_utils.py:471] 2024-04-07 18:59:27,130 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/config.json
[INFO|configuration_utils.py:697] 2024-04-07 18:59:27,130 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:2482] 2024-04-07 18:59:39,289 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2502] 2024-04-07 18:59:39,290 >> tokenizer config file saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 18:59:39,291 >> Special tokens file saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-200/special_tokens_map.json
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 60%|█████▉    | 201/336 [37:18<1:06:47, 29.68s/it] 60%|██████    | 202/336 [37:29<53:41, 24.04s/it]   60%|██████    | 203/336 [37:39<44:13, 19.95s/it] 61%|██████    | 204/336 [37:50<37:42, 17.14s/it] 61%|██████    | 205/336 [38:00<33:05, 15.16s/it] 61%|██████▏   | 206/336 [38:11<29:40, 13.70s/it] 62%|██████▏   | 207/336 [38:21<27:18, 12.70s/it] 62%|██████▏   | 208/336 [38:32<25:59, 12.18s/it] 62%|██████▏   | 209/336 [38:43<24:56, 11.78s/it] 62%|██████▎   | 210/336 [38:53<23:55, 11.39s/it]                                                  62%|██████▎   | 210/336 [38:53<23:55, 11.39s/it] 63%|██████▎   | 211/336 [39:04<22:58, 11.03s/it] 63%|██████▎   | 212/336 [39:14<22:32, 10.91s/it] 63%|██████▎   | 213/336 [39:25<22:12, 10.83s/it] 64%|██████▎   | 214/336 [39:35<21:39, 10.65s/it] 64%|██████▍   | 215/336 [39:46<21:24, 10.61s/it] 64%|██████▍   | 216/336 [39:56<21:06, 10.56s/it] 65%|██████▍   | 217/336 [40:07<20:52, 10.52s/it] 65%|██████▍   | 218/336 [40:17<20:55, 10.64s/it] 65%|██████▌   | 219/336 [40:28<20:43, 10.63s/it] 65%|██████▌   | 220/336 [40:38<20:05, 10.39s/it]                                                  65%|██████▌   | 220/336 [40:38<20:05, 10.39s/it] 66%|██████▌   | 221/336 [40:49<20:04, 10.47s/it] 66%|██████▌   | 222/336 [40:59<19:48, 10.43s/it] 66%|██████▋   | 223/336 [41:09<19:22, 10.29s/it] 67%|██████▋   | 224/336 [41:20<19:28, 10.44s/it] 67%|██████▋   | 225/336 [41:30<19:10, 10.37s/it] 67%|██████▋   | 226/336 [41:41<19:15, 10.51s/it] 68%|██████▊   | 227/336 [41:51<18:49, 10.36s/it] 68%|██████▊   | 228/336 [42:01<18:49, 10.46s/it] 68%|██████▊   | 229/336 [42:12<18:53, 10.60s/it] 68%|██████▊   | 230/336 [42:23<18:35, 10.52s/it]                                                  68%|██████▊   | 230/336 [42:23<18:35, 10.52s/it] 69%|██████▉   | 231/336 [42:33<18:07, 10.35s/it] 69%|██████▉   | 232/336 [42:43<17:54, 10.33s/it] 69%|██████▉   | 233/336 [42:53<17:52, 10.41s/it] 70%|██████▉   | 234/336 [43:03<17:28, 10.28s/it] 70%|██████▉   | 235/336 [43:14<17:37, 10.47s/it] 70%|███████   | 236/336 [43:25<17:25, 10.46s/it] 71%|███████   | 237/336 [43:35<17:07, 10.38s/it] 71%|███████   | 238/336 [43:46<17:03, 10.44s/it] 71%|███████   | 239/336 [43:56<16:58, 10.50s/it] 71%|███████▏  | 240/336 [44:06<16:34, 10.36s/it]                                                  71%|███████▏  | 240/336 [44:06<16:34, 10.36s/it] 72%|███████▏  | 241/336 [44:17<16:36, 10.49s/it] 72%|███████▏  | 242/336 [44:27<16:13, 10.36s/it] 72%|███████▏  | 243/336 [44:37<15:49, 10.21s/it] 73%|███████▎  | 244/336 [44:46<15:21, 10.02s/it] 73%|███████▎  | 245/336 [44:57<15:30, 10.23s/it] 73%|███████▎  | 246/336 [45:07<15:19, 10.21s/it] 74%|███████▎  | 247/336 [45:18<15:07, 10.20s/it] 74%|███████▍  | 248/336 [45:28<15:03, 10.26s/it] 74%|███████▍  | 249/336 [45:38<14:47, 10.20s/it] 74%|███████▍  | 250/336 [45:48<14:38, 10.21s/it]                                                  74%|███████▍  | 250/336 [45:48<14:38, 10.21s/it] 75%|███████▍  | 251/336 [45:59<14:41, 10.37s/it] 75%|███████▌  | 252/336 [46:09<14:32, 10.39s/it] 75%|███████▌  | 253/336 [46:20<14:35, 10.55s/it] 76%|███████▌  | 254/336 [46:31<14:36, 10.69s/it] 76%|███████▌  | 255/336 [46:42<14:32, 10.77s/it] 76%|███████▌  | 256/336 [46:52<14:03, 10.54s/it] 76%|███████▋  | 257/336 [47:03<13:54, 10.57s/it] 77%|███████▋  | 258/336 [47:13<13:36, 10.46s/it] 77%|███████▋  | 259/336 [47:24<13:33, 10.57s/it] 77%|███████▋  | 260/336 [47:35<13:21, 10.55s/it]                                                  77%|███████▋  | 260/336 [47:35<13:21, 10.55s/it] 78%|███████▊  | 261/336 [47:45<13:09, 10.52s/it] 78%|███████▊  | 262/336 [47:55<12:54, 10.46s/it] 78%|███████▊  | 263/336 [48:06<12:46, 10.50s/it] 79%|███████▊  | 264/336 [48:16<12:34, 10.47s/it] 79%|███████▉  | 265/336 [48:26<12:11, 10.30s/it] 79%|███████▉  | 266/336 [48:37<12:11, 10.46s/it] 79%|███████▉  | 267/336 [48:48<12:05, 10.51s/it] 80%|███████▉  | 268/336 [48:58<11:58, 10.56s/it] 80%|████████  | 269/336 [49:09<11:48, 10.58s/it] 80%|████████  | 270/336 [49:20<11:38, 10.58s/it]                                                  80%|████████  | 270/336 [49:20<11:38, 10.58s/it] 81%|████████  | 271/336 [49:30<11:29, 10.61s/it] 81%|████████  | 272/336 [49:40<11:13, 10.52s/it] 81%|████████▏ | 273/336 [49:51<11:00, 10.49s/it] 82%|████████▏ | 274/336 [50:01<10:49, 10.48s/it] 82%|████████▏ | 275/336 [50:12<10:41, 10.51s/it] 82%|████████▏ | 276/336 [50:22<10:29, 10.49s/it] 82%|████████▏ | 277/336 [50:33<10:20, 10.52s/it] 83%|████████▎ | 278/336 [50:43<10:09, 10.50s/it] 83%|████████▎ | 279/336 [50:54<09:52, 10.40s/it] 83%|████████▎ | 280/336 [51:04<09:41, 10.39s/it]                                                  83%|████████▎ | 280/336 [51:04<09:41, 10.39s/it] 84%|████████▎ | 281/336 [51:15<09:36, 10.48s/it] 84%|████████▍ | 282/336 [51:25<09:30, 10.56s/it] 84%|████████▍ | 283/336 [51:36<09:15, 10.49s/it] 85%|████████▍ | 284/336 [51:46<08:56, 10.31s/it] 85%|████████▍ | 285/336 [51:56<08:46, 10.33s/it] 85%|████████▌ | 286/336 [52:07<08:42, 10.44s/it] 85%|████████▌ | 287/336 [52:18<08:37, 10.56s/it] 86%|████████▌ | 288/336 [52:28<08:21, 10.45s/it] 86%|████████▌ | 289/336 [52:39<08:19, 10.64s/it] 86%|████████▋ | 290/336 [52:49<08:06, 10.57s/it]                                                  86%|████████▋ | 290/336 [52:49<08:06, 10.57s/it] 87%|████████▋ | 291/336 [52:59<07:51, 10.48s/it] 87%|████████▋ | 292/336 [53:10<07:37, 10.41s/it] 87%|████████▋ | 293/336 [53:20<07:26, 10.38s/it] 88%|████████▊ | 294/336 [53:31<07:21, 10.51s/it] 88%|████████▊ | 295/336 [53:41<07:09, 10.47s/it] 88%|████████▊ | 296/336 [53:52<07:00, 10.52s/it] 88%|████████▊ | 297/336 [54:02<06:48, 10.46s/it] 89%|████████▊ | 298/336 [54:12<06:29, 10.24s/it] 89%|████████▉ | 299/336 [54:22<06:20, 10.30s/it] 89%|████████▉ | 300/336 [54:33<06:13, 10.37s/it]                                                  89%|████████▉ | 300/336 [54:33<06:13, 10.37s/it][INFO|trainer.py:3512] 2024-04-07 19:17:30,783 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:17:30,783 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:17:30,784 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:08,  2.58it/s][A
 12%|█▏        | 3/25 [00:01<00:11,  1.84it/s][A
 16%|█▌        | 4/25 [00:02<00:13,  1.54it/s][A
 20%|██        | 5/25 [00:03<00:13,  1.46it/s][A
 24%|██▍       | 6/25 [00:03<00:13,  1.38it/s][A
 28%|██▊       | 7/25 [00:04<00:13,  1.33it/s][A
 32%|███▏      | 8/25 [00:05<00:13,  1.31it/s][A
 36%|███▌      | 9/25 [00:06<00:12,  1.30it/s][A
 40%|████      | 10/25 [00:07<00:11,  1.31it/s][A
 44%|████▍     | 11/25 [00:07<00:10,  1.33it/s][A
 48%|████▊     | 12/25 [00:08<00:09,  1.33it/s][A
 52%|█████▏    | 13/25 [00:09<00:09,  1.30it/s][A
 56%|█████▌    | 14/25 [00:10<00:08,  1.29it/s][A
 60%|██████    | 15/25 [00:10<00:07,  1.29it/s][A
 64%|██████▍   | 16/25 [00:11<00:07,  1.23it/s][A
 68%|██████▊   | 17/25 [00:12<00:06,  1.27it/s][A
 72%|███████▏  | 18/25 [00:13<00:05,  1.21it/s][A
 76%|███████▌  | 19/25 [00:14<00:04,  1.26it/s][A
 80%|████████  | 20/25 [00:14<00:04,  1.25it/s][A
 84%|████████▍ | 21/25 [00:15<00:03,  1.27it/s][A
 88%|████████▊ | 22/25 [00:16<00:02,  1.31it/s][A
 92%|█████████▏| 23/25 [00:17<00:01,  1.30it/s][A
 96%|█████████▌| 24/25 [00:18<00:00,  1.30it/s][A
100%|██████████| 25/25 [00:18<00:00,  1.31it/s][A                                                 
                                               [A 89%|████████▉ | 300/336 [54:52<06:13, 10.37s/it]
100%|██████████| 25/25 [00:18<00:00,  1.31it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:17:55,987 >> Saving model checkpoint to ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300
[INFO|configuration_utils.py:471] 2024-04-07 19:17:55,988 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/config.json
[INFO|configuration_utils.py:697] 2024-04-07 19:17:55,989 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/generation_config.json
[INFO|modeling_utils.py:2482] 2024-04-07 19:18:08,266 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:18:08,267 >> tokenizer config file saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:18:08,268 >> Special tokens file saved in ../../saves/LLaMA2-7B/full/04_sft/checkpoint-300/special_tokens_map.json
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1879: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 90%|████████▉ | 301/336 [55:47<17:16, 29.62s/it] 90%|████████▉ | 302/336 [55:57<13:26, 23.71s/it] 90%|█████████ | 303/336 [56:07<10:47, 19.63s/it] 90%|█████████ | 304/336 [56:18<09:00, 16.91s/it] 91%|█████████ | 305/336 [56:29<07:47, 15.08s/it] 91%|█████████ | 306/336 [56:39<06:52, 13.75s/it] 91%|█████████▏| 307/336 [56:50<06:13, 12.87s/it] 92%|█████████▏| 308/336 [57:01<05:39, 12.13s/it] 92%|█████████▏| 309/336 [57:11<05:13, 11.61s/it] 92%|█████████▏| 310/336 [57:22<04:59, 11.51s/it]                                                  92%|█████████▏| 310/336 [57:22<04:59, 11.51s/it] 93%|█████████▎| 311/336 [57:33<04:42, 11.28s/it] 93%|█████████▎| 312/336 [57:43<04:23, 10.98s/it] 93%|█████████▎| 313/336 [57:53<04:06, 10.71s/it] 93%|█████████▎| 314/336 [58:04<03:54, 10.66s/it] 94%|█████████▍| 315/336 [58:15<03:43, 10.66s/it] 94%|█████████▍| 316/336 [58:25<03:30, 10.54s/it] 94%|█████████▍| 317/336 [58:36<03:21, 10.61s/it] 95%|█████████▍| 318/336 [58:46<03:09, 10.51s/it] 95%|█████████▍| 319/336 [58:56<02:57, 10.42s/it] 95%|█████████▌| 320/336 [59:07<02:48, 10.55s/it]                                                  95%|█████████▌| 320/336 [59:07<02:48, 10.55s/it] 96%|█████████▌| 321/336 [59:18<02:38, 10.58s/it] 96%|█████████▌| 322/336 [59:28<02:28, 10.60s/it] 96%|█████████▌| 323/336 [59:39<02:18, 10.63s/it] 96%|█████████▋| 324/336 [59:50<02:07, 10.64s/it] 97%|█████████▋| 325/336 [1:00:00<01:56, 10.56s/it] 97%|█████████▋| 326/336 [1:00:10<01:44, 10.45s/it] 97%|█████████▋| 327/336 [1:00:21<01:34, 10.53s/it] 98%|█████████▊| 328/336 [1:00:32<01:25, 10.65s/it] 98%|█████████▊| 329/336 [1:00:42<01:13, 10.56s/it] 98%|█████████▊| 330/336 [1:00:53<01:03, 10.56s/it]                                                    98%|█████████▊| 330/336 [1:00:53<01:03, 10.56s/it] 99%|█████████▊| 331/336 [1:01:03<00:52, 10.49s/it] 99%|█████████▉| 332/336 [1:01:14<00:42, 10.51s/it] 99%|█████████▉| 333/336 [1:01:24<00:31, 10.52s/it] 99%|█████████▉| 334/336 [1:01:35<00:21, 10.54s/it]100%|█████████▉| 335/336 [1:01:45<00:10, 10.40s/it]100%|██████████| 336/336 [1:01:55<00:00, 10.39s/it][INFO|trainer.py:2231] 2024-04-07 19:24:53,172 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   100%|██████████| 336/336 [1:01:55<00:00, 10.39s/it]100%|██████████| 336/336 [1:01:55<00:00, 11.06s/it]
[INFO|trainer.py:3203] 2024-04-07 19:24:58,718 >> Saving model checkpoint to ../../saves/LLaMA2-7B/full/04_sft
[INFO|configuration_utils.py:471] 2024-04-07 19:24:58,720 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/config.json
[INFO|configuration_utils.py:697] 2024-04-07 19:24:58,721 >> Configuration saved in ../../saves/LLaMA2-7B/full/04_sft/generation_config.json
[INFO|modeling_utils.py:2482] 2024-04-07 19:25:11,549 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at ../../saves/LLaMA2-7B/full/04_sft/model.safetensors.index.json.
[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:25:11,550 >> tokenizer config file saved in ../../saves/LLaMA2-7B/full/04_sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:25:11,550 >> Special tokens file saved in ../../saves/LLaMA2-7B/full/04_sft/special_tokens_map.json
[INFO|trainer.py:3512] 2024-04-07 19:25:26,468 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:25:26,468 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:25:26,468 >>   Batch size = 1
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:00<00:08,  2.69it/s] 12%|█▏        | 3/25 [00:01<00:11,  1.94it/s] 16%|█▌        | 4/25 [00:02<00:12,  1.63it/s] 20%|██        | 5/25 [00:02<00:13,  1.53it/s] 24%|██▍       | 6/25 [00:03<00:13,  1.43it/s] 28%|██▊       | 7/25 [00:04<00:13,  1.37it/s] 32%|███▏      | 8/25 [00:05<00:12,  1.35it/s] 36%|███▌      | 9/25 [00:06<00:12,  1.33it/s] 40%|████      | 10/25 [00:06<00:11,  1.32it/s] 44%|████▍     | 11/25 [00:07<00:10,  1.32it/s] 48%|████▊     | 12/25 [00:08<00:09,  1.34it/s] 52%|█████▏    | 13/25 [00:09<00:09,  1.32it/s] 56%|█████▌    | 14/25 [00:09<00:08,  1.32it/s] 60%|██████    | 15/25 [00:10<00:07,  1.30it/s] 64%|██████▍   | 16/25 [00:11<00:06,  1.32it/s] 68%|██████▊   | 17/25 [00:12<00:06,  1.32it/s] 72%|███████▏  | 18/25 [00:12<00:05,  1.33it/s] 76%|███████▌  | 19/25 [00:13<00:04,  1.29it/s] 80%|████████  | 20/25 [00:14<00:03,  1.30it/s] 84%|████████▍ | 21/25 [00:15<00:03,  1.31it/s] 88%|████████▊ | 22/25 [00:16<00:02,  1.24it/s] 92%|█████████▏| 23/25 [00:16<00:01,  1.27it/s] 96%|█████████▌| 24/25 [00:17<00:00,  1.21it/s]100%|██████████| 25/25 [00:18<00:00,  1.22it/s]100%|██████████| 25/25 [00:18<00:00,  1.34it/s]
[INFO|modelcard.py:450] 2024-04-07 19:25:45,742 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
