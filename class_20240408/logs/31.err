[INFO|tokenization_utils_base.py:2084] 2024-04-07 23:03:32,151 >> loading file tokenizer.model from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-04-07 23:03:32,151 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-04-07 23:03:32,151 >> loading file special_tokens_map.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 23:03:32,151 >> loading file tokenizer_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer_config.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 23:03:32,151 >> loading file tokenizer.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.json
Converting format of dataset (num_proc=16):   0%|          | 0/20 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  30%|███       | 6/20 [00:00<00:00, 53.41 examples/s]Converting format of dataset (num_proc=16):  95%|█████████▌| 19/20 [00:00<00:00, 90.42 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 20/20 [00:00<00:00, 68.44 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/20 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 2/20 [00:00<00:01, 15.36 examples/s]Running tokenizer on dataset (num_proc=16):  60%|██████    | 12/20 [00:00<00:00, 58.08 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 20/20 [00:00<00:00, 53.72 examples/s]
[INFO|configuration_utils.py:726] 2024-04-07 23:03:34,716 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 23:03:34,718 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3283] 2024-04-07 23:03:34,761 >> loading weights file model.safetensors from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-04-07 23:03:34,762 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-07 23:03:34,763 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.43s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.59s/it]
[INFO|modeling_utils.py:4024] 2024-04-07 23:03:52,166 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-04-07 23:03:52,166 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 23:03:52,388 >> loading configuration file generation_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 23:03:52,389 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:3512] 2024-04-07 23:03:57,272 >> ***** Running Prediction *****
[INFO|trainer.py:3514] 2024-04-07 23:03:57,272 >>   Num examples = 20
[INFO|trainer.py:3517] 2024-04-07 23:03:57,272 >>   Batch size = 1
  0%|          | 0/20 [00:00<?, ?it/s] 10%|█         | 2/20 [00:06<00:59,  3.33s/it] 15%|█▌        | 3/20 [00:14<01:28,  5.21s/it] 20%|██        | 4/20 [00:21<01:31,  5.69s/it] 25%|██▌       | 5/20 [00:33<02:00,  8.05s/it] 30%|███       | 6/20 [00:41<01:53,  8.08s/it] 35%|███▌      | 7/20 [00:48<01:40,  7.71s/it] 40%|████      | 8/20 [00:59<01:43,  8.59s/it] 45%|████▌     | 9/20 [01:08<01:38,  8.99s/it] 50%|█████     | 10/20 [01:17<01:29,  8.93s/it] 55%|█████▌    | 11/20 [01:28<01:24,  9.34s/it] 60%|██████    | 12/20 [01:38<01:18,  9.80s/it] 65%|██████▌   | 13/20 [01:47<01:06,  9.48s/it] 70%|███████   | 14/20 [01:55<00:54,  9.08s/it] 75%|███████▌  | 15/20 [02:05<00:46,  9.39s/it] 80%|████████  | 16/20 [02:13<00:36,  9.00s/it] 85%|████████▌ | 17/20 [02:22<00:26,  8.95s/it] 90%|█████████ | 18/20 [02:37<00:21, 10.62s/it] 95%|█████████▌| 19/20 [02:46<00:10, 10.29s/it]100%|██████████| 20/20 [02:55<00:00,  9.72s/it]Building prefix dict from the default dictionary ...
Dumping model to file cache /tmp/jieba.cache
Loading model cost 0.547 seconds.
Prefix dict has been built successfully.
100%|██████████| 20/20 [02:56<00:00,  8.83s/it]
