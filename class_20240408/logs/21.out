[2024-04-07 20:29:45,541] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
04/07/2024 20:29:47 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16
04/07/2024 20:29:48 - INFO - llmtuner.data.template - Add pad token: </s>
04/07/2024 20:29:48 - INFO - llmtuner.data.loader - Loading dataset demo_cp.json...
04/07/2024 20:29:48 - WARNING - llmtuner.data.utils - Checksum failed: mismatched SHA-1 hash value at ../../data/demo_cp.json.
input_ids:
[29871, 231, 194, 135, 31741, 31824, 29906, 29900, 29906, 29906, 30470, 29906, 30534, 29906, 29946, 30325, 30753, 30806, 30752, 231, 193, 184, 234, 134, 146, 31510, 235, 155, 176, 31220, 30214, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 31238, 233, 137, 133, 30869, 30494, 234, 133, 189, 30557, 232, 131, 142, 30895, 233, 171, 156, 30214, 31570, 31149, 30591, 30636, 31698, 231, 194, 135, 30210, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 31290, 232, 178, 169, 235, 182, 173, 234, 144, 171, 30939, 30214, 31100, 30417, 231, 194, 135, 31741, 31824, 236, 170, 147, 31363, 30214, 31325, 235, 172, 181, 30533, 31561, 29906, 30534, 29906, 29947, 30325, 232, 179, 142, 31376, 231, 194, 135, 31741, 31824, 30481, 30982, 235, 176, 186, 30482, 31608, 30956, 233, 153, 191, 30601, 30636, 30210, 30666, 31785, 235, 143, 181, 30688, 31032, 236, 163, 155, 30533, 31688, 30899, 29896, 30325, 235, 176, 183, 235, 181, 175, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 30275, 232, 167, 177, 31014, 31260, 30481, 31656, 232, 166, 150, 30482, 30214, 231, 187, 169, 29871, 232, 179, 142, 31376, 29871, 231, 194, 135, 31741, 31824, 30210, 232, 144, 151, 31931, 30267, 13, 232, 179, 144, 233, 153, 191, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 30419, 4300, 29876, 391, 2849, 30409, 30210, 235, 174, 142, 31376, 30214, 231, 194, 135, 31741, 31824, 31066, 31398, 30636, 29906, 30534, 29906, 29947, 30325, 232, 134, 136, 30742, 233, 138, 140, 30383, 30481, 30982, 235, 176, 186, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 30313, 30855, 31107, 234, 158, 141, 30330, 30672, 232, 131, 148, 30210, 30980, 235, 134, 161, 30392, 30672, 232, 131, 148, 30210, 232, 135, 173, 31244, 30698, 232, 142, 156, 30577, 30287, 30267, 231, 194, 135, 31741, 31824, 30990, 236, 154, 159, 31540, 233, 170, 142, 30287, 31331, 232, 178, 172, 233, 136, 145, 233, 173, 165, 235, 169, 153, 30744, 30417, 235, 174, 142, 31376, 30267, 30482, 231, 194, 135, 31741, 31824, 234, 187, 192, 234, 184, 180, 233, 156, 177, 30675, 30419, 29963, 4528, 17216, 12065, 262, 30409, 30505, 29906, 30534, 29906, 29929, 30325, 30210, 233, 191, 151, 235, 173, 173, 30429, 30214, 30953, 233, 181, 149, 31302, 31436, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 30287, 30745, 30267, 13, 30413, 236, 132, 145, 231, 194, 135, 31741, 31824, 31066, 31398, 30636, 30899, 31543, 31176, 31741, 31176, 30419, 1748, 479, 29875, 23212, 10139, 30409, 29896, 30325, 30505, 31181, 235, 131, 182, 31149, 30670, 31831, 31107, 231, 189, 161, 31066, 31398, 31871, 232, 166, 138, 30419, 13448, 284, 3761, 28243, 4135, 29474, 30409, 30429, 30746, 30858, 30214, 231, 194, 135, 31741, 31824, 31290, 232, 132, 157, 30544, 30742, 233, 138, 140, 30214, 232, 144, 182, 30698, 31376, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 30419, 29924, 1025, 4273, 30409, 31661, 31733, 236, 155, 190, 233, 150, 142, 30481, 29945, 29974, 29906, 30482, 31411, 235, 174, 138, 30214, 231, 187, 169, 234, 171, 180, 31389, 31411, 235, 174, 138, 30392, 30505, 30982, 30946, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 236, 163, 155, 31181, 31366, 233, 152, 183, 30557, 30214, 235, 131, 134, 31180, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 30210, 31107, 234, 158, 141, 30267, 13, 30481, 29945, 29974, 29906, 30482, 31411, 235, 174, 138, 31272, 31272, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 30330, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 30330, 234, 134, 146, 31510, 235, 155, 176, 30330, 231, 194, 135, 31741, 31824, 30330, 233, 176, 147, 31952, 30670, 30753, 233, 157, 171, 30733, 30732, 234, 184, 135, 234, 188, 151, 30419, 3267, 4741, 30409, 30214, 30666, 30429, 233, 176, 147, 234, 158, 162, 31436, 30630, 30915, 233, 150, 151, 31450, 235, 170, 131, 232, 178, 162, 232, 150, 164, 233, 153, 191, 29906, 29900, 29896, 29896, 30470, 31404, 31020, 31965, 30448, 30214, 30878, 31830, 29896, 30936, 31411, 235, 174, 138, 30392, 30505, 29906, 29900, 29896, 29929, 30470, 232, 189, 152, 30214, 231, 192, 137, 31020, 234, 184, 133, 233, 181, 149, 30417, 31450, 31502, 30494, 30801, 30267, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 29906, 29900, 29906, 29941, 30470, 30746, 30592, 30417, 31474, 233, 132, 165, 232, 193, 172, 31389, 31411, 235, 174, 138, 30214, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 234, 171, 180, 31389, 31411, 235, 174, 138, 31290, 30413, 30946, 30505, 30267, 13, 232, 146, 169, 31066, 30214, 30980, 234, 133, 189, 30481, 29945, 29974, 29906, 30482, 31411, 235, 174, 138, 30494, 232, 150, 164, 30210, 234, 134, 146, 31510, 235, 155, 176, 232, 140, 138, 30746, 30592, 30214, 31557, 30698, 30417, 231, 194, 135, 31741, 31824, 30214, 31238, 30413, 31411, 232, 146, 134, 235, 139, 138, 30267, 231, 194, 135, 31741, 31824, 31838, 31694, 30525, 232, 173, 149, 236, 174, 151, 30866, 30915, 236, 157, 158, 232, 133, 182, 30848, 30564, 30843, 30419, 4074, 29888, 1165, 30409, 29896, 30325, 29871, 31302, 30780, 29871, 30214, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 30503, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 235, 174, 138, 31791, 30690, 30746, 29906, 29900, 29906, 29946, 30470, 29896, 30534, 232, 189, 152, 30417, 30505, 3267, 4741, 236, 170, 147, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 30690, 30746, 232, 159, 155, 31411, 30806, 30214, 235, 139, 138, 31411, 232, 147, 135, 30915, 30210, 31066, 31398, 30313, 232, 150, 164, 30769, 30417, 30544, 232, 187, 176, 30267, 13, 233, 148, 172, 234, 139, 193, 30923, 234, 150, 169, 30392, 232, 151, 178, 30287, 31295, 30503, 231, 194, 135, 31741, 31824, 31092, 232, 166, 167, 30210, 30658, 235, 155, 138, 235, 132, 178, 30666, 234, 158, 162, 31611, 30503, 30915, 30214, 231, 192, 137, 235, 139, 138, 234, 134, 146, 31510, 235, 155, 176, 30990, 236, 135, 179, 30210, 235, 132, 185, 31824, 31141, 30828, 30591, 232, 181, 187, 30533, 31561, 30505, 29896, 29929, 29929, 29896, 30470, 235, 155]
inputs:
俄羅斯2022年2月24日全面入侵烏克蘭後，摩爾多瓦就憂心成為下個目標，因其東部親俄的聶斯特河東岸地區已實質獨立，更有俄羅斯駐軍，而該地區2月28日尋求俄羅斯「保護」；位於南部的加告茲自治領地首長1日譴責摩爾多瓦中央政府「打壓」，並 尋求 俄羅斯的協助。
對於聶斯特河東岸地區（Transnistria）的請求，俄羅斯外交部2月28日僅回應：「保護聶斯特河東岸地區人民利益、我們的同胞是我們的優先要務之一。俄羅斯相關機構一向審慎檢視所有請求。」俄羅斯總統普京（Vladimir Putin）在2月29日的演說上，也沒提及聶斯特河東岸地區一事。
不過俄羅斯外交部長拉夫羅夫（Sergei Lavrov）1日在土耳其安塔利亞外交論壇（Antalya Diplomacy Forum）上表示，俄羅斯已做出回應，即要求摩爾多瓦（Moldova）別再阻擋「5+2」會談，並稱此會談是在保存摩爾多瓦領土完整下，考量聶斯特河東岸地區的利益。
「5+2」會談由由聶斯特河東岸地區、摩爾多瓦、烏克蘭、俄羅斯、歐洲安全暨合作組織（OSCE），加上歐盟及美國擔任觀察員於2011年開始進行，最近1次會談是在2019年底，但始終沒有任何成果。聶斯特河東岸地區2023年表明有意恢復此會談，摩爾多瓦稱此會談已不存在。
另外，同為「5+2」會談成員的烏克蘭則表明，只要有俄羅斯，就不會參與。俄羅斯非官方媒體《國際傳真社》（Interfax）1日 提到 ，摩爾多瓦和聶斯特河東岸地區談判代表2024年1月底有在OSCE駐摩爾多瓦代表團會面，與會各國的外交人員都有出席。
摩爾多瓦是唯一未和俄羅斯接壤的前蘇聯加盟共和國，但與烏克蘭相鄰的聶斯特河東岸地區在1991年��
04/07/2024 20:30:11 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.
04/07/2024 20:30:11 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA
04/07/2024 20:30:12 - INFO - llmtuner.model.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622
{'loss': 1.6284, 'grad_norm': 0.07783514261245728, 'learning_rate': 2.5e-05, 'epoch': 0.07}
{'loss': 1.6492, 'grad_norm': 0.09075106680393219, 'learning_rate': 5e-05, 'epoch': 0.14}
{'loss': 1.604, 'grad_norm': 0.11783061176538467, 'learning_rate': 4.9930426471784284e-05, 'epoch': 0.2}
{'loss': 1.6196, 'grad_norm': 0.1547887623310089, 'learning_rate': 4.97220931252034e-05, 'epoch': 0.27}
{'loss': 1.617, 'grad_norm': 0.20652790367603302, 'learning_rate': 4.937615951913468e-05, 'epoch': 0.34}
{'loss': 1.5919, 'grad_norm': 0.22202490270137787, 'learning_rate': 4.8894551079298334e-05, 'epoch': 0.41}
{'loss': 1.5595, 'grad_norm': 0.2182852178812027, 'learning_rate': 4.827994838156459e-05, 'epoch': 0.47}
{'loss': 1.5903, 'grad_norm': 0.23490741848945618, 'learning_rate': 4.7535772232184026e-05, 'epoch': 0.54}
{'loss': 1.537, 'grad_norm': 0.2502870261669159, 'learning_rate': 4.666616462798276e-05, 'epoch': 0.61}
{'loss': 1.5482, 'grad_norm': 0.27808037400245667, 'learning_rate': 4.567596570249578e-05, 'epoch': 0.68}
{'eval_loss': 1.5633543729782104, 'eval_runtime': 32.5019, 'eval_samples_per_second': 4.061, 'eval_steps_per_second': 4.061, 'epoch': 0.68}
{'loss': 1.5566, 'grad_norm': 0.27263274788856506, 'learning_rate': 4.457068678635361e-05, 'epoch': 0.74}
{'loss': 1.5909, 'grad_norm': 0.29567480087280273, 'learning_rate': 4.335647973186494e-05, 'epoch': 0.81}
{'loss': 1.545, 'grad_norm': 0.2655565142631531, 'learning_rate': 4.2040102672530977e-05, 'epoch': 0.88}
{'loss': 1.5704, 'grad_norm': 0.3057863414287567, 'learning_rate': 4.062888240807012e-05, 'epoch': 0.95}
{'loss': 1.5571, 'grad_norm': 0.29839736223220825, 'learning_rate': 3.9130673624313434e-05, 'epoch': 1.02}
{'loss': 1.5556, 'grad_norm': 0.2910688519477844, 'learning_rate': 3.755381517494808e-05, 'epoch': 1.08}
{'loss': 1.5454, 'grad_norm': 0.31637707352638245, 'learning_rate': 3.5907083668439595e-05, 'epoch': 1.15}
{'loss': 1.5283, 'grad_norm': 0.3549622595310211, 'learning_rate': 3.419964461846252e-05, 'epoch': 1.22}
{'loss': 1.5664, 'grad_norm': 0.3317941129207611, 'learning_rate': 3.244100142973047e-05, 'epoch': 1.29}
{'loss': 1.5608, 'grad_norm': 0.36772608757019043, 'learning_rate': 3.064094250316446e-05, 'epoch': 1.35}
{'eval_loss': 1.5495514869689941, 'eval_runtime': 32.4764, 'eval_samples_per_second': 4.064, 'eval_steps_per_second': 4.064, 'epoch': 1.35}
{'loss': 1.5275, 'grad_norm': 0.3543001711368561, 'learning_rate': 2.8809486754805877e-05, 'epoch': 1.42}
{'loss': 1.5226, 'grad_norm': 0.35102567076683044, 'learning_rate': 2.6956827851709467e-05, 'epoch': 1.49}
{'loss': 1.4992, 'grad_norm': 0.36016780138015747, 'learning_rate': 2.5093277475192706e-05, 'epoch': 1.56}
{'loss': 1.5525, 'grad_norm': 0.36803585290908813, 'learning_rate': 2.322920792723178e-05, 'epoch': 1.62}
{'loss': 1.5206, 'grad_norm': 0.3684661090373993, 'learning_rate': 2.137499439944995e-05, 'epoch': 1.69}
{'loss': 1.5457, 'grad_norm': 0.376741498708725, 'learning_rate': 1.95409572260227e-05, 'epoch': 1.76}
{'loss': 1.539, 'grad_norm': 0.3772610127925873, 'learning_rate': 1.7737304441912736e-05, 'epoch': 1.83}
{'loss': 1.541, 'grad_norm': 0.3752709925174713, 'learning_rate': 1.5974074966149387e-05, 'epoch': 1.9}
{'loss': 1.5356, 'grad_norm': 0.37095507979393005, 'learning_rate': 1.4261082726387279e-05, 'epoch': 1.96}
{'loss': 1.531, 'grad_norm': 0.38661104440689087, 'learning_rate': 1.2607862035740536e-05, 'epoch': 2.03}
{'eval_loss': 1.544027328491211, 'eval_runtime': 32.4749, 'eval_samples_per_second': 4.065, 'eval_steps_per_second': 4.065, 'epoch': 2.03}
{'loss': 1.5281, 'grad_norm': 0.38673561811447144, 'learning_rate': 1.1023614525918551e-05, 'epoch': 2.1}
{'loss': 1.5416, 'grad_norm': 0.4219439625740051, 'learning_rate': 9.51715793202734e-06, 'epoch': 2.17}
{'loss': 1.5138, 'grad_norm': 0.40480539202690125, 'learning_rate': 8.096877014094182e-06, 'epoch': 2.23}
{'loss': 1.5419, 'grad_norm': 0.4086528718471527, 'learning_rate': 6.770676888480518e-06, 'epoch': 2.3}
{'loss': 1.5501, 'grad_norm': 0.40750640630722046, 'learning_rate': 5.545939028935068e-06, 'epoch': 2.37}
{'loss': 1.5761, 'grad_norm': 0.41703367233276367, 'learning_rate': 4.429480182180071e-06, 'epoch': 2.44}
{'loss': 1.5292, 'grad_norm': 0.4056072533130646, 'learning_rate': 3.427514426701647e-06, 'epoch': 2.5}
{'loss': 1.5318, 'grad_norm': 0.4083805978298187, 'learning_rate': 2.5456185859205865e-06, 'epoch': 2.57}
{'loss': 1.5282, 'grad_norm': 0.4310603737831116, 'learning_rate': 1.7887011882498362e-06, 'epoch': 2.64}
{'loss': 1.5132, 'grad_norm': 0.41453829407691956, 'learning_rate': 1.1609751468033115e-06, 'epoch': 2.71}
{'eval_loss': 1.542891502380371, 'eval_runtime': 32.5133, 'eval_samples_per_second': 4.06, 'eval_steps_per_second': 4.06, 'epoch': 2.71}
{'loss': 1.51, 'grad_norm': 0.40492627024650574, 'learning_rate': 6.65934310817512e-07, 'epoch': 2.77}
{'loss': 1.4802, 'grad_norm': 0.3814353048801422, 'learning_rate': 3.063340192980685e-07, 'epoch': 2.84}
{'loss': 1.5316, 'grad_norm': 0.4020698666572571, 'learning_rate': 8.417576512725622e-08, 'epoch': 2.91}
{'loss': 1.5217, 'grad_norm': 0.41199183464050293, 'learning_rate': 6.960549902651492e-10, 'epoch': 2.98}
{'train_runtime': 3001.406, 'train_samples_per_second': 1.181, 'train_steps_per_second': 0.147, 'train_loss': 1.550663456354552, 'epoch': 2.98}
***** train metrics *****
  epoch                    =       2.98
  train_loss               =     1.5507
  train_runtime            = 0:50:01.40
  train_samples_per_second =      1.181
  train_steps_per_second   =      0.147
Figure saved at: ../../saves/LLaMA2-7B/lora/pretrain/training_loss.png
Figure saved at: ../../saves/LLaMA2-7B/lora/pretrain/training_eval_loss.png
***** eval metrics *****
  epoch                   =       2.98
  eval_loss               =     1.5429
  eval_runtime            = 0:00:32.48
  eval_samples_per_second =      4.063
  eval_steps_per_second   =      4.063
  perplexity              =     4.6781
