[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file tokenizer.model from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file special_tokens_map.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file tokenizer_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer_config.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file tokenizer.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.json
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  19%|â–ˆâ–‰        | 189/1000 [00:00<00:00, 1759.82 examples/s]Converting format of dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [00:00<00:00, 4635.68 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 3563.62 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:09, 99.19 examples/s]Running tokenizer on dataset (num_proc=16):  13%|â–ˆâ–Ž        | 126/1000 [00:00<00:04, 193.46 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:00<00:01, 502.34 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 627/1000 [00:00<00:00, 1046.56 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 920.13 examples/s]
[INFO|configuration_utils.py:726] 2024-04-07 19:32:12,772 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:32:12,775 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3283] 2024-04-07 19:32:12,803 >> loading weights file model.safetensors from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-04-07 19:32:12,804 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-07 19:32:12,805 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:15<00:15, 15.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  7.59s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:17<00:00,  8.77s/it]
[INFO|modeling_utils.py:4024] 2024-04-07 19:32:30,564 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-04-07 19:32:30,565 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 19:32:30,794 >> loading configuration file generation_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 19:32:30,795 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-04-07 19:32:31,003 >> Using auto half precision backend
[INFO|trainer.py:1969] 2024-04-07 19:32:31,191 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-04-07 19:32:31,191 >>   Num examples = 900
[INFO|trainer.py:1971] 2024-04-07 19:32:31,191 >>   Num Epochs = 1
[INFO|trainer.py:1972] 2024-04-07 19:32:31,191 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-04-07 19:32:31,191 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-04-07 19:32:31,191 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1977] 2024-04-07 19:32:31,191 >>   Total optimization steps = 112
[INFO|trainer.py:1978] 2024-04-07 19:32:31,193 >>   Number of trainable parameters = 4,194,304
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:09<17:58,  9.72s/it]  2%|â–         | 2/112 [00:17<15:48,  8.62s/it]  3%|â–Ž         | 3/112 [00:23<13:23,  7.37s/it]  4%|â–Ž         | 4/112 [00:30<13:12,  7.34s/it]  4%|â–         | 5/112 [00:38<13:09,  7.38s/it]  5%|â–Œ         | 6/112 [00:43<11:59,  6.78s/it]  6%|â–‹         | 7/112 [00:52<12:47,  7.31s/it]  7%|â–‹         | 8/112 [01:01<13:38,  7.87s/it]  8%|â–Š         | 9/112 [01:08<13:03,  7.60s/it]  9%|â–‰         | 10/112 [01:13<11:54,  7.01s/it]                                                  9%|â–‰         | 10/112 [01:13<11:54,  7.01s/it] 10%|â–‰         | 11/112 [01:21<11:59,  7.12s/it] 11%|â–ˆ         | 12/112 [01:26<11:04,  6.64s/it] 12%|â–ˆâ–        | 13/112 [01:34<11:16,  6.83s/it] 12%|â–ˆâ–Ž        | 14/112 [01:40<11:05,  6.79s/it] 13%|â–ˆâ–Ž        | 15/112 [01:48<11:28,  7.10s/it] 14%|â–ˆâ–        | 16/112 [01:54<10:35,  6.62s/it] 15%|â–ˆâ–Œ        | 17/112 [02:00<10:20,  6.53s/it] 16%|â–ˆâ–Œ        | 18/112 [02:06<10:00,  6.39s/it] 17%|â–ˆâ–‹        | 19/112 [02:14<10:29,  6.77s/it] 18%|â–ˆâ–Š        | 20/112 [02:22<10:53,  7.11s/it]                                                 18%|â–ˆâ–Š        | 20/112 [02:22<10:53,  7.11s/it] 19%|â–ˆâ–‰        | 21/112 [02:29<11:01,  7.27s/it] 20%|â–ˆâ–‰        | 22/112 [02:39<12:09,  8.10s/it] 21%|â–ˆâ–ˆ        | 23/112 [02:45<11:01,  7.44s/it] 21%|â–ˆâ–ˆâ–       | 24/112 [02:53<10:55,  7.45s/it] 22%|â–ˆâ–ˆâ–       | 25/112 [02:59<10:28,  7.23s/it] 23%|â–ˆâ–ˆâ–Ž       | 26/112 [03:08<10:51,  7.58s/it] 24%|â–ˆâ–ˆâ–       | 27/112 [03:15<10:43,  7.58s/it] 25%|â–ˆâ–ˆâ–Œ       | 28/112 [03:24<11:14,  8.04s/it] 26%|â–ˆâ–ˆâ–Œ       | 29/112 [03:32<10:49,  7.82s/it] 27%|â–ˆâ–ˆâ–‹       | 30/112 [03:40<10:43,  7.85s/it]                                                 27%|â–ˆâ–ˆâ–‹       | 30/112 [03:40<10:43,  7.85s/it] 28%|â–ˆâ–ˆâ–Š       | 31/112 [03:46<09:58,  7.38s/it] 29%|â–ˆâ–ˆâ–Š       | 32/112 [03:53<09:44,  7.31s/it] 29%|â–ˆâ–ˆâ–‰       | 33/112 [04:00<09:17,  7.05s/it] 30%|â–ˆâ–ˆâ–ˆ       | 34/112 [04:08<09:42,  7.47s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 35/112 [04:13<08:45,  6.83s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 36/112 [04:22<09:12,  7.27s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 37/112 [04:30<09:30,  7.60s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 38/112 [04:36<08:46,  7.12s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 39/112 [04:41<08:00,  6.58s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 40/112 [04:48<08:04,  6.73s/it]                                                 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 40/112 [04:48<08:04,  6.73s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 41/112 [04:55<07:44,  6.55s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 42/112 [05:03<08:26,  7.23s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 43/112 [05:12<08:37,  7.50s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 44/112 [05:18<08:11,  7.22s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 45/112 [05:26<08:15,  7.39s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 46/112 [05:32<07:45,  7.05s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 47/112 [05:39<07:34,  6.99s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 48/112 [05:46<07:32,  7.08s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 49/112 [05:52<07:06,  6.77s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/112 [05:59<06:51,  6.64s/it]                                                 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 50/112 [05:59<06:51,  6.64s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 51/112 [06:04<06:25,  6.32s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 52/112 [06:11<06:32,  6.54s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 53/112 [06:19<06:40,  6.78s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 54/112 [06:26<06:52,  7.10s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 55/112 [06:32<06:18,  6.64s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 56/112 [06:38<06:07,  6.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 57/112 [06:47<06:40,  7.29s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 58/112 [06:56<06:51,  7.62s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 59/112 [07:04<06:47,  7.68s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 60/112 [07:09<05:59,  6.91s/it]                                                 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 60/112 [07:09<05:59,  6.91s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 61/112 [07:13<05:15,  6.18s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 62/112 [07:21<05:30,  6.60s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 63/112 [07:29<05:45,  7.06s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 64/112 [07:35<05:19,  6.65s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 65/112 [07:44<05:54,  7.54s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 66/112 [07:50<05:26,  7.11s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 67/112 [07:56<05:05,  6.79s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 68/112 [08:02<04:49,  6.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 69/112 [08:10<04:54,  6.85s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 70/112 [08:15<04:27,  6.36s/it]                                                 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 70/112 [08:15<04:27,  6.36s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 71/112 [08:21<04:16,  6.26s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 72/112 [08:27<03:59,  5.98s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 73/112 [08:33<03:53,  5.98s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 74/112 [08:43<04:39,  7.35s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 75/112 [08:48<04:04,  6.61s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 76/112 [08:56<04:14,  7.06s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 77/112 [09:02<03:52,  6.64s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 78/112 [09:08<03:37,  6.39s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 79/112 [09:16<03:47,  6.90s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/112 [09:21<03:27,  6.48s/it]                                                 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/112 [09:21<03:27,  6.48s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 81/112 [09:27<03:18,  6.39s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 82/112 [09:35<03:24,  6.80s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 83/112 [09:43<03:23,  7.03s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 84/112 [09:51<03:30,  7.52s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 85/112 [09:57<03:10,  7.05s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 86/112 [10:03<02:54,  6.72s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 87/112 [10:09<02:42,  6.51s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 88/112 [10:17<02:42,  6.75s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 89/112 [10:24<02:37,  6.83s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 90/112 [10:31<02:37,  7.16s/it]                                                 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 90/112 [10:31<02:37,  7.16s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 91/112 [10:39<02:32,  7.24s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 92/112 [10:43<02:08,  6.40s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 93/112 [10:50<02:05,  6.59s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 94/112 [10:58<02:02,  6.82s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 95/112 [11:04<01:51,  6.55s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 96/112 [11:11<01:47,  6.74s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 97/112 [11:16<01:34,  6.30s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 98/112 [11:24<01:33,  6.71s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 99/112 [11:30<01:26,  6.63s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 100/112 [11:36<01:14,  6.25s/it]                                                  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 100/112 [11:36<01:14,  6.25s/it][INFO|trainer.py:3512] 2024-04-07 19:44:07,243 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:44:07,243 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:44:07,243 >>   Batch size = 1

  0%|          | 0/100 [00:00<?, ?it/s][A
  2%|â–         | 2/100 [00:00<00:12,  7.72it/s][A
  3%|â–Ž         | 3/100 [00:00<00:18,  5.12it/s][A
  4%|â–         | 4/100 [00:00<00:17,  5.63it/s][A
  5%|â–Œ         | 5/100 [00:01<00:22,  4.22it/s][A
  6%|â–Œ         | 6/100 [00:01<00:33,  2.79it/s][A
  7%|â–‹         | 7/100 [00:01<00:28,  3.30it/s][A
  8%|â–Š         | 8/100 [00:02<00:36,  2.49it/s][A
  9%|â–‰         | 9/100 [00:02<00:30,  3.01it/s][A
 10%|â–ˆ         | 10/100 [00:02<00:25,  3.54it/s][A
 11%|â–ˆ         | 11/100 [00:03<00:26,  3.40it/s][A
 12%|â–ˆâ–        | 12/100 [00:03<00:25,  3.43it/s][A
 13%|â–ˆâ–Ž        | 13/100 [00:04<00:34,  2.50it/s][A
 14%|â–ˆâ–        | 14/100 [00:04<00:31,  2.76it/s][A
 15%|â–ˆâ–Œ        | 15/100 [00:04<00:26,  3.17it/s][A
 16%|â–ˆâ–Œ        | 16/100 [00:04<00:23,  3.63it/s][A
 17%|â–ˆâ–‹        | 17/100 [00:05<00:23,  3.55it/s][A
 18%|â–ˆâ–Š        | 18/100 [00:05<00:24,  3.38it/s][A
 19%|â–ˆâ–‰        | 19/100 [00:05<00:25,  3.16it/s][A
 20%|â–ˆâ–ˆ        | 20/100 [00:06<00:26,  2.97it/s][A
 21%|â–ˆâ–ˆ        | 21/100 [00:06<00:26,  3.01it/s][A
 22%|â–ˆâ–ˆâ–       | 22/100 [00:06<00:27,  2.87it/s][A
 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:07<00:30,  2.49it/s][A
 24%|â–ˆâ–ˆâ–       | 24/100 [00:07<00:27,  2.73it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:07<00:25,  2.94it/s][A
 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:08<00:23,  3.14it/s][A
 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:08<00:30,  2.39it/s][A
 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:09<00:27,  2.66it/s][A
 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:09<00:27,  2.55it/s][A
 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:10<00:31,  2.20it/s][A
 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:10<00:26,  2.61it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:10<00:23,  2.89it/s][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:10<00:21,  3.08it/s][A
 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:11<00:27,  2.40it/s][A
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:12<00:30,  2.10it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:12<00:27,  2.31it/s][A
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:13<00:31,  2.03it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:13<00:24,  2.51it/s][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:13<00:20,  3.01it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:13<00:17,  3.37it/s][A
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:13<00:17,  3.40it/s][A
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:14<00:21,  2.71it/s][A
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:14<00:21,  2.67it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:15<00:18,  3.08it/s][A
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:15<00:15,  3.55it/s][A
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:15<00:14,  3.83it/s][A
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:15<00:14,  3.58it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:15<00:12,  4.18it/s][A
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:16<00:14,  3.54it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:16<00:13,  3.60it/s][A
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:16<00:14,  3.40it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:17<00:12,  3.84it/s][A
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:17<00:17,  2.73it/s][A
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:17<00:14,  3.07it/s][A
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:18<00:16,  2.66it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:18<00:13,  3.17it/s][A
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:18<00:13,  3.21it/s][A
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:19<00:11,  3.68it/s][A
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:19<00:11,  3.44it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:19<00:13,  2.98it/s][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:20<00:12,  3.17it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:20<00:13,  2.86it/s][A
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:20<00:12,  2.93it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:21<00:11,  3.05it/s][A
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:21<00:10,  3.21it/s][A
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:21<00:10,  3.34it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:22<00:13,  2.49it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:22<00:11,  2.71it/s][A
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:22<00:09,  3.20it/s][A
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:23<00:09,  3.01it/s][A
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:23<00:08,  3.39it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:23<00:08,  3.44it/s][A
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:24<00:10,  2.50it/s][A
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:24<00:08,  2.92it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:24<00:08,  2.91it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:25<00:08,  2.93it/s][A
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:25<00:07,  2.89it/s][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:25<00:07,  2.92it/s][A
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:26<00:08,  2.59it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:26<00:06,  3.07it/s][A
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:26<00:05,  3.52it/s][A
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:27<00:04,  3.78it/s][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:27<00:04,  3.65it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:27<00:04,  3.56it/s][A
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:27<00:04,  3.33it/s][A
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:28<00:03,  3.64it/s][A
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:28<00:03,  3.33it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:29<00:04,  2.78it/s][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:29<00:03,  3.14it/s][A
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:29<00:02,  3.59it/s][A
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:29<00:03,  2.81it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:30<00:02,  2.96it/s][A
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:30<00:02,  2.93it/s][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:30<00:01,  3.39it/s][A
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:31<00:01,  2.74it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:31<00:01,  2.94it/s][A
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:31<00:00,  3.42it/s][A
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:32<00:00,  3.44it/s][A
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:32<00:00,  3.16it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:32<00:00,  3.63it/s][A                                                 
                                                 [A 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 100/112 [12:09<01:14,  6.25s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:32<00:00,  3.63it/s][A
                                                 [A[INFO|trainer.py:3203] 2024-04-07 19:44:40,364 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100
[INFO|configuration_utils.py:726] 2024-04-07 19:44:41,329 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:44:41,331 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:44:41,453 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:44:41,454 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100/special_tokens_map.json
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 101/112 [12:16<03:02, 16.64s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 102/112 [12:22<02:12, 13.26s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 103/112 [12:31<01:49, 12.16s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 104/112 [12:37<01:20, 10.12s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 105/112 [12:43<01:02,  8.94s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 106/112 [12:49<00:48,  8.03s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 107/112 [12:56<00:38,  7.77s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 108/112 [13:01<00:27,  6.96s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 109/112 [13:08<00:20,  6.93s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 110/112 [13:13<00:12,  6.44s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 110/112 [13:13<00:12,  6.44s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 111/112 [13:20<00:06,  6.43s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [13:26<00:00,  6.50s/it][INFO|trainer.py:2231] 2024-04-07 19:45:58,017 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2436] 2024-04-07 19:45:58,017 >> Loading best model from ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100 (score: 1.7525826692581177).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [13:26<00:00,  6.50s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [13:26<00:00,  7.20s/it]
[INFO|trainer.py:3203] 2024-04-07 19:45:58,061 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/orpo
[INFO|configuration_utils.py:726] 2024-04-07 19:45:58,523 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:45:58,525 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:45:58,607 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/orpo/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:45:58,608 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/orpo/special_tokens_map.json
[INFO|trainer.py:3512] 2024-04-07 19:46:17,460 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:46:17,460 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:46:17,460 >>   Batch size = 1
  0%|          | 0/100 [00:00<?, ?it/s]  2%|â–         | 2/100 [00:00<00:12,  7.54it/s]  3%|â–Ž         | 3/100 [00:00<00:18,  5.11it/s]  4%|â–         | 4/100 [00:00<00:17,  5.60it/s]  5%|â–Œ         | 5/100 [00:01<00:22,  4.21it/s]  6%|â–Œ         | 6/100 [00:01<00:33,  2.79it/s]  7%|â–‹         | 7/100 [00:01<00:28,  3.30it/s]  8%|â–Š         | 8/100 [00:02<00:37,  2.46it/s]  9%|â–‰         | 9/100 [00:02<00:30,  3.00it/s] 10%|â–ˆ         | 10/100 [00:02<00:25,  3.52it/s] 11%|â–ˆ         | 11/100 [00:03<00:26,  3.38it/s] 12%|â–ˆâ–        | 12/100 [00:03<00:25,  3.40it/s] 13%|â–ˆâ–Ž        | 13/100 [00:04<00:34,  2.49it/s] 14%|â–ˆâ–        | 14/100 [00:04<00:31,  2.74it/s] 15%|â–ˆâ–Œ        | 15/100 [00:04<00:26,  3.18it/s] 16%|â–ˆâ–Œ        | 16/100 [00:04<00:23,  3.64it/s] 17%|â–ˆâ–‹        | 17/100 [00:05<00:23,  3.59it/s] 18%|â–ˆâ–Š        | 18/100 [00:05<00:24,  3.38it/s] 19%|â–ˆâ–‰        | 19/100 [00:05<00:25,  3.20it/s] 20%|â–ˆâ–ˆ        | 20/100 [00:06<00:26,  2.98it/s] 21%|â–ˆâ–ˆ        | 21/100 [00:06<00:26,  3.03it/s] 22%|â–ˆâ–ˆâ–       | 22/100 [00:06<00:27,  2.87it/s] 23%|â–ˆâ–ˆâ–Ž       | 23/100 [00:07<00:31,  2.46it/s] 24%|â–ˆâ–ˆâ–       | 24/100 [00:07<00:28,  2.66it/s] 25%|â–ˆâ–ˆâ–Œ       | 25/100 [00:07<00:26,  2.88it/s] 26%|â–ˆâ–ˆâ–Œ       | 26/100 [00:08<00:23,  3.10it/s] 27%|â–ˆâ–ˆâ–‹       | 27/100 [00:08<00:30,  2.40it/s] 28%|â–ˆâ–ˆâ–Š       | 28/100 [00:09<00:26,  2.68it/s] 29%|â–ˆâ–ˆâ–‰       | 29/100 [00:09<00:27,  2.56it/s] 30%|â–ˆâ–ˆâ–ˆ       | 30/100 [00:10<00:31,  2.19it/s] 31%|â–ˆâ–ˆâ–ˆ       | 31/100 [00:10<00:26,  2.61it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 32/100 [00:10<00:23,  2.89it/s] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 33/100 [00:10<00:21,  3.07it/s] 34%|â–ˆâ–ˆâ–ˆâ–      | 34/100 [00:11<00:27,  2.41it/s] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 35/100 [00:12<00:31,  2.10it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 36/100 [00:12<00:27,  2.31it/s] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 37/100 [00:13<00:30,  2.05it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 38/100 [00:13<00:24,  2.53it/s] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 39/100 [00:13<00:20,  3.02it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 40/100 [00:13<00:18,  3.32it/s] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 41/100 [00:13<00:17,  3.39it/s] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/100 [00:14<00:21,  2.71it/s] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 43/100 [00:14<00:21,  2.69it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 44/100 [00:15<00:18,  3.09it/s] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 45/100 [00:15<00:15,  3.59it/s] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 46/100 [00:15<00:14,  3.85it/s] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 47/100 [00:15<00:14,  3.60it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 48/100 [00:15<00:12,  4.21it/s] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 49/100 [00:16<00:14,  3.57it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 50/100 [00:16<00:13,  3.64it/s] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 51/100 [00:16<00:14,  3.42it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/100 [00:17<00:12,  3.87it/s] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 53/100 [00:17<00:17,  2.75it/s] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 54/100 [00:17<00:14,  3.11it/s] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 55/100 [00:18<00:16,  2.65it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 56/100 [00:18<00:13,  3.16it/s] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 57/100 [00:18<00:13,  3.20it/s] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 58/100 [00:19<00:11,  3.64it/s] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 59/100 [00:19<00:12,  3.40it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 60/100 [00:19<00:13,  2.96it/s] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 61/100 [00:20<00:12,  3.15it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 62/100 [00:20<00:13,  2.82it/s] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 63/100 [00:20<00:12,  2.88it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 64/100 [00:21<00:11,  3.01it/s] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 65/100 [00:21<00:11,  3.17it/s] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 66/100 [00:21<00:10,  3.34it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 67/100 [00:22<00:13,  2.52it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 68/100 [00:22<00:11,  2.75it/s] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 69/100 [00:22<00:09,  3.23it/s] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 70/100 [00:23<00:09,  3.02it/s] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 71/100 [00:23<00:08,  3.37it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 72/100 [00:23<00:08,  3.48it/s] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 73/100 [00:24<00:10,  2.53it/s] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 74/100 [00:24<00:08,  2.96it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 75/100 [00:24<00:08,  2.94it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:25<00:08,  2.94it/s] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 77/100 [00:25<00:07,  2.90it/s] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 78/100 [00:25<00:07,  2.96it/s] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 79/100 [00:26<00:08,  2.62it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 80/100 [00:26<00:06,  3.07it/s] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 81/100 [00:26<00:05,  3.49it/s] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 82/100 [00:27<00:04,  3.72it/s] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 83/100 [00:27<00:04,  3.60it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 84/100 [00:27<00:04,  3.53it/s] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 85/100 [00:27<00:04,  3.36it/s] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 86/100 [00:28<00:03,  3.68it/s] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 87/100 [00:28<00:03,  3.35it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 88/100 [00:29<00:04,  2.80it/s] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 89/100 [00:29<00:03,  3.18it/s] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 90/100 [00:29<00:02,  3.65it/s] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 91/100 [00:29<00:03,  2.84it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 92/100 [00:30<00:02,  2.99it/s] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 93/100 [00:30<00:02,  2.96it/s] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 94/100 [00:30<00:01,  3.40it/s] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 95/100 [00:31<00:01,  2.72it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 96/100 [00:31<00:01,  2.96it/s] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 97/100 [00:31<00:00,  3.47it/s] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 98/100 [00:32<00:00,  3.46it/s] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 99/100 [00:32<00:00,  3.17it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:32<00:00,  3.62it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:32<00:00,  3.07it/s]
[INFO|modelcard.py:450] 2024-04-07 19:46:50,592 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
