[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file tokenizer.model from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file special_tokens_map.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file tokenizer_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer_config.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:32:09,445 >> loading file tokenizer.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.json
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  19%|█▉        | 189/1000 [00:00<00:00, 1759.82 examples/s]Converting format of dataset (num_proc=16):  88%|████████▊ | 876/1000 [00:00<00:00, 4635.68 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 3563.62 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:09, 99.19 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 126/1000 [00:00<00:04, 193.46 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:00<00:01, 502.34 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 627/1000 [00:00<00:00, 1046.56 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:01<00:00, 920.13 examples/s]
[INFO|configuration_utils.py:726] 2024-04-07 19:32:12,772 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:32:12,775 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3283] 2024-04-07 19:32:12,803 >> loading weights file model.safetensors from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-04-07 19:32:12,804 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-07 19:32:12,805 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:15<00:15, 15.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  7.59s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.77s/it]
[INFO|modeling_utils.py:4024] 2024-04-07 19:32:30,564 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-04-07 19:32:30,565 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 19:32:30,794 >> loading configuration file generation_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 19:32:30,795 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-04-07 19:32:31,003 >> Using auto half precision backend
[INFO|trainer.py:1969] 2024-04-07 19:32:31,191 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-04-07 19:32:31,191 >>   Num examples = 900
[INFO|trainer.py:1971] 2024-04-07 19:32:31,191 >>   Num Epochs = 1
[INFO|trainer.py:1972] 2024-04-07 19:32:31,191 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-04-07 19:32:31,191 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-04-07 19:32:31,191 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:1977] 2024-04-07 19:32:31,191 >>   Total optimization steps = 112
[INFO|trainer.py:1978] 2024-04-07 19:32:31,193 >>   Number of trainable parameters = 4,194,304
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:09<17:58,  9.72s/it]  2%|▏         | 2/112 [00:17<15:48,  8.62s/it]  3%|▎         | 3/112 [00:23<13:23,  7.37s/it]  4%|▎         | 4/112 [00:30<13:12,  7.34s/it]  4%|▍         | 5/112 [00:38<13:09,  7.38s/it]  5%|▌         | 6/112 [00:43<11:59,  6.78s/it]  6%|▋         | 7/112 [00:52<12:47,  7.31s/it]  7%|▋         | 8/112 [01:01<13:38,  7.87s/it]  8%|▊         | 9/112 [01:08<13:03,  7.60s/it]  9%|▉         | 10/112 [01:13<11:54,  7.01s/it]                                                  9%|▉         | 10/112 [01:13<11:54,  7.01s/it] 10%|▉         | 11/112 [01:21<11:59,  7.12s/it] 11%|█         | 12/112 [01:26<11:04,  6.64s/it] 12%|█▏        | 13/112 [01:34<11:16,  6.83s/it] 12%|█▎        | 14/112 [01:40<11:05,  6.79s/it] 13%|█▎        | 15/112 [01:48<11:28,  7.10s/it] 14%|█▍        | 16/112 [01:54<10:35,  6.62s/it] 15%|█▌        | 17/112 [02:00<10:20,  6.53s/it] 16%|█▌        | 18/112 [02:06<10:00,  6.39s/it] 17%|█▋        | 19/112 [02:14<10:29,  6.77s/it] 18%|█▊        | 20/112 [02:22<10:53,  7.11s/it]                                                 18%|█▊        | 20/112 [02:22<10:53,  7.11s/it] 19%|█▉        | 21/112 [02:29<11:01,  7.27s/it] 20%|█▉        | 22/112 [02:39<12:09,  8.10s/it] 21%|██        | 23/112 [02:45<11:01,  7.44s/it] 21%|██▏       | 24/112 [02:53<10:55,  7.45s/it] 22%|██▏       | 25/112 [02:59<10:28,  7.23s/it] 23%|██▎       | 26/112 [03:08<10:51,  7.58s/it] 24%|██▍       | 27/112 [03:15<10:43,  7.58s/it] 25%|██▌       | 28/112 [03:24<11:14,  8.04s/it] 26%|██▌       | 29/112 [03:32<10:49,  7.82s/it] 27%|██▋       | 30/112 [03:40<10:43,  7.85s/it]                                                 27%|██▋       | 30/112 [03:40<10:43,  7.85s/it] 28%|██▊       | 31/112 [03:46<09:58,  7.38s/it] 29%|██▊       | 32/112 [03:53<09:44,  7.31s/it] 29%|██▉       | 33/112 [04:00<09:17,  7.05s/it] 30%|███       | 34/112 [04:08<09:42,  7.47s/it] 31%|███▏      | 35/112 [04:13<08:45,  6.83s/it] 32%|███▏      | 36/112 [04:22<09:12,  7.27s/it] 33%|███▎      | 37/112 [04:30<09:30,  7.60s/it] 34%|███▍      | 38/112 [04:36<08:46,  7.12s/it] 35%|███▍      | 39/112 [04:41<08:00,  6.58s/it] 36%|███▌      | 40/112 [04:48<08:04,  6.73s/it]                                                 36%|███▌      | 40/112 [04:48<08:04,  6.73s/it] 37%|███▋      | 41/112 [04:55<07:44,  6.55s/it] 38%|███▊      | 42/112 [05:03<08:26,  7.23s/it] 38%|███▊      | 43/112 [05:12<08:37,  7.50s/it] 39%|███▉      | 44/112 [05:18<08:11,  7.22s/it] 40%|████      | 45/112 [05:26<08:15,  7.39s/it] 41%|████      | 46/112 [05:32<07:45,  7.05s/it] 42%|████▏     | 47/112 [05:39<07:34,  6.99s/it] 43%|████▎     | 48/112 [05:46<07:32,  7.08s/it] 44%|████▍     | 49/112 [05:52<07:06,  6.77s/it] 45%|████▍     | 50/112 [05:59<06:51,  6.64s/it]                                                 45%|████▍     | 50/112 [05:59<06:51,  6.64s/it] 46%|████▌     | 51/112 [06:04<06:25,  6.32s/it] 46%|████▋     | 52/112 [06:11<06:32,  6.54s/it] 47%|████▋     | 53/112 [06:19<06:40,  6.78s/it] 48%|████▊     | 54/112 [06:26<06:52,  7.10s/it] 49%|████▉     | 55/112 [06:32<06:18,  6.64s/it] 50%|█████     | 56/112 [06:38<06:07,  6.56s/it] 51%|█████     | 57/112 [06:47<06:40,  7.29s/it] 52%|█████▏    | 58/112 [06:56<06:51,  7.62s/it] 53%|█████▎    | 59/112 [07:04<06:47,  7.68s/it] 54%|█████▎    | 60/112 [07:09<05:59,  6.91s/it]                                                 54%|█████▎    | 60/112 [07:09<05:59,  6.91s/it] 54%|█████▍    | 61/112 [07:13<05:15,  6.18s/it] 55%|█████▌    | 62/112 [07:21<05:30,  6.60s/it] 56%|█████▋    | 63/112 [07:29<05:45,  7.06s/it] 57%|█████▋    | 64/112 [07:35<05:19,  6.65s/it] 58%|█████▊    | 65/112 [07:44<05:54,  7.54s/it] 59%|█████▉    | 66/112 [07:50<05:26,  7.11s/it] 60%|█████▉    | 67/112 [07:56<05:05,  6.79s/it] 61%|██████    | 68/112 [08:02<04:49,  6.58s/it] 62%|██████▏   | 69/112 [08:10<04:54,  6.85s/it] 62%|██████▎   | 70/112 [08:15<04:27,  6.36s/it]                                                 62%|██████▎   | 70/112 [08:15<04:27,  6.36s/it] 63%|██████▎   | 71/112 [08:21<04:16,  6.26s/it] 64%|██████▍   | 72/112 [08:27<03:59,  5.98s/it] 65%|██████▌   | 73/112 [08:33<03:53,  5.98s/it] 66%|██████▌   | 74/112 [08:43<04:39,  7.35s/it] 67%|██████▋   | 75/112 [08:48<04:04,  6.61s/it] 68%|██████▊   | 76/112 [08:56<04:14,  7.06s/it] 69%|██████▉   | 77/112 [09:02<03:52,  6.64s/it] 70%|██████▉   | 78/112 [09:08<03:37,  6.39s/it] 71%|███████   | 79/112 [09:16<03:47,  6.90s/it] 71%|███████▏  | 80/112 [09:21<03:27,  6.48s/it]                                                 71%|███████▏  | 80/112 [09:21<03:27,  6.48s/it] 72%|███████▏  | 81/112 [09:27<03:18,  6.39s/it] 73%|███████▎  | 82/112 [09:35<03:24,  6.80s/it] 74%|███████▍  | 83/112 [09:43<03:23,  7.03s/it] 75%|███████▌  | 84/112 [09:51<03:30,  7.52s/it] 76%|███████▌  | 85/112 [09:57<03:10,  7.05s/it] 77%|███████▋  | 86/112 [10:03<02:54,  6.72s/it] 78%|███████▊  | 87/112 [10:09<02:42,  6.51s/it] 79%|███████▊  | 88/112 [10:17<02:42,  6.75s/it] 79%|███████▉  | 89/112 [10:24<02:37,  6.83s/it] 80%|████████  | 90/112 [10:31<02:37,  7.16s/it]                                                 80%|████████  | 90/112 [10:31<02:37,  7.16s/it] 81%|████████▏ | 91/112 [10:39<02:32,  7.24s/it] 82%|████████▏ | 92/112 [10:43<02:08,  6.40s/it] 83%|████████▎ | 93/112 [10:50<02:05,  6.59s/it] 84%|████████▍ | 94/112 [10:58<02:02,  6.82s/it] 85%|████████▍ | 95/112 [11:04<01:51,  6.55s/it] 86%|████████▌ | 96/112 [11:11<01:47,  6.74s/it] 87%|████████▋ | 97/112 [11:16<01:34,  6.30s/it] 88%|████████▊ | 98/112 [11:24<01:33,  6.71s/it] 88%|████████▊ | 99/112 [11:30<01:26,  6.63s/it] 89%|████████▉ | 100/112 [11:36<01:14,  6.25s/it]                                                  89%|████████▉ | 100/112 [11:36<01:14,  6.25s/it][INFO|trainer.py:3512] 2024-04-07 19:44:07,243 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:44:07,243 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:44:07,243 >>   Batch size = 1

  0%|          | 0/100 [00:00<?, ?it/s][A
  2%|▏         | 2/100 [00:00<00:12,  7.72it/s][A
  3%|▎         | 3/100 [00:00<00:18,  5.12it/s][A
  4%|▍         | 4/100 [00:00<00:17,  5.63it/s][A
  5%|▌         | 5/100 [00:01<00:22,  4.22it/s][A
  6%|▌         | 6/100 [00:01<00:33,  2.79it/s][A
  7%|▋         | 7/100 [00:01<00:28,  3.30it/s][A
  8%|▊         | 8/100 [00:02<00:36,  2.49it/s][A
  9%|▉         | 9/100 [00:02<00:30,  3.01it/s][A
 10%|█         | 10/100 [00:02<00:25,  3.54it/s][A
 11%|█         | 11/100 [00:03<00:26,  3.40it/s][A
 12%|█▏        | 12/100 [00:03<00:25,  3.43it/s][A
 13%|█▎        | 13/100 [00:04<00:34,  2.50it/s][A
 14%|█▍        | 14/100 [00:04<00:31,  2.76it/s][A
 15%|█▌        | 15/100 [00:04<00:26,  3.17it/s][A
 16%|█▌        | 16/100 [00:04<00:23,  3.63it/s][A
 17%|█▋        | 17/100 [00:05<00:23,  3.55it/s][A
 18%|█▊        | 18/100 [00:05<00:24,  3.38it/s][A
 19%|█▉        | 19/100 [00:05<00:25,  3.16it/s][A
 20%|██        | 20/100 [00:06<00:26,  2.97it/s][A
 21%|██        | 21/100 [00:06<00:26,  3.01it/s][A
 22%|██▏       | 22/100 [00:06<00:27,  2.87it/s][A
 23%|██▎       | 23/100 [00:07<00:30,  2.49it/s][A
 24%|██▍       | 24/100 [00:07<00:27,  2.73it/s][A
 25%|██▌       | 25/100 [00:07<00:25,  2.94it/s][A
 26%|██▌       | 26/100 [00:08<00:23,  3.14it/s][A
 27%|██▋       | 27/100 [00:08<00:30,  2.39it/s][A
 28%|██▊       | 28/100 [00:09<00:27,  2.66it/s][A
 29%|██▉       | 29/100 [00:09<00:27,  2.55it/s][A
 30%|███       | 30/100 [00:10<00:31,  2.20it/s][A
 31%|███       | 31/100 [00:10<00:26,  2.61it/s][A
 32%|███▏      | 32/100 [00:10<00:23,  2.89it/s][A
 33%|███▎      | 33/100 [00:10<00:21,  3.08it/s][A
 34%|███▍      | 34/100 [00:11<00:27,  2.40it/s][A
 35%|███▌      | 35/100 [00:12<00:30,  2.10it/s][A
 36%|███▌      | 36/100 [00:12<00:27,  2.31it/s][A
 37%|███▋      | 37/100 [00:13<00:31,  2.03it/s][A
 38%|███▊      | 38/100 [00:13<00:24,  2.51it/s][A
 39%|███▉      | 39/100 [00:13<00:20,  3.01it/s][A
 40%|████      | 40/100 [00:13<00:17,  3.37it/s][A
 41%|████      | 41/100 [00:13<00:17,  3.40it/s][A
 42%|████▏     | 42/100 [00:14<00:21,  2.71it/s][A
 43%|████▎     | 43/100 [00:14<00:21,  2.67it/s][A
 44%|████▍     | 44/100 [00:15<00:18,  3.08it/s][A
 45%|████▌     | 45/100 [00:15<00:15,  3.55it/s][A
 46%|████▌     | 46/100 [00:15<00:14,  3.83it/s][A
 47%|████▋     | 47/100 [00:15<00:14,  3.58it/s][A
 48%|████▊     | 48/100 [00:15<00:12,  4.18it/s][A
 49%|████▉     | 49/100 [00:16<00:14,  3.54it/s][A
 50%|█████     | 50/100 [00:16<00:13,  3.60it/s][A
 51%|█████     | 51/100 [00:16<00:14,  3.40it/s][A
 52%|█████▏    | 52/100 [00:17<00:12,  3.84it/s][A
 53%|█████▎    | 53/100 [00:17<00:17,  2.73it/s][A
 54%|█████▍    | 54/100 [00:17<00:14,  3.07it/s][A
 55%|█████▌    | 55/100 [00:18<00:16,  2.66it/s][A
 56%|█████▌    | 56/100 [00:18<00:13,  3.17it/s][A
 57%|█████▋    | 57/100 [00:18<00:13,  3.21it/s][A
 58%|█████▊    | 58/100 [00:19<00:11,  3.68it/s][A
 59%|█████▉    | 59/100 [00:19<00:11,  3.44it/s][A
 60%|██████    | 60/100 [00:19<00:13,  2.98it/s][A
 61%|██████    | 61/100 [00:20<00:12,  3.17it/s][A
 62%|██████▏   | 62/100 [00:20<00:13,  2.86it/s][A
 63%|██████▎   | 63/100 [00:20<00:12,  2.93it/s][A
 64%|██████▍   | 64/100 [00:21<00:11,  3.05it/s][A
 65%|██████▌   | 65/100 [00:21<00:10,  3.21it/s][A
 66%|██████▌   | 66/100 [00:21<00:10,  3.34it/s][A
 67%|██████▋   | 67/100 [00:22<00:13,  2.49it/s][A
 68%|██████▊   | 68/100 [00:22<00:11,  2.71it/s][A
 69%|██████▉   | 69/100 [00:22<00:09,  3.20it/s][A
 70%|███████   | 70/100 [00:23<00:09,  3.01it/s][A
 71%|███████   | 71/100 [00:23<00:08,  3.39it/s][A
 72%|███████▏  | 72/100 [00:23<00:08,  3.44it/s][A
 73%|███████▎  | 73/100 [00:24<00:10,  2.50it/s][A
 74%|███████▍  | 74/100 [00:24<00:08,  2.92it/s][A
 75%|███████▌  | 75/100 [00:24<00:08,  2.91it/s][A
 76%|███████▌  | 76/100 [00:25<00:08,  2.93it/s][A
 77%|███████▋  | 77/100 [00:25<00:07,  2.89it/s][A
 78%|███████▊  | 78/100 [00:25<00:07,  2.92it/s][A
 79%|███████▉  | 79/100 [00:26<00:08,  2.59it/s][A
 80%|████████  | 80/100 [00:26<00:06,  3.07it/s][A
 81%|████████  | 81/100 [00:26<00:05,  3.52it/s][A
 82%|████████▏ | 82/100 [00:27<00:04,  3.78it/s][A
 83%|████████▎ | 83/100 [00:27<00:04,  3.65it/s][A
 84%|████████▍ | 84/100 [00:27<00:04,  3.56it/s][A
 85%|████████▌ | 85/100 [00:27<00:04,  3.33it/s][A
 86%|████████▌ | 86/100 [00:28<00:03,  3.64it/s][A
 87%|████████▋ | 87/100 [00:28<00:03,  3.33it/s][A
 88%|████████▊ | 88/100 [00:29<00:04,  2.78it/s][A
 89%|████████▉ | 89/100 [00:29<00:03,  3.14it/s][A
 90%|█████████ | 90/100 [00:29<00:02,  3.59it/s][A
 91%|█████████ | 91/100 [00:29<00:03,  2.81it/s][A
 92%|█████████▏| 92/100 [00:30<00:02,  2.96it/s][A
 93%|█████████▎| 93/100 [00:30<00:02,  2.93it/s][A
 94%|█████████▍| 94/100 [00:30<00:01,  3.39it/s][A
 95%|█████████▌| 95/100 [00:31<00:01,  2.74it/s][A
 96%|█████████▌| 96/100 [00:31<00:01,  2.94it/s][A
 97%|█████████▋| 97/100 [00:31<00:00,  3.42it/s][A
 98%|█████████▊| 98/100 [00:32<00:00,  3.44it/s][A
 99%|█████████▉| 99/100 [00:32<00:00,  3.16it/s][A
100%|██████████| 100/100 [00:32<00:00,  3.63it/s][A                                                 
                                                 [A 89%|████████▉ | 100/112 [12:09<01:14,  6.25s/it]
100%|██████████| 100/100 [00:32<00:00,  3.63it/s][A
                                                 [A[INFO|trainer.py:3203] 2024-04-07 19:44:40,364 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100
[INFO|configuration_utils.py:726] 2024-04-07 19:44:41,329 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:44:41,331 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:44:41,453 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:44:41,454 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100/special_tokens_map.json
 90%|█████████ | 101/112 [12:16<03:02, 16.64s/it] 91%|█████████ | 102/112 [12:22<02:12, 13.26s/it] 92%|█████████▏| 103/112 [12:31<01:49, 12.16s/it] 93%|█████████▎| 104/112 [12:37<01:20, 10.12s/it] 94%|█████████▍| 105/112 [12:43<01:02,  8.94s/it] 95%|█████████▍| 106/112 [12:49<00:48,  8.03s/it] 96%|█████████▌| 107/112 [12:56<00:38,  7.77s/it] 96%|█████████▋| 108/112 [13:01<00:27,  6.96s/it] 97%|█████████▋| 109/112 [13:08<00:20,  6.93s/it] 98%|█████████▊| 110/112 [13:13<00:12,  6.44s/it]                                                  98%|█████████▊| 110/112 [13:13<00:12,  6.44s/it] 99%|█████████▉| 111/112 [13:20<00:06,  6.43s/it]100%|██████████| 112/112 [13:26<00:00,  6.50s/it][INFO|trainer.py:2231] 2024-04-07 19:45:58,017 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2436] 2024-04-07 19:45:58,017 >> Loading best model from ../../saves/LLaMA2-7B/lora/orpo/checkpoint-100 (score: 1.7525826692581177).
                                                 100%|██████████| 112/112 [13:26<00:00,  6.50s/it]100%|██████████| 112/112 [13:26<00:00,  7.20s/it]
[INFO|trainer.py:3203] 2024-04-07 19:45:58,061 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/orpo
[INFO|configuration_utils.py:726] 2024-04-07 19:45:58,523 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:45:58,525 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:45:58,607 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/orpo/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:45:58,608 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/orpo/special_tokens_map.json
[INFO|trainer.py:3512] 2024-04-07 19:46:17,460 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:46:17,460 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:46:17,460 >>   Batch size = 1
  0%|          | 0/100 [00:00<?, ?it/s]  2%|▏         | 2/100 [00:00<00:12,  7.54it/s]  3%|▎         | 3/100 [00:00<00:18,  5.11it/s]  4%|▍         | 4/100 [00:00<00:17,  5.60it/s]  5%|▌         | 5/100 [00:01<00:22,  4.21it/s]  6%|▌         | 6/100 [00:01<00:33,  2.79it/s]  7%|▋         | 7/100 [00:01<00:28,  3.30it/s]  8%|▊         | 8/100 [00:02<00:37,  2.46it/s]  9%|▉         | 9/100 [00:02<00:30,  3.00it/s] 10%|█         | 10/100 [00:02<00:25,  3.52it/s] 11%|█         | 11/100 [00:03<00:26,  3.38it/s] 12%|█▏        | 12/100 [00:03<00:25,  3.40it/s] 13%|█▎        | 13/100 [00:04<00:34,  2.49it/s] 14%|█▍        | 14/100 [00:04<00:31,  2.74it/s] 15%|█▌        | 15/100 [00:04<00:26,  3.18it/s] 16%|█▌        | 16/100 [00:04<00:23,  3.64it/s] 17%|█▋        | 17/100 [00:05<00:23,  3.59it/s] 18%|█▊        | 18/100 [00:05<00:24,  3.38it/s] 19%|█▉        | 19/100 [00:05<00:25,  3.20it/s] 20%|██        | 20/100 [00:06<00:26,  2.98it/s] 21%|██        | 21/100 [00:06<00:26,  3.03it/s] 22%|██▏       | 22/100 [00:06<00:27,  2.87it/s] 23%|██▎       | 23/100 [00:07<00:31,  2.46it/s] 24%|██▍       | 24/100 [00:07<00:28,  2.66it/s] 25%|██▌       | 25/100 [00:07<00:26,  2.88it/s] 26%|██▌       | 26/100 [00:08<00:23,  3.10it/s] 27%|██▋       | 27/100 [00:08<00:30,  2.40it/s] 28%|██▊       | 28/100 [00:09<00:26,  2.68it/s] 29%|██▉       | 29/100 [00:09<00:27,  2.56it/s] 30%|███       | 30/100 [00:10<00:31,  2.19it/s] 31%|███       | 31/100 [00:10<00:26,  2.61it/s] 32%|███▏      | 32/100 [00:10<00:23,  2.89it/s] 33%|███▎      | 33/100 [00:10<00:21,  3.07it/s] 34%|███▍      | 34/100 [00:11<00:27,  2.41it/s] 35%|███▌      | 35/100 [00:12<00:31,  2.10it/s] 36%|███▌      | 36/100 [00:12<00:27,  2.31it/s] 37%|███▋      | 37/100 [00:13<00:30,  2.05it/s] 38%|███▊      | 38/100 [00:13<00:24,  2.53it/s] 39%|███▉      | 39/100 [00:13<00:20,  3.02it/s] 40%|████      | 40/100 [00:13<00:18,  3.32it/s] 41%|████      | 41/100 [00:13<00:17,  3.39it/s] 42%|████▏     | 42/100 [00:14<00:21,  2.71it/s] 43%|████▎     | 43/100 [00:14<00:21,  2.69it/s] 44%|████▍     | 44/100 [00:15<00:18,  3.09it/s] 45%|████▌     | 45/100 [00:15<00:15,  3.59it/s] 46%|████▌     | 46/100 [00:15<00:14,  3.85it/s] 47%|████▋     | 47/100 [00:15<00:14,  3.60it/s] 48%|████▊     | 48/100 [00:15<00:12,  4.21it/s] 49%|████▉     | 49/100 [00:16<00:14,  3.57it/s] 50%|█████     | 50/100 [00:16<00:13,  3.64it/s] 51%|█████     | 51/100 [00:16<00:14,  3.42it/s] 52%|█████▏    | 52/100 [00:17<00:12,  3.87it/s] 53%|█████▎    | 53/100 [00:17<00:17,  2.75it/s] 54%|█████▍    | 54/100 [00:17<00:14,  3.11it/s] 55%|█████▌    | 55/100 [00:18<00:16,  2.65it/s] 56%|█████▌    | 56/100 [00:18<00:13,  3.16it/s] 57%|█████▋    | 57/100 [00:18<00:13,  3.20it/s] 58%|█████▊    | 58/100 [00:19<00:11,  3.64it/s] 59%|█████▉    | 59/100 [00:19<00:12,  3.40it/s] 60%|██████    | 60/100 [00:19<00:13,  2.96it/s] 61%|██████    | 61/100 [00:20<00:12,  3.15it/s] 62%|██████▏   | 62/100 [00:20<00:13,  2.82it/s] 63%|██████▎   | 63/100 [00:20<00:12,  2.88it/s] 64%|██████▍   | 64/100 [00:21<00:11,  3.01it/s] 65%|██████▌   | 65/100 [00:21<00:11,  3.17it/s] 66%|██████▌   | 66/100 [00:21<00:10,  3.34it/s] 67%|██████▋   | 67/100 [00:22<00:13,  2.52it/s] 68%|██████▊   | 68/100 [00:22<00:11,  2.75it/s] 69%|██████▉   | 69/100 [00:22<00:09,  3.23it/s] 70%|███████   | 70/100 [00:23<00:09,  3.02it/s] 71%|███████   | 71/100 [00:23<00:08,  3.37it/s] 72%|███████▏  | 72/100 [00:23<00:08,  3.48it/s] 73%|███████▎  | 73/100 [00:24<00:10,  2.53it/s] 74%|███████▍  | 74/100 [00:24<00:08,  2.96it/s] 75%|███████▌  | 75/100 [00:24<00:08,  2.94it/s] 76%|███████▌  | 76/100 [00:25<00:08,  2.94it/s] 77%|███████▋  | 77/100 [00:25<00:07,  2.90it/s] 78%|███████▊  | 78/100 [00:25<00:07,  2.96it/s] 79%|███████▉  | 79/100 [00:26<00:08,  2.62it/s] 80%|████████  | 80/100 [00:26<00:06,  3.07it/s] 81%|████████  | 81/100 [00:26<00:05,  3.49it/s] 82%|████████▏ | 82/100 [00:27<00:04,  3.72it/s] 83%|████████▎ | 83/100 [00:27<00:04,  3.60it/s] 84%|████████▍ | 84/100 [00:27<00:04,  3.53it/s] 85%|████████▌ | 85/100 [00:27<00:04,  3.36it/s] 86%|████████▌ | 86/100 [00:28<00:03,  3.68it/s] 87%|████████▋ | 87/100 [00:28<00:03,  3.35it/s] 88%|████████▊ | 88/100 [00:29<00:04,  2.80it/s] 89%|████████▉ | 89/100 [00:29<00:03,  3.18it/s] 90%|█████████ | 90/100 [00:29<00:02,  3.65it/s] 91%|█████████ | 91/100 [00:29<00:03,  2.84it/s] 92%|█████████▏| 92/100 [00:30<00:02,  2.99it/s] 93%|█████████▎| 93/100 [00:30<00:02,  2.96it/s] 94%|█████████▍| 94/100 [00:30<00:01,  3.40it/s] 95%|█████████▌| 95/100 [00:31<00:01,  2.72it/s] 96%|█████████▌| 96/100 [00:31<00:01,  2.96it/s] 97%|█████████▋| 97/100 [00:31<00:00,  3.47it/s] 98%|█████████▊| 98/100 [00:32<00:00,  3.46it/s] 99%|█████████▉| 99/100 [00:32<00:00,  3.17it/s]100%|██████████| 100/100 [00:32<00:00,  3.62it/s]100%|██████████| 100/100 [00:32<00:00,  3.07it/s]
[INFO|modelcard.py:450] 2024-04-07 19:46:50,592 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
