[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file tokenizer.model from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file special_tokens_map.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file tokenizer_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer_config.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file tokenizer.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.json
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1000 examples [00:00, 56162.18 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  13%|█▎        | 126/1000 [00:00<00:00, 1192.86 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 4706.69 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:02, 331.39 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 2678.60 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:01, 586.73 examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:01, 628.35 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 3545.84 examples/s]
Converting format of dataset (num_proc=16):  56%|█████▋    | 565/1000 [00:00<00:00, 3193.98 examples/s]Converting format of dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:02, 380.76 examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 2887.10 examples/s]
Converting format of dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 3027.56 examples/s]
[INFO|configuration_utils.py:726] 2024-04-07 19:27:54,119 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:27:54,120 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3283] 2024-04-07 19:27:54,166 >> loading weights file model.safetensors from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-04-07 19:27:54,167 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-07 19:27:54,168 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:06, 143.54 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:06, 138.58 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 63/1000 [00:00<00:07, 127.97 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 502/1000 [00:00<00:00, 1163.87 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▏      | 315/1000 [00:00<00:00, 699.65 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 440/1000 [00:00<00:00, 943.07 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 938/1000 [00:00<00:00, 2081.41 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 1362.42 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 1337.93 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|██████████| 1000/1000 [00:00<00:00, 1247.58 examples/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.72s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.69s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.48s/it]
[INFO|modeling_utils.py:4024] 2024-04-07 19:28:03,502 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-04-07 19:28:03,502 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 19:28:03,757 >> loading configuration file generation_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 19:28:03,757 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-04-07 19:28:04,019 >> Using auto half precision backend
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.53s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.68s/it]
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:1969] 2024-04-07 19:28:05,864 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-04-07 19:28:05,865 >>   Num examples = 900
[INFO|trainer.py:1971] 2024-04-07 19:28:05,865 >>   Num Epochs = 3
[INFO|trainer.py:1972] 2024-04-07 19:28:05,865 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-04-07 19:28:05,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-04-07 19:28:05,865 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-04-07 19:28:05,865 >>   Total optimization steps = 336
[INFO|trainer.py:1978] 2024-04-07 19:28:05,867 >>   Number of trainable parameters = 4,194,304
  0%|          | 0/336 [00:00<?, ?it/s]  0%|          | 1/336 [00:05<31:53,  5.71s/it]  1%|          | 2/336 [00:06<16:47,  3.02s/it]  1%|          | 3/336 [00:07<11:59,  2.16s/it]  1%|          | 4/336 [00:08<09:26,  1.71s/it]  1%|▏         | 5/336 [00:10<08:01,  1.45s/it]  2%|▏         | 6/336 [00:11<07:22,  1.34s/it]  2%|▏         | 7/336 [00:12<06:42,  1.22s/it]  2%|▏         | 8/336 [00:13<06:18,  1.15s/it]  3%|▎         | 9/336 [00:14<06:00,  1.10s/it]  3%|▎         | 10/336 [00:15<05:49,  1.07s/it]                                                  3%|▎         | 10/336 [00:15<05:49,  1.07s/it]  3%|▎         | 11/336 [00:16<05:38,  1.04s/it]  4%|▎         | 12/336 [00:17<05:33,  1.03s/it]  4%|▍         | 13/336 [00:18<05:41,  1.06s/it]  4%|▍         | 14/336 [00:19<05:35,  1.04s/it]  4%|▍         | 15/336 [00:20<05:32,  1.03s/it]  5%|▍         | 16/336 [00:21<05:39,  1.06s/it]  5%|▌         | 17/336 [00:22<05:30,  1.04s/it]  5%|▌         | 18/336 [00:23<05:39,  1.07s/it]  6%|▌         | 19/336 [00:24<05:35,  1.06s/it]  6%|▌         | 20/336 [00:25<05:29,  1.04s/it]                                                  6%|▌         | 20/336 [00:25<05:29,  1.04s/it]  6%|▋         | 21/336 [00:26<05:24,  1.03s/it]  7%|▋         | 22/336 [00:27<05:23,  1.03s/it]  7%|▋         | 23/336 [00:28<05:41,  1.09s/it]  7%|▋         | 24/336 [00:29<05:33,  1.07s/it]  7%|▋         | 25/336 [00:30<05:35,  1.08s/it]  8%|▊         | 26/336 [00:32<05:39,  1.09s/it]  8%|▊         | 27/336 [00:33<05:30,  1.07s/it]  8%|▊         | 28/336 [00:34<05:24,  1.05s/it]  9%|▊         | 29/336 [00:35<05:31,  1.08s/it]  9%|▉         | 30/336 [00:36<05:22,  1.05s/it]                                                  9%|▉         | 30/336 [00:36<05:22,  1.05s/it]  9%|▉         | 31/336 [00:37<05:18,  1.04s/it] 10%|▉         | 32/336 [00:38<05:15,  1.04s/it] 10%|▉         | 33/336 [00:39<05:13,  1.04s/it] 10%|█         | 34/336 [00:40<05:22,  1.07s/it] 10%|█         | 35/336 [00:41<05:24,  1.08s/it] 11%|█         | 36/336 [00:42<05:16,  1.06s/it] 11%|█         | 37/336 [00:43<05:10,  1.04s/it] 11%|█▏        | 38/336 [00:44<05:17,  1.07s/it] 12%|█▏        | 39/336 [00:45<05:24,  1.09s/it] 12%|█▏        | 40/336 [00:46<05:25,  1.10s/it]                                                 12%|█▏        | 40/336 [00:46<05:25,  1.10s/it] 12%|█▏        | 41/336 [00:48<05:25,  1.10s/it] 12%|█▎        | 42/336 [00:49<05:26,  1.11s/it] 13%|█▎        | 43/336 [00:50<05:26,  1.11s/it] 13%|█▎        | 44/336 [00:51<05:21,  1.10s/it] 13%|█▎        | 45/336 [00:52<05:10,  1.07s/it] 14%|█▎        | 46/336 [00:53<05:05,  1.05s/it] 14%|█▍        | 47/336 [00:54<05:00,  1.04s/it] 14%|█▍        | 48/336 [00:55<04:59,  1.04s/it] 15%|█▍        | 49/336 [00:56<04:58,  1.04s/it] 15%|█▍        | 50/336 [00:57<05:03,  1.06s/it]                                                 15%|█▍        | 50/336 [00:57<05:03,  1.06s/it] 15%|█▌        | 51/336 [00:58<05:06,  1.08s/it] 15%|█▌        | 52/336 [00:59<05:19,  1.13s/it] 16%|█▌        | 53/336 [01:00<05:09,  1.09s/it] 16%|█▌        | 54/336 [01:02<05:12,  1.11s/it] 16%|█▋        | 55/336 [01:03<05:05,  1.09s/it] 17%|█▋        | 56/336 [01:04<04:57,  1.06s/it] 17%|█▋        | 57/336 [01:05<04:52,  1.05s/it] 17%|█▋        | 58/336 [01:06<04:45,  1.03s/it] 18%|█▊        | 59/336 [01:07<04:44,  1.03s/it] 18%|█▊        | 60/336 [01:08<04:45,  1.03s/it]                                                 18%|█▊        | 60/336 [01:08<04:45,  1.03s/it] 18%|█▊        | 61/336 [01:09<04:44,  1.03s/it] 18%|█▊        | 62/336 [01:10<04:51,  1.06s/it] 19%|█▉        | 63/336 [01:11<04:46,  1.05s/it] 19%|█▉        | 64/336 [01:12<04:51,  1.07s/it] 19%|█▉        | 65/336 [01:13<04:46,  1.06s/it] 20%|█▉        | 66/336 [01:14<04:44,  1.05s/it] 20%|█▉        | 67/336 [01:15<04:42,  1.05s/it] 20%|██        | 68/336 [01:16<04:46,  1.07s/it] 21%|██        | 69/336 [01:17<04:41,  1.05s/it] 21%|██        | 70/336 [01:18<04:47,  1.08s/it]                                                 21%|██        | 70/336 [01:18<04:47,  1.08s/it] 21%|██        | 71/336 [01:19<04:40,  1.06s/it] 21%|██▏       | 72/336 [01:20<04:33,  1.04s/it] 22%|██▏       | 73/336 [01:21<04:31,  1.03s/it] 22%|██▏       | 74/336 [01:22<04:29,  1.03s/it] 22%|██▏       | 75/336 [01:24<04:37,  1.06s/it] 23%|██▎       | 76/336 [01:25<04:34,  1.05s/it] 23%|██▎       | 77/336 [01:26<04:30,  1.05s/it] 23%|██▎       | 78/336 [01:27<04:28,  1.04s/it] 24%|██▎       | 79/336 [01:28<04:35,  1.07s/it] 24%|██▍       | 80/336 [01:29<04:38,  1.09s/it]                                                 24%|██▍       | 80/336 [01:29<04:38,  1.09s/it] 24%|██▍       | 81/336 [01:30<04:30,  1.06s/it] 24%|██▍       | 82/336 [01:31<04:36,  1.09s/it] 25%|██▍       | 83/336 [01:32<04:32,  1.08s/it] 25%|██▌       | 84/336 [01:33<04:26,  1.06s/it] 25%|██▌       | 85/336 [01:34<04:21,  1.04s/it] 26%|██▌       | 86/336 [01:35<04:17,  1.03s/it] 26%|██▌       | 87/336 [01:36<04:16,  1.03s/it] 26%|██▌       | 88/336 [01:37<04:33,  1.10s/it] 26%|██▋       | 89/336 [01:38<04:26,  1.08s/it] 27%|██▋       | 90/336 [01:40<04:31,  1.10s/it]                                                 27%|██▋       | 90/336 [01:40<04:31,  1.10s/it] 27%|██▋       | 91/336 [01:41<04:22,  1.07s/it] 27%|██▋       | 92/336 [01:42<04:34,  1.13s/it] 28%|██▊       | 93/336 [01:43<04:24,  1.09s/it] 28%|██▊       | 94/336 [01:44<04:18,  1.07s/it] 28%|██▊       | 95/336 [01:45<04:20,  1.08s/it] 29%|██▊       | 96/336 [01:46<04:22,  1.09s/it] 29%|██▉       | 97/336 [01:47<04:25,  1.11s/it] 29%|██▉       | 98/336 [01:48<04:24,  1.11s/it] 29%|██▉       | 99/336 [01:49<04:14,  1.08s/it] 30%|██▉       | 100/336 [01:50<04:17,  1.09s/it]                                                  30%|██▉       | 100/336 [01:50<04:17,  1.09s/it][INFO|trainer.py:3512] 2024-04-07 19:29:57,423 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:29:57,423 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:29:57,423 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:11,  2.07it/s][A
 12%|█▏        | 3/25 [00:01<00:13,  1.64it/s][A
 16%|█▌        | 4/25 [00:02<00:14,  1.46it/s][A
 20%|██        | 5/25 [00:03<00:14,  1.34it/s][A
 24%|██▍       | 6/25 [00:04<00:15,  1.25it/s][A
 28%|██▊       | 7/25 [00:05<00:14,  1.21it/s][A
 32%|███▏      | 8/25 [00:06<00:13,  1.22it/s][A
 36%|███▌      | 9/25 [00:06<00:13,  1.21it/s][A
 40%|████      | 10/25 [00:07<00:12,  1.20it/s][A
 44%|████▍     | 11/25 [00:08<00:11,  1.18it/s][A
 48%|████▊     | 12/25 [00:09<00:11,  1.16it/s][A
 52%|█████▏    | 13/25 [00:10<00:10,  1.15it/s][A
 56%|█████▌    | 14/25 [00:11<00:09,  1.16it/s][A
 60%|██████    | 15/25 [00:12<00:08,  1.15it/s][A
 64%|██████▍   | 16/25 [00:13<00:07,  1.13it/s][A
 68%|██████▊   | 17/25 [00:13<00:07,  1.14it/s][A
 72%|███████▏  | 18/25 [00:14<00:06,  1.12it/s][A
 76%|███████▌  | 19/25 [00:15<00:05,  1.15it/s][A
 80%|████████  | 20/25 [00:16<00:04,  1.12it/s][A
 84%|████████▍ | 21/25 [00:17<00:03,  1.13it/s][A
 88%|████████▊ | 22/25 [00:18<00:02,  1.13it/s][A
 92%|█████████▏| 23/25 [00:19<00:01,  1.15it/s][A
 96%|█████████▌| 24/25 [00:20<00:00,  1.16it/s][A
100%|██████████| 25/25 [00:20<00:00,  1.19it/s][A                                                 
                                               [A 30%|██▉       | 100/336 [02:12<04:17,  1.09s/it]
100%|██████████| 25/25 [00:20<00:00,  1.19it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:30:19,106 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-100
[INFO|configuration_utils.py:726] 2024-04-07 19:30:19,609 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:30:19,611 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:30:19,713 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:30:19,714 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-100/special_tokens_map.json
 30%|███       | 101/336 [02:14<30:29,  7.78s/it] 30%|███       | 102/336 [02:15<22:33,  5.78s/it] 31%|███       | 103/336 [02:16<16:53,  4.35s/it] 31%|███       | 104/336 [02:17<12:57,  3.35s/it] 31%|███▏      | 105/336 [02:18<10:29,  2.72s/it] 32%|███▏      | 106/336 [02:19<08:34,  2.24s/it] 32%|███▏      | 107/336 [02:20<07:08,  1.87s/it] 32%|███▏      | 108/336 [02:21<06:09,  1.62s/it] 32%|███▏      | 109/336 [02:22<05:28,  1.45s/it] 33%|███▎      | 110/336 [02:24<04:58,  1.32s/it]                                                  33%|███▎      | 110/336 [02:24<04:58,  1.32s/it] 33%|███▎      | 111/336 [02:25<04:36,  1.23s/it] 33%|███▎      | 112/336 [02:26<04:21,  1.17s/it] 34%|███▎      | 113/336 [02:27<04:09,  1.12s/it] 34%|███▍      | 114/336 [02:28<04:09,  1.12s/it] 34%|███▍      | 115/336 [02:29<03:58,  1.08s/it] 35%|███▍      | 116/336 [02:30<04:02,  1.10s/it] 35%|███▍      | 117/336 [02:31<03:55,  1.07s/it] 35%|███▌      | 118/336 [02:32<03:57,  1.09s/it] 35%|███▌      | 119/336 [02:33<03:50,  1.06s/it] 36%|███▌      | 120/336 [02:34<03:52,  1.08s/it]                                                  36%|███▌      | 120/336 [02:34<03:52,  1.08s/it] 36%|███▌      | 121/336 [02:35<03:47,  1.06s/it] 36%|███▋      | 122/336 [02:36<03:49,  1.07s/it] 37%|███▋      | 123/336 [02:37<03:49,  1.08s/it] 37%|███▋      | 124/336 [02:38<03:53,  1.10s/it] 37%|███▋      | 125/336 [02:39<03:48,  1.08s/it] 38%|███▊      | 126/336 [02:41<03:51,  1.10s/it] 38%|███▊      | 127/336 [02:42<03:52,  1.11s/it] 38%|███▊      | 128/336 [02:43<03:53,  1.12s/it] 38%|███▊      | 129/336 [02:44<03:47,  1.10s/it] 39%|███▊      | 130/336 [02:45<03:47,  1.10s/it]                                                  39%|███▊      | 130/336 [02:45<03:47,  1.10s/it] 39%|███▉      | 131/336 [02:46<03:40,  1.08s/it] 39%|███▉      | 132/336 [02:47<03:34,  1.05s/it] 40%|███▉      | 133/336 [02:48<03:31,  1.04s/it] 40%|███▉      | 134/336 [02:49<03:35,  1.06s/it] 40%|████      | 135/336 [02:50<03:30,  1.05s/it] 40%|████      | 136/336 [02:51<03:33,  1.07s/it] 41%|████      | 137/336 [02:52<03:27,  1.04s/it] 41%|████      | 138/336 [02:53<03:24,  1.03s/it] 41%|████▏     | 139/336 [02:54<03:22,  1.03s/it] 42%|████▏     | 140/336 [02:55<03:20,  1.02s/it]                                                  42%|████▏     | 140/336 [02:55<03:20,  1.02s/it] 42%|████▏     | 141/336 [02:56<03:18,  1.02s/it] 42%|████▏     | 142/336 [02:57<03:17,  1.02s/it] 43%|████▎     | 143/336 [02:58<03:18,  1.03s/it] 43%|████▎     | 144/336 [03:00<03:25,  1.07s/it] 43%|████▎     | 145/336 [03:01<03:20,  1.05s/it] 43%|████▎     | 146/336 [03:02<03:23,  1.07s/it] 44%|████▍     | 147/336 [03:03<03:18,  1.05s/it] 44%|████▍     | 148/336 [03:04<03:21,  1.07s/it] 44%|████▍     | 149/336 [03:05<03:24,  1.10s/it] 45%|████▍     | 150/336 [03:06<03:18,  1.07s/it]                                                  45%|████▍     | 150/336 [03:06<03:18,  1.07s/it] 45%|████▍     | 151/336 [03:07<03:17,  1.07s/it] 45%|████▌     | 152/336 [03:08<03:19,  1.08s/it] 46%|████▌     | 153/336 [03:09<03:13,  1.05s/it] 46%|████▌     | 154/336 [03:10<03:09,  1.04s/it] 46%|████▌     | 155/336 [03:11<03:14,  1.07s/it] 46%|████▋     | 156/336 [03:12<03:10,  1.06s/it] 47%|████▋     | 157/336 [03:13<03:07,  1.05s/it] 47%|████▋     | 158/336 [03:14<03:03,  1.03s/it] 47%|████▋     | 159/336 [03:16<03:08,  1.07s/it] 48%|████▊     | 160/336 [03:17<03:10,  1.08s/it]                                                  48%|████▊     | 160/336 [03:17<03:10,  1.08s/it] 48%|████▊     | 161/336 [03:18<03:07,  1.07s/it] 48%|████▊     | 162/336 [03:19<03:09,  1.09s/it] 49%|████▊     | 163/336 [03:20<03:08,  1.09s/it] 49%|████▉     | 164/336 [03:21<03:09,  1.10s/it] 49%|████▉     | 165/336 [03:22<03:04,  1.08s/it] 49%|████▉     | 166/336 [03:23<03:05,  1.09s/it] 50%|████▉     | 167/336 [03:24<03:00,  1.07s/it] 50%|█████     | 168/336 [03:25<02:56,  1.05s/it] 50%|█████     | 169/336 [03:26<02:55,  1.05s/it] 51%|█████     | 170/336 [03:27<02:52,  1.04s/it]                                                  51%|█████     | 170/336 [03:27<02:52,  1.04s/it] 51%|█████     | 171/336 [03:28<02:49,  1.03s/it] 51%|█████     | 172/336 [03:29<02:52,  1.05s/it] 51%|█████▏    | 173/336 [03:30<02:49,  1.04s/it] 52%|█████▏    | 174/336 [03:32<02:51,  1.06s/it] 52%|█████▏    | 175/336 [03:32<02:46,  1.03s/it] 52%|█████▏    | 176/336 [03:34<02:49,  1.06s/it] 53%|█████▎    | 177/336 [03:35<02:48,  1.06s/it] 53%|█████▎    | 178/336 [03:36<02:51,  1.09s/it] 53%|█████▎    | 179/336 [03:37<02:50,  1.09s/it] 54%|█████▎    | 180/336 [03:38<02:45,  1.06s/it]                                                  54%|█████▎    | 180/336 [03:38<02:45,  1.06s/it] 54%|█████▍    | 181/336 [03:39<02:42,  1.05s/it] 54%|█████▍    | 182/336 [03:40<02:46,  1.08s/it] 54%|█████▍    | 183/336 [03:41<02:42,  1.06s/it] 55%|█████▍    | 184/336 [03:42<02:44,  1.08s/it] 55%|█████▌    | 185/336 [03:43<02:39,  1.06s/it] 55%|█████▌    | 186/336 [03:44<02:42,  1.09s/it] 56%|█████▌    | 187/336 [03:45<02:38,  1.06s/it] 56%|█████▌    | 188/336 [03:46<02:34,  1.05s/it] 56%|█████▋    | 189/336 [03:47<02:33,  1.04s/it] 57%|█████▋    | 190/336 [03:49<02:35,  1.07s/it]                                                  57%|█████▋    | 190/336 [03:49<02:35,  1.07s/it] 57%|█████▋    | 191/336 [03:50<02:32,  1.05s/it] 57%|█████▋    | 192/336 [03:51<02:28,  1.03s/it] 57%|█████▋    | 193/336 [03:52<02:25,  1.02s/it] 58%|█████▊    | 194/336 [03:53<02:25,  1.02s/it] 58%|█████▊    | 195/336 [03:54<02:22,  1.01s/it] 58%|█████▊    | 196/336 [03:55<02:27,  1.06s/it] 59%|█████▊    | 197/336 [03:56<02:25,  1.05s/it] 59%|█████▉    | 198/336 [03:57<02:29,  1.08s/it] 59%|█████▉    | 199/336 [03:58<02:25,  1.07s/it] 60%|█████▉    | 200/336 [03:59<02:22,  1.04s/it]                                                  60%|█████▉    | 200/336 [03:59<02:22,  1.04s/it][INFO|trainer.py:3512] 2024-04-07 19:32:05,867 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:32:05,868 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:32:05,868 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:11,  2.05it/s][A
 12%|█▏        | 3/25 [00:01<00:13,  1.62it/s][A
 16%|█▌        | 4/25 [00:02<00:14,  1.42it/s][A
 20%|██        | 5/25 [00:03<00:15,  1.30it/s][A
 24%|██▍       | 6/25 [00:04<00:15,  1.22it/s][A
 28%|██▊       | 7/25 [00:05<00:14,  1.22it/s][A
 32%|███▏      | 8/25 [00:06<00:14,  1.20it/s][A
 36%|███▌      | 9/25 [00:07<00:14,  1.14it/s][A
 40%|████      | 10/25 [00:08<00:13,  1.09it/s][A
 44%|████▍     | 11/25 [00:09<00:12,  1.10it/s][A
 48%|████▊     | 12/25 [00:09<00:11,  1.13it/s][A
 52%|█████▏    | 13/25 [00:10<00:10,  1.15it/s][A
 56%|█████▌    | 14/25 [00:11<00:09,  1.16it/s][A
 60%|██████    | 15/25 [00:12<00:08,  1.15it/s][A
 64%|██████▍   | 16/25 [00:13<00:07,  1.13it/s][A
 68%|██████▊   | 17/25 [00:14<00:07,  1.13it/s][A
 72%|███████▏  | 18/25 [00:15<00:06,  1.12it/s][A
 76%|███████▌  | 19/25 [00:15<00:05,  1.14it/s][A
 80%|████████  | 20/25 [00:16<00:04,  1.15it/s][A
 84%|████████▍ | 21/25 [00:17<00:03,  1.14it/s][A
 88%|████████▊ | 22/25 [00:18<00:02,  1.15it/s][A
 92%|█████████▏| 23/25 [00:19<00:01,  1.12it/s][A
 96%|█████████▌| 24/25 [00:20<00:00,  1.13it/s][A
100%|██████████| 25/25 [00:21<00:00,  1.16it/s][A                                                 
                                               [A 60%|█████▉    | 200/336 [04:21<02:22,  1.04s/it]
100%|██████████| 25/25 [00:21<00:00,  1.16it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:32:27,921 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-200
[INFO|configuration_utils.py:726] 2024-04-07 19:32:28,399 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:32:28,401 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:32:28,510 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:32:28,512 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-200/special_tokens_map.json
 60%|█████▉    | 201/336 [04:23<17:40,  7.86s/it] 60%|██████    | 202/336 [04:24<13:02,  5.84s/it] 60%|██████    | 203/336 [04:25<09:42,  4.38s/it] 61%|██████    | 204/336 [04:26<07:24,  3.37s/it] 61%|██████    | 205/336 [04:27<05:49,  2.67s/it] 61%|██████▏   | 206/336 [04:28<04:42,  2.17s/it] 62%|██████▏   | 207/336 [04:29<03:56,  1.83s/it] 62%|██████▏   | 208/336 [04:30<03:22,  1.58s/it] 62%|██████▏   | 209/336 [04:31<03:03,  1.45s/it] 62%|██████▎   | 210/336 [04:32<02:46,  1.32s/it]                                                  62%|██████▎   | 210/336 [04:32<02:46,  1.32s/it] 63%|██████▎   | 211/336 [04:33<02:37,  1.26s/it] 63%|██████▎   | 212/336 [04:34<02:29,  1.20s/it] 63%|██████▎   | 213/336 [04:35<02:25,  1.18s/it] 64%|██████▎   | 214/336 [04:36<02:20,  1.15s/it] 64%|██████▍   | 215/336 [04:38<02:19,  1.15s/it] 64%|██████▍   | 216/336 [04:39<02:13,  1.12s/it] 65%|██████▍   | 217/336 [04:40<02:13,  1.12s/it] 65%|██████▍   | 218/336 [04:41<02:08,  1.09s/it] 65%|██████▌   | 219/336 [04:42<02:06,  1.08s/it] 65%|██████▌   | 220/336 [04:43<02:07,  1.10s/it]                                                  65%|██████▌   | 220/336 [04:43<02:07,  1.10s/it] 66%|██████▌   | 221/336 [04:44<02:03,  1.07s/it] 66%|██████▌   | 222/336 [04:45<02:05,  1.10s/it] 66%|██████▋   | 223/336 [04:46<02:00,  1.07s/it] 67%|██████▋   | 224/336 [04:47<01:57,  1.05s/it] 67%|██████▋   | 225/336 [04:48<01:59,  1.07s/it] 67%|██████▋   | 226/336 [04:49<01:56,  1.06s/it] 68%|██████▊   | 227/336 [04:50<01:58,  1.09s/it] 68%|██████▊   | 228/336 [04:52<01:59,  1.10s/it] 68%|██████▊   | 229/336 [04:53<01:58,  1.11s/it] 68%|██████▊   | 230/336 [04:54<01:54,  1.08s/it]                                                  68%|██████▊   | 230/336 [04:54<01:54,  1.08s/it] 69%|██████▉   | 231/336 [04:55<01:54,  1.09s/it] 69%|██████▉   | 232/336 [04:56<01:50,  1.07s/it] 69%|██████▉   | 233/336 [04:57<01:48,  1.05s/it] 70%|██████▉   | 234/336 [04:58<01:46,  1.04s/it] 70%|██████▉   | 235/336 [04:59<01:48,  1.08s/it] 70%|███████   | 236/336 [05:00<01:45,  1.06s/it] 71%|███████   | 237/336 [05:01<01:43,  1.05s/it] 71%|███████   | 238/336 [05:02<01:45,  1.07s/it] 71%|███████   | 239/336 [05:03<01:39,  1.03s/it] 71%|███████▏  | 240/336 [05:04<01:38,  1.02s/it]                                                  71%|███████▏  | 240/336 [05:04<01:38,  1.02s/it] 72%|███████▏  | 241/336 [05:05<01:39,  1.05s/it] 72%|███████▏  | 242/336 [05:06<01:40,  1.07s/it] 72%|███████▏  | 243/336 [05:07<01:37,  1.05s/it] 73%|███████▎  | 244/336 [05:08<01:35,  1.04s/it] 73%|███████▎  | 245/336 [05:10<01:37,  1.07s/it] 73%|███████▎  | 246/336 [05:11<01:37,  1.08s/it] 74%|███████▎  | 247/336 [05:12<01:35,  1.07s/it] 74%|███████▍  | 248/336 [05:13<01:32,  1.05s/it] 74%|███████▍  | 249/336 [05:14<01:29,  1.03s/it] 74%|███████▍  | 250/336 [05:15<01:30,  1.05s/it]                                                  74%|███████▍  | 250/336 [05:15<01:30,  1.05s/it] 75%|███████▍  | 251/336 [05:16<01:31,  1.08s/it] 75%|███████▌  | 252/336 [05:17<01:28,  1.06s/it] 75%|███████▌  | 253/336 [05:18<01:29,  1.08s/it] 76%|███████▌  | 254/336 [05:19<01:26,  1.05s/it] 76%|███████▌  | 255/336 [05:20<01:24,  1.04s/it] 76%|███████▌  | 256/336 [05:21<01:26,  1.08s/it] 76%|███████▋  | 257/336 [05:22<01:25,  1.09s/it] 77%|███████▋  | 258/336 [05:23<01:22,  1.06s/it] 77%|███████▋  | 259/336 [05:24<01:20,  1.05s/it] 77%|███████▋  | 260/336 [05:25<01:19,  1.04s/it]                                                  77%|███████▋  | 260/336 [05:25<01:19,  1.04s/it] 78%|███████▊  | 261/336 [05:26<01:18,  1.04s/it] 78%|███████▊  | 262/336 [05:27<01:16,  1.04s/it] 78%|███████▊  | 263/336 [05:29<01:16,  1.04s/it] 79%|███████▊  | 264/336 [05:30<01:15,  1.05s/it] 79%|███████▉  | 265/336 [05:31<01:13,  1.03s/it] 79%|███████▉  | 266/336 [05:32<01:13,  1.06s/it] 79%|███████▉  | 267/336 [05:33<01:13,  1.06s/it] 80%|███████▉  | 268/336 [05:34<01:11,  1.05s/it] 80%|████████  | 269/336 [05:35<01:10,  1.05s/it] 80%|████████  | 270/336 [05:36<01:10,  1.07s/it]                                                  80%|████████  | 270/336 [05:36<01:10,  1.07s/it] 81%|████████  | 271/336 [05:37<01:08,  1.06s/it] 81%|████████  | 272/336 [05:38<01:08,  1.07s/it] 81%|████████▏ | 273/336 [05:39<01:06,  1.05s/it] 82%|████████▏ | 274/336 [05:40<01:06,  1.07s/it] 82%|████████▏ | 275/336 [05:41<01:06,  1.09s/it] 82%|████████▏ | 276/336 [05:42<01:04,  1.07s/it] 82%|████████▏ | 277/336 [05:43<01:01,  1.04s/it] 83%|████████▎ | 278/336 [05:44<01:01,  1.07s/it] 83%|████████▎ | 279/336 [05:45<00:59,  1.05s/it] 83%|████████▎ | 280/336 [05:47<01:02,  1.11s/it]                                                  83%|████████▎ | 280/336 [05:47<01:02,  1.11s/it] 84%|████████▎ | 281/336 [05:48<00:59,  1.09s/it] 84%|████████▍ | 282/336 [05:49<00:57,  1.07s/it] 84%|████████▍ | 283/336 [05:50<00:56,  1.06s/it] 85%|████████▍ | 284/336 [05:51<00:54,  1.05s/it] 85%|████████▍ | 285/336 [05:52<00:54,  1.07s/it] 85%|████████▌ | 286/336 [05:53<00:52,  1.05s/it] 85%|████████▌ | 287/336 [05:54<00:50,  1.03s/it] 86%|████████▌ | 288/336 [05:55<00:49,  1.02s/it] 86%|████████▌ | 289/336 [05:56<00:49,  1.05s/it] 86%|████████▋ | 290/336 [05:57<00:47,  1.04s/it]                                                  86%|████████▋ | 290/336 [05:57<00:47,  1.04s/it] 87%|████████▋ | 291/336 [05:58<00:48,  1.07s/it] 87%|████████▋ | 292/336 [05:59<00:47,  1.08s/it] 87%|████████▋ | 293/336 [06:00<00:45,  1.05s/it] 88%|████████▊ | 294/336 [06:01<00:45,  1.09s/it] 88%|████████▊ | 295/336 [06:02<00:43,  1.06s/it] 88%|████████▊ | 296/336 [06:04<00:43,  1.09s/it] 88%|████████▊ | 297/336 [06:05<00:41,  1.07s/it] 89%|████████▊ | 298/336 [06:06<00:39,  1.04s/it] 89%|████████▉ | 299/336 [06:07<00:38,  1.03s/it] 89%|████████▉ | 300/336 [06:08<00:37,  1.03s/it]                                                  89%|████████▉ | 300/336 [06:08<00:37,  1.03s/it][INFO|trainer.py:3512] 2024-04-07 19:34:14,601 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:34:14,601 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:34:14,601 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|▊         | 2/25 [00:00<00:10,  2.19it/s][A
 12%|█▏        | 3/25 [00:01<00:13,  1.60it/s][A
 16%|█▌        | 4/25 [00:02<00:15,  1.40it/s][A
 20%|██        | 5/25 [00:03<00:15,  1.26it/s][A
 24%|██▍       | 6/25 [00:04<00:15,  1.23it/s][A
 28%|██▊       | 7/25 [00:05<00:14,  1.22it/s][A
 32%|███▏      | 8/25 [00:06<00:14,  1.20it/s][A
 36%|███▌      | 9/25 [00:07<00:13,  1.17it/s][A
 40%|████      | 10/25 [00:07<00:12,  1.16it/s][A
 44%|████▍     | 11/25 [00:08<00:12,  1.16it/s][A
 48%|████▊     | 12/25 [00:09<00:11,  1.15it/s][A
 52%|█████▏    | 13/25 [00:10<00:10,  1.14it/s][A
 56%|█████▌    | 14/25 [00:11<00:09,  1.15it/s][A
 60%|██████    | 15/25 [00:12<00:08,  1.14it/s][A
 64%|██████▍   | 16/25 [00:13<00:07,  1.14it/s][A
 68%|██████▊   | 17/25 [00:14<00:07,  1.11it/s][A
 72%|███████▏  | 18/25 [00:15<00:06,  1.11it/s][A
 76%|███████▌  | 19/25 [00:15<00:05,  1.12it/s][A
 80%|████████  | 20/25 [00:16<00:04,  1.14it/s][A
 84%|████████▍ | 21/25 [00:17<00:03,  1.14it/s][A
 88%|████████▊ | 22/25 [00:18<00:02,  1.13it/s][A
 92%|█████████▏| 23/25 [00:19<00:01,  1.14it/s][A
 96%|█████████▌| 24/25 [00:20<00:00,  1.16it/s][A
100%|██████████| 25/25 [00:21<00:00,  1.15it/s][A                                                 
                                               [A 89%|████████▉ | 300/336 [06:30<00:37,  1.03s/it]
100%|██████████| 25/25 [00:21<00:00,  1.15it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:34:36,585 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300
[INFO|configuration_utils.py:726] 2024-04-07 19:34:37,067 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:34:37,068 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:34:37,158 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:34:37,158 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300/special_tokens_map.json
 90%|████████▉ | 301/336 [06:31<04:33,  7.82s/it] 90%|████████▉ | 302/336 [06:32<03:16,  5.78s/it] 90%|█████████ | 303/336 [06:33<02:23,  4.35s/it] 90%|█████████ | 304/336 [06:34<01:48,  3.39s/it] 91%|█████████ | 305/336 [06:36<01:23,  2.69s/it] 91%|█████████ | 306/336 [06:37<01:05,  2.18s/it] 91%|█████████▏| 307/336 [06:38<00:54,  1.86s/it] 92%|█████████▏| 308/336 [06:39<00:45,  1.61s/it] 92%|█████████▏| 309/336 [06:40<00:39,  1.46s/it] 92%|█████████▏| 310/336 [06:41<00:34,  1.33s/it]                                                  92%|█████████▏| 310/336 [06:41<00:34,  1.33s/it] 93%|█████████▎| 311/336 [06:42<00:30,  1.24s/it] 93%|█████████▎| 312/336 [06:43<00:28,  1.20s/it] 93%|█████████▎| 313/336 [06:44<00:26,  1.14s/it] 93%|█████████▎| 314/336 [06:45<00:25,  1.14s/it] 94%|█████████▍| 315/336 [06:46<00:23,  1.10s/it] 94%|█████████▍| 316/336 [06:47<00:21,  1.07s/it] 94%|█████████▍| 317/336 [06:48<00:20,  1.05s/it] 95%|█████████▍| 318/336 [06:49<00:18,  1.05s/it] 95%|█████████▍| 319/336 [06:50<00:17,  1.03s/it] 95%|█████████▌| 320/336 [06:51<00:16,  1.03s/it]                                                  95%|█████████▌| 320/336 [06:51<00:16,  1.03s/it] 96%|█████████▌| 321/336 [06:52<00:15,  1.02s/it] 96%|█████████▌| 322/336 [06:53<00:14,  1.01s/it] 96%|█████████▌| 323/336 [06:54<00:13,  1.04s/it] 96%|█████████▋| 324/336 [06:55<00:12,  1.03s/it] 97%|█████████▋| 325/336 [06:57<00:12,  1.10s/it] 97%|█████████▋| 326/336 [06:58<00:11,  1.11s/it] 97%|█████████▋| 327/336 [06:59<00:10,  1.11s/it] 98%|█████████▊| 328/336 [07:00<00:08,  1.08s/it] 98%|█████████▊| 329/336 [07:01<00:07,  1.11s/it] 98%|█████████▊| 330/336 [07:02<00:06,  1.08s/it]                                                  98%|█████████▊| 330/336 [07:02<00:06,  1.08s/it] 99%|█████████▊| 331/336 [07:03<00:05,  1.13s/it] 99%|█████████▉| 332/336 [07:04<00:04,  1.13s/it] 99%|█████████▉| 333/336 [07:05<00:03,  1.09s/it] 99%|█████████▉| 334/336 [07:06<00:02,  1.07s/it]100%|█████████▉| 335/336 [07:07<00:01,  1.07s/it]100%|██████████| 336/336 [07:08<00:00,  1.05s/it][INFO|trainer.py:2231] 2024-04-07 19:35:15,401 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2436] 2024-04-07 19:35:15,402 >> Loading best model from ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300 (score: 0.2556951642036438).
                                                 100%|██████████| 336/336 [07:11<00:00,  1.05s/it]100%|██████████| 336/336 [07:11<00:00,  1.29s/it]
[INFO|trainer.py:3203] 2024-04-07 19:35:18,391 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft
[INFO|configuration_utils.py:726] 2024-04-07 19:35:18,882 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:35:18,884 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:35:18,974 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:35:18,975 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/special_tokens_map.json
[INFO|trainer.py:3512] 2024-04-07 19:35:33,078 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:35:33,079 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:35:33,079 >>   Batch size = 1
  0%|          | 0/25 [00:00<?, ?it/s]  8%|▊         | 2/25 [00:00<00:10,  2.28it/s] 12%|█▏        | 3/25 [00:01<00:13,  1.64it/s] 16%|█▌        | 4/25 [00:02<00:15,  1.38it/s] 20%|██        | 5/25 [00:03<00:15,  1.31it/s] 24%|██▍       | 6/25 [00:04<00:15,  1.24it/s] 28%|██▊       | 7/25 [00:05<00:15,  1.19it/s] 32%|███▏      | 8/25 [00:06<00:14,  1.17it/s] 36%|███▌      | 9/25 [00:07<00:14,  1.14it/s] 40%|████      | 10/25 [00:08<00:13,  1.12it/s] 44%|████▍     | 11/25 [00:08<00:12,  1.12it/s] 48%|████▊     | 12/25 [00:09<00:12,  1.05it/s] 52%|█████▏    | 13/25 [00:11<00:11,  1.01it/s] 56%|█████▌    | 14/25 [00:11<00:10,  1.04it/s] 60%|██████    | 15/25 [00:12<00:09,  1.04it/s] 64%|██████▍   | 16/25 [00:13<00:08,  1.05it/s] 68%|██████▊   | 17/25 [00:14<00:07,  1.07it/s] 72%|███████▏  | 18/25 [00:15<00:06,  1.06it/s] 76%|███████▌  | 19/25 [00:16<00:05,  1.07it/s] 80%|████████  | 20/25 [00:17<00:04,  1.06it/s] 84%|████████▍ | 21/25 [00:18<00:03,  1.04it/s] 88%|████████▊ | 22/25 [00:19<00:02,  1.06it/s] 92%|█████████▏| 23/25 [00:20<00:01,  1.09it/s] 96%|█████████▌| 24/25 [00:21<00:00,  1.12it/s]100%|██████████| 25/25 [00:22<00:00,  1.13it/s]100%|██████████| 25/25 [00:22<00:00,  1.13it/s]
[INFO|modelcard.py:450] 2024-04-07 19:35:55,919 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
