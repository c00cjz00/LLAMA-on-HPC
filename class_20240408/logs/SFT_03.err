[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file tokenizer.model from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.model
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file special_tokens_map.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/special_tokens_map.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file tokenizer_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer_config.json
[INFO|tokenization_utils_base.py:2084] 2024-04-07 19:27:42,162 >> loading file tokenizer.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/tokenizer.json
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1000 examples [00:00, 56162.18 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  13%|â–ˆâ–Ž        | 126/1000 [00:00<00:00, 1192.86 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 4706.69 examples/s]
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:02, 331.39 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2678.60 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:01, 586.73 examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:01, 628.35 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 3545.84 examples/s]
Converting format of dataset (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [00:00<00:00, 3193.98 examples/s]Converting format of dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:02, 380.76 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2887.10 examples/s]
Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 3027.56 examples/s]
[INFO|configuration_utils.py:726] 2024-04-07 19:27:54,119 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:27:54,120 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|modeling_utils.py:3283] 2024-04-07 19:27:54,166 >> loading weights file model.safetensors from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/model.safetensors.index.json
[INFO|modeling_utils.py:1417] 2024-04-07 19:27:54,167 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:928] 2024-04-07 19:27:54,168 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "eos_token_id": 2
}

Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:06, 143.54 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:06, 138.58 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:07, 127.97 examples/s]Running tokenizer on dataset (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 502/1000 [00:00<00:00, 1163.87 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:00<00:00, 699.65 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 440/1000 [00:00<00:00, 943.07 examples/s]Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [00:00<00:00, 2081.41 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1362.42 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1337.93 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1247.58 examples/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.72s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:06<00:06,  6.69s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:07<00:07,  7.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:08<00:00,  4.48s/it]
[INFO|modeling_utils.py:4024] 2024-04-07 19:28:03,502 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4032] 2024-04-07 19:28:03,502 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-hf.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 19:28:03,757 >> loading configuration file generation_config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 19:28:03,757 >> Generate config GenerationConfig {
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_length": 4096,
  "pad_token_id": 0,
  "temperature": 0.6,
  "top_p": 0.9
}

/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:607] 2024-04-07 19:28:04,019 >> Using auto half precision backend
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.53s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.15s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.53s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:09<00:00,  4.68s/it]
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
/home/wenning1/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)
  warnings.warn(
[INFO|trainer.py:1969] 2024-04-07 19:28:05,864 >> ***** Running training *****
[INFO|trainer.py:1970] 2024-04-07 19:28:05,865 >>   Num examples = 900
[INFO|trainer.py:1971] 2024-04-07 19:28:05,865 >>   Num Epochs = 3
[INFO|trainer.py:1972] 2024-04-07 19:28:05,865 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1975] 2024-04-07 19:28:05,865 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1976] 2024-04-07 19:28:05,865 >>   Gradient Accumulation steps = 2
[INFO|trainer.py:1977] 2024-04-07 19:28:05,865 >>   Total optimization steps = 336
[INFO|trainer.py:1978] 2024-04-07 19:28:05,867 >>   Number of trainable parameters = 4,194,304
  0%|          | 0/336 [00:00<?, ?it/s]  0%|          | 1/336 [00:05<31:53,  5.71s/it]  1%|          | 2/336 [00:06<16:47,  3.02s/it]  1%|          | 3/336 [00:07<11:59,  2.16s/it]  1%|          | 4/336 [00:08<09:26,  1.71s/it]  1%|â–         | 5/336 [00:10<08:01,  1.45s/it]  2%|â–         | 6/336 [00:11<07:22,  1.34s/it]  2%|â–         | 7/336 [00:12<06:42,  1.22s/it]  2%|â–         | 8/336 [00:13<06:18,  1.15s/it]  3%|â–Ž         | 9/336 [00:14<06:00,  1.10s/it]  3%|â–Ž         | 10/336 [00:15<05:49,  1.07s/it]                                                  3%|â–Ž         | 10/336 [00:15<05:49,  1.07s/it]  3%|â–Ž         | 11/336 [00:16<05:38,  1.04s/it]  4%|â–Ž         | 12/336 [00:17<05:33,  1.03s/it]  4%|â–         | 13/336 [00:18<05:41,  1.06s/it]  4%|â–         | 14/336 [00:19<05:35,  1.04s/it]  4%|â–         | 15/336 [00:20<05:32,  1.03s/it]  5%|â–         | 16/336 [00:21<05:39,  1.06s/it]  5%|â–Œ         | 17/336 [00:22<05:30,  1.04s/it]  5%|â–Œ         | 18/336 [00:23<05:39,  1.07s/it]  6%|â–Œ         | 19/336 [00:24<05:35,  1.06s/it]  6%|â–Œ         | 20/336 [00:25<05:29,  1.04s/it]                                                  6%|â–Œ         | 20/336 [00:25<05:29,  1.04s/it]  6%|â–‹         | 21/336 [00:26<05:24,  1.03s/it]  7%|â–‹         | 22/336 [00:27<05:23,  1.03s/it]  7%|â–‹         | 23/336 [00:28<05:41,  1.09s/it]  7%|â–‹         | 24/336 [00:29<05:33,  1.07s/it]  7%|â–‹         | 25/336 [00:30<05:35,  1.08s/it]  8%|â–Š         | 26/336 [00:32<05:39,  1.09s/it]  8%|â–Š         | 27/336 [00:33<05:30,  1.07s/it]  8%|â–Š         | 28/336 [00:34<05:24,  1.05s/it]  9%|â–Š         | 29/336 [00:35<05:31,  1.08s/it]  9%|â–‰         | 30/336 [00:36<05:22,  1.05s/it]                                                  9%|â–‰         | 30/336 [00:36<05:22,  1.05s/it]  9%|â–‰         | 31/336 [00:37<05:18,  1.04s/it] 10%|â–‰         | 32/336 [00:38<05:15,  1.04s/it] 10%|â–‰         | 33/336 [00:39<05:13,  1.04s/it] 10%|â–ˆ         | 34/336 [00:40<05:22,  1.07s/it] 10%|â–ˆ         | 35/336 [00:41<05:24,  1.08s/it] 11%|â–ˆ         | 36/336 [00:42<05:16,  1.06s/it] 11%|â–ˆ         | 37/336 [00:43<05:10,  1.04s/it] 11%|â–ˆâ–        | 38/336 [00:44<05:17,  1.07s/it] 12%|â–ˆâ–        | 39/336 [00:45<05:24,  1.09s/it] 12%|â–ˆâ–        | 40/336 [00:46<05:25,  1.10s/it]                                                 12%|â–ˆâ–        | 40/336 [00:46<05:25,  1.10s/it] 12%|â–ˆâ–        | 41/336 [00:48<05:25,  1.10s/it] 12%|â–ˆâ–Ž        | 42/336 [00:49<05:26,  1.11s/it] 13%|â–ˆâ–Ž        | 43/336 [00:50<05:26,  1.11s/it] 13%|â–ˆâ–Ž        | 44/336 [00:51<05:21,  1.10s/it] 13%|â–ˆâ–Ž        | 45/336 [00:52<05:10,  1.07s/it] 14%|â–ˆâ–Ž        | 46/336 [00:53<05:05,  1.05s/it] 14%|â–ˆâ–        | 47/336 [00:54<05:00,  1.04s/it] 14%|â–ˆâ–        | 48/336 [00:55<04:59,  1.04s/it] 15%|â–ˆâ–        | 49/336 [00:56<04:58,  1.04s/it] 15%|â–ˆâ–        | 50/336 [00:57<05:03,  1.06s/it]                                                 15%|â–ˆâ–        | 50/336 [00:57<05:03,  1.06s/it] 15%|â–ˆâ–Œ        | 51/336 [00:58<05:06,  1.08s/it] 15%|â–ˆâ–Œ        | 52/336 [00:59<05:19,  1.13s/it] 16%|â–ˆâ–Œ        | 53/336 [01:00<05:09,  1.09s/it] 16%|â–ˆâ–Œ        | 54/336 [01:02<05:12,  1.11s/it] 16%|â–ˆâ–‹        | 55/336 [01:03<05:05,  1.09s/it] 17%|â–ˆâ–‹        | 56/336 [01:04<04:57,  1.06s/it] 17%|â–ˆâ–‹        | 57/336 [01:05<04:52,  1.05s/it] 17%|â–ˆâ–‹        | 58/336 [01:06<04:45,  1.03s/it] 18%|â–ˆâ–Š        | 59/336 [01:07<04:44,  1.03s/it] 18%|â–ˆâ–Š        | 60/336 [01:08<04:45,  1.03s/it]                                                 18%|â–ˆâ–Š        | 60/336 [01:08<04:45,  1.03s/it] 18%|â–ˆâ–Š        | 61/336 [01:09<04:44,  1.03s/it] 18%|â–ˆâ–Š        | 62/336 [01:10<04:51,  1.06s/it] 19%|â–ˆâ–‰        | 63/336 [01:11<04:46,  1.05s/it] 19%|â–ˆâ–‰        | 64/336 [01:12<04:51,  1.07s/it] 19%|â–ˆâ–‰        | 65/336 [01:13<04:46,  1.06s/it] 20%|â–ˆâ–‰        | 66/336 [01:14<04:44,  1.05s/it] 20%|â–ˆâ–‰        | 67/336 [01:15<04:42,  1.05s/it] 20%|â–ˆâ–ˆ        | 68/336 [01:16<04:46,  1.07s/it] 21%|â–ˆâ–ˆ        | 69/336 [01:17<04:41,  1.05s/it] 21%|â–ˆâ–ˆ        | 70/336 [01:18<04:47,  1.08s/it]                                                 21%|â–ˆâ–ˆ        | 70/336 [01:18<04:47,  1.08s/it] 21%|â–ˆâ–ˆ        | 71/336 [01:19<04:40,  1.06s/it] 21%|â–ˆâ–ˆâ–       | 72/336 [01:20<04:33,  1.04s/it] 22%|â–ˆâ–ˆâ–       | 73/336 [01:21<04:31,  1.03s/it] 22%|â–ˆâ–ˆâ–       | 74/336 [01:22<04:29,  1.03s/it] 22%|â–ˆâ–ˆâ–       | 75/336 [01:24<04:37,  1.06s/it] 23%|â–ˆâ–ˆâ–Ž       | 76/336 [01:25<04:34,  1.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 77/336 [01:26<04:30,  1.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 78/336 [01:27<04:28,  1.04s/it] 24%|â–ˆâ–ˆâ–Ž       | 79/336 [01:28<04:35,  1.07s/it] 24%|â–ˆâ–ˆâ–       | 80/336 [01:29<04:38,  1.09s/it]                                                 24%|â–ˆâ–ˆâ–       | 80/336 [01:29<04:38,  1.09s/it] 24%|â–ˆâ–ˆâ–       | 81/336 [01:30<04:30,  1.06s/it] 24%|â–ˆâ–ˆâ–       | 82/336 [01:31<04:36,  1.09s/it] 25%|â–ˆâ–ˆâ–       | 83/336 [01:32<04:32,  1.08s/it] 25%|â–ˆâ–ˆâ–Œ       | 84/336 [01:33<04:26,  1.06s/it] 25%|â–ˆâ–ˆâ–Œ       | 85/336 [01:34<04:21,  1.04s/it] 26%|â–ˆâ–ˆâ–Œ       | 86/336 [01:35<04:17,  1.03s/it] 26%|â–ˆâ–ˆâ–Œ       | 87/336 [01:36<04:16,  1.03s/it] 26%|â–ˆâ–ˆâ–Œ       | 88/336 [01:37<04:33,  1.10s/it] 26%|â–ˆâ–ˆâ–‹       | 89/336 [01:38<04:26,  1.08s/it] 27%|â–ˆâ–ˆâ–‹       | 90/336 [01:40<04:31,  1.10s/it]                                                 27%|â–ˆâ–ˆâ–‹       | 90/336 [01:40<04:31,  1.10s/it] 27%|â–ˆâ–ˆâ–‹       | 91/336 [01:41<04:22,  1.07s/it] 27%|â–ˆâ–ˆâ–‹       | 92/336 [01:42<04:34,  1.13s/it] 28%|â–ˆâ–ˆâ–Š       | 93/336 [01:43<04:24,  1.09s/it] 28%|â–ˆâ–ˆâ–Š       | 94/336 [01:44<04:18,  1.07s/it] 28%|â–ˆâ–ˆâ–Š       | 95/336 [01:45<04:20,  1.08s/it] 29%|â–ˆâ–ˆâ–Š       | 96/336 [01:46<04:22,  1.09s/it] 29%|â–ˆâ–ˆâ–‰       | 97/336 [01:47<04:25,  1.11s/it] 29%|â–ˆâ–ˆâ–‰       | 98/336 [01:48<04:24,  1.11s/it] 29%|â–ˆâ–ˆâ–‰       | 99/336 [01:49<04:14,  1.08s/it] 30%|â–ˆâ–ˆâ–‰       | 100/336 [01:50<04:17,  1.09s/it]                                                  30%|â–ˆâ–ˆâ–‰       | 100/336 [01:50<04:17,  1.09s/it][INFO|trainer.py:3512] 2024-04-07 19:29:57,423 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:29:57,423 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:29:57,423 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:11,  2.07it/s][A
 12%|â–ˆâ–        | 3/25 [00:01<00:13,  1.64it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:02<00:14,  1.46it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:03<00:14,  1.34it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:04<00:15,  1.25it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:05<00:14,  1.21it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:06<00:13,  1.22it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:06<00:13,  1.21it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:07<00:12,  1.20it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:08<00:11,  1.18it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:09<00:11,  1.16it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:10<00:10,  1.15it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:11<00:09,  1.16it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:12<00:08,  1.15it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:13<00:07,  1.13it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:13<00:07,  1.14it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:14<00:06,  1.12it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:15<00:05,  1.15it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:16<00:04,  1.12it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:17<00:03,  1.13it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:18<00:02,  1.13it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:19<00:01,  1.15it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:20<00:00,  1.16it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:20<00:00,  1.19it/s][A                                                 
                                               [A 30%|â–ˆâ–ˆâ–‰       | 100/336 [02:12<04:17,  1.09s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:20<00:00,  1.19it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:30:19,106 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-100
[INFO|configuration_utils.py:726] 2024-04-07 19:30:19,609 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:30:19,611 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:30:19,713 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:30:19,714 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-100/special_tokens_map.json
 30%|â–ˆâ–ˆâ–ˆ       | 101/336 [02:14<30:29,  7.78s/it] 30%|â–ˆâ–ˆâ–ˆ       | 102/336 [02:15<22:33,  5.78s/it] 31%|â–ˆâ–ˆâ–ˆ       | 103/336 [02:16<16:53,  4.35s/it] 31%|â–ˆâ–ˆâ–ˆ       | 104/336 [02:17<12:57,  3.35s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 105/336 [02:18<10:29,  2.72s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 106/336 [02:19<08:34,  2.24s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 107/336 [02:20<07:08,  1.87s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 108/336 [02:21<06:09,  1.62s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 109/336 [02:22<05:28,  1.45s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 110/336 [02:24<04:58,  1.32s/it]                                                  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 110/336 [02:24<04:58,  1.32s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 111/336 [02:25<04:36,  1.23s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 112/336 [02:26<04:21,  1.17s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 113/336 [02:27<04:09,  1.12s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 114/336 [02:28<04:09,  1.12s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 115/336 [02:29<03:58,  1.08s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 116/336 [02:30<04:02,  1.10s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 117/336 [02:31<03:55,  1.07s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 118/336 [02:32<03:57,  1.09s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 119/336 [02:33<03:50,  1.06s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 120/336 [02:34<03:52,  1.08s/it]                                                  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 120/336 [02:34<03:52,  1.08s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 121/336 [02:35<03:47,  1.06s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 122/336 [02:36<03:49,  1.07s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 123/336 [02:37<03:49,  1.08s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 124/336 [02:38<03:53,  1.10s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 125/336 [02:39<03:48,  1.08s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 126/336 [02:41<03:51,  1.10s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 127/336 [02:42<03:52,  1.11s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 128/336 [02:43<03:53,  1.12s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 129/336 [02:44<03:47,  1.10s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 130/336 [02:45<03:47,  1.10s/it]                                                  39%|â–ˆâ–ˆâ–ˆâ–Š      | 130/336 [02:45<03:47,  1.10s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 131/336 [02:46<03:40,  1.08s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 132/336 [02:47<03:34,  1.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 133/336 [02:48<03:31,  1.04s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 134/336 [02:49<03:35,  1.06s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 135/336 [02:50<03:30,  1.05s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 136/336 [02:51<03:33,  1.07s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 137/336 [02:52<03:27,  1.04s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 138/336 [02:53<03:24,  1.03s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 139/336 [02:54<03:22,  1.03s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 140/336 [02:55<03:20,  1.02s/it]                                                  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 140/336 [02:55<03:20,  1.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 141/336 [02:56<03:18,  1.02s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 142/336 [02:57<03:17,  1.02s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 143/336 [02:58<03:18,  1.03s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 144/336 [03:00<03:25,  1.07s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 145/336 [03:01<03:20,  1.05s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 146/336 [03:02<03:23,  1.07s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 147/336 [03:03<03:18,  1.05s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 148/336 [03:04<03:21,  1.07s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 149/336 [03:05<03:24,  1.10s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/336 [03:06<03:18,  1.07s/it]                                                  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 150/336 [03:06<03:18,  1.07s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 151/336 [03:07<03:17,  1.07s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 152/336 [03:08<03:19,  1.08s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 153/336 [03:09<03:13,  1.05s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 154/336 [03:10<03:09,  1.04s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 155/336 [03:11<03:14,  1.07s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 156/336 [03:12<03:10,  1.06s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 157/336 [03:13<03:07,  1.05s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 158/336 [03:14<03:03,  1.03s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 159/336 [03:16<03:08,  1.07s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 160/336 [03:17<03:10,  1.08s/it]                                                  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 160/336 [03:17<03:10,  1.08s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 161/336 [03:18<03:07,  1.07s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 162/336 [03:19<03:09,  1.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 163/336 [03:20<03:08,  1.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 164/336 [03:21<03:09,  1.10s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 165/336 [03:22<03:04,  1.08s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 166/336 [03:23<03:05,  1.09s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 167/336 [03:24<03:00,  1.07s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 168/336 [03:25<02:56,  1.05s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 169/336 [03:26<02:55,  1.05s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 170/336 [03:27<02:52,  1.04s/it]                                                  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 170/336 [03:27<02:52,  1.04s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 171/336 [03:28<02:49,  1.03s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 172/336 [03:29<02:52,  1.05s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 173/336 [03:30<02:49,  1.04s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 174/336 [03:32<02:51,  1.06s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 175/336 [03:32<02:46,  1.03s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 176/336 [03:34<02:49,  1.06s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 177/336 [03:35<02:48,  1.06s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 178/336 [03:36<02:51,  1.09s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 179/336 [03:37<02:50,  1.09s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 180/336 [03:38<02:45,  1.06s/it]                                                  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 180/336 [03:38<02:45,  1.06s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 181/336 [03:39<02:42,  1.05s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 182/336 [03:40<02:46,  1.08s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 183/336 [03:41<02:42,  1.06s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 184/336 [03:42<02:44,  1.08s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 185/336 [03:43<02:39,  1.06s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 186/336 [03:44<02:42,  1.09s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 187/336 [03:45<02:38,  1.06s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 188/336 [03:46<02:34,  1.05s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 189/336 [03:47<02:33,  1.04s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 190/336 [03:49<02:35,  1.07s/it]                                                  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 190/336 [03:49<02:35,  1.07s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 191/336 [03:50<02:32,  1.05s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 192/336 [03:51<02:28,  1.03s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 193/336 [03:52<02:25,  1.02s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 194/336 [03:53<02:25,  1.02s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 195/336 [03:54<02:22,  1.01s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 196/336 [03:55<02:27,  1.06s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 197/336 [03:56<02:25,  1.05s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 198/336 [03:57<02:29,  1.08s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 199/336 [03:58<02:25,  1.07s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 200/336 [03:59<02:22,  1.04s/it]                                                  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 200/336 [03:59<02:22,  1.04s/it][INFO|trainer.py:3512] 2024-04-07 19:32:05,867 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:32:05,868 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:32:05,868 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:11,  2.05it/s][A
 12%|â–ˆâ–        | 3/25 [00:01<00:13,  1.62it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:02<00:14,  1.42it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:03<00:15,  1.30it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:04<00:15,  1.22it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:05<00:14,  1.22it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:06<00:14,  1.20it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:07<00:14,  1.14it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:08<00:13,  1.09it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:09<00:12,  1.10it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:09<00:11,  1.13it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:10<00:10,  1.15it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:11<00:09,  1.16it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:12<00:08,  1.15it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:13<00:07,  1.13it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:14<00:07,  1.13it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:15<00:06,  1.12it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:15<00:05,  1.14it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:16<00:04,  1.15it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:17<00:03,  1.14it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:18<00:02,  1.15it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:19<00:01,  1.12it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:20<00:00,  1.13it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:21<00:00,  1.16it/s][A                                                 
                                               [A 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 200/336 [04:21<02:22,  1.04s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:21<00:00,  1.16it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:32:27,921 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-200
[INFO|configuration_utils.py:726] 2024-04-07 19:32:28,399 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:32:28,401 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:32:28,510 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:32:28,512 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-200/special_tokens_map.json
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 201/336 [04:23<17:40,  7.86s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 202/336 [04:24<13:02,  5.84s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 203/336 [04:25<09:42,  4.38s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 204/336 [04:26<07:24,  3.37s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 205/336 [04:27<05:49,  2.67s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 206/336 [04:28<04:42,  2.17s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 207/336 [04:29<03:56,  1.83s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 208/336 [04:30<03:22,  1.58s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 209/336 [04:31<03:03,  1.45s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 210/336 [04:32<02:46,  1.32s/it]                                                  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 210/336 [04:32<02:46,  1.32s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 211/336 [04:33<02:37,  1.26s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 212/336 [04:34<02:29,  1.20s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 213/336 [04:35<02:25,  1.18s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 214/336 [04:36<02:20,  1.15s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 215/336 [04:38<02:19,  1.15s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 216/336 [04:39<02:13,  1.12s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 217/336 [04:40<02:13,  1.12s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 218/336 [04:41<02:08,  1.09s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 219/336 [04:42<02:06,  1.08s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 220/336 [04:43<02:07,  1.10s/it]                                                  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 220/336 [04:43<02:07,  1.10s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 221/336 [04:44<02:03,  1.07s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 222/336 [04:45<02:05,  1.10s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 223/336 [04:46<02:00,  1.07s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 224/336 [04:47<01:57,  1.05s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 225/336 [04:48<01:59,  1.07s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 226/336 [04:49<01:56,  1.06s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 227/336 [04:50<01:58,  1.09s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 228/336 [04:52<01:59,  1.10s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 229/336 [04:53<01:58,  1.11s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 230/336 [04:54<01:54,  1.08s/it]                                                  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 230/336 [04:54<01:54,  1.08s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 231/336 [04:55<01:54,  1.09s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 232/336 [04:56<01:50,  1.07s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 233/336 [04:57<01:48,  1.05s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 234/336 [04:58<01:46,  1.04s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 235/336 [04:59<01:48,  1.08s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 236/336 [05:00<01:45,  1.06s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 237/336 [05:01<01:43,  1.05s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 238/336 [05:02<01:45,  1.07s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 239/336 [05:03<01:39,  1.03s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 240/336 [05:04<01:38,  1.02s/it]                                                  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 240/336 [05:04<01:38,  1.02s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 241/336 [05:05<01:39,  1.05s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 242/336 [05:06<01:40,  1.07s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 243/336 [05:07<01:37,  1.05s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 244/336 [05:08<01:35,  1.04s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 245/336 [05:10<01:37,  1.07s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 246/336 [05:11<01:37,  1.08s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 247/336 [05:12<01:35,  1.07s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 248/336 [05:13<01:32,  1.05s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 249/336 [05:14<01:29,  1.03s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250/336 [05:15<01:30,  1.05s/it]                                                  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 250/336 [05:15<01:30,  1.05s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251/336 [05:16<01:31,  1.08s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 252/336 [05:17<01:28,  1.06s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 253/336 [05:18<01:29,  1.08s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 254/336 [05:19<01:26,  1.05s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 255/336 [05:20<01:24,  1.04s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 256/336 [05:21<01:26,  1.08s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 257/336 [05:22<01:25,  1.09s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 258/336 [05:23<01:22,  1.06s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 259/336 [05:24<01:20,  1.05s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 260/336 [05:25<01:19,  1.04s/it]                                                  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 260/336 [05:25<01:19,  1.04s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 261/336 [05:26<01:18,  1.04s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 262/336 [05:27<01:16,  1.04s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 263/336 [05:29<01:16,  1.04s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 264/336 [05:30<01:15,  1.05s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 265/336 [05:31<01:13,  1.03s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 266/336 [05:32<01:13,  1.06s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 267/336 [05:33<01:13,  1.06s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 268/336 [05:34<01:11,  1.05s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 269/336 [05:35<01:10,  1.05s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 270/336 [05:36<01:10,  1.07s/it]                                                  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 270/336 [05:36<01:10,  1.07s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 271/336 [05:37<01:08,  1.06s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 272/336 [05:38<01:08,  1.07s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 273/336 [05:39<01:06,  1.05s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 274/336 [05:40<01:06,  1.07s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 275/336 [05:41<01:06,  1.09s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 276/336 [05:42<01:04,  1.07s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 277/336 [05:43<01:01,  1.04s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 278/336 [05:44<01:01,  1.07s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 279/336 [05:45<00:59,  1.05s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 280/336 [05:47<01:02,  1.11s/it]                                                  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 280/336 [05:47<01:02,  1.11s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 281/336 [05:48<00:59,  1.09s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 282/336 [05:49<00:57,  1.07s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 283/336 [05:50<00:56,  1.06s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 284/336 [05:51<00:54,  1.05s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 285/336 [05:52<00:54,  1.07s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 286/336 [05:53<00:52,  1.05s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 287/336 [05:54<00:50,  1.03s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 288/336 [05:55<00:49,  1.02s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 289/336 [05:56<00:49,  1.05s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 290/336 [05:57<00:47,  1.04s/it]                                                  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 290/336 [05:57<00:47,  1.04s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 291/336 [05:58<00:48,  1.07s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 292/336 [05:59<00:47,  1.08s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 293/336 [06:00<00:45,  1.05s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 294/336 [06:01<00:45,  1.09s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 295/336 [06:02<00:43,  1.06s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 296/336 [06:04<00:43,  1.09s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 297/336 [06:05<00:41,  1.07s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 298/336 [06:06<00:39,  1.04s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 299/336 [06:07<00:38,  1.03s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 300/336 [06:08<00:37,  1.03s/it]                                                  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 300/336 [06:08<00:37,  1.03s/it][INFO|trainer.py:3512] 2024-04-07 19:34:14,601 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:34:14,601 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:34:14,601 >>   Batch size = 1

  0%|          | 0/25 [00:00<?, ?it/s][A
  8%|â–Š         | 2/25 [00:00<00:10,  2.19it/s][A
 12%|â–ˆâ–        | 3/25 [00:01<00:13,  1.60it/s][A
 16%|â–ˆâ–Œ        | 4/25 [00:02<00:15,  1.40it/s][A
 20%|â–ˆâ–ˆ        | 5/25 [00:03<00:15,  1.26it/s][A
 24%|â–ˆâ–ˆâ–       | 6/25 [00:04<00:15,  1.23it/s][A
 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:05<00:14,  1.22it/s][A
 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:06<00:14,  1.20it/s][A
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:07<00:13,  1.17it/s][A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:07<00:12,  1.16it/s][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:08<00:12,  1.16it/s][A
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:09<00:11,  1.15it/s][A
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:10<00:10,  1.14it/s][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:11<00:09,  1.15it/s][A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:12<00:08,  1.14it/s][A
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:13<00:07,  1.14it/s][A
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:14<00:07,  1.11it/s][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:15<00:06,  1.11it/s][A
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:15<00:05,  1.12it/s][A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:16<00:04,  1.14it/s][A
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:17<00:03,  1.14it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:18<00:02,  1.13it/s][A
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:19<00:01,  1.14it/s][A
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:20<00:00,  1.16it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:21<00:00,  1.15it/s][A                                                 
                                               [A 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 300/336 [06:30<00:37,  1.03s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:21<00:00,  1.15it/s][A
                                               [A[INFO|trainer.py:3203] 2024-04-07 19:34:36,585 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300
[INFO|configuration_utils.py:726] 2024-04-07 19:34:37,067 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:34:37,068 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:34:37,158 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:34:37,158 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300/special_tokens_map.json
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 301/336 [06:31<04:33,  7.82s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 302/336 [06:32<03:16,  5.78s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 303/336 [06:33<02:23,  4.35s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 304/336 [06:34<01:48,  3.39s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 305/336 [06:36<01:23,  2.69s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 306/336 [06:37<01:05,  2.18s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 307/336 [06:38<00:54,  1.86s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 308/336 [06:39<00:45,  1.61s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 309/336 [06:40<00:39,  1.46s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 310/336 [06:41<00:34,  1.33s/it]                                                  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 310/336 [06:41<00:34,  1.33s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 311/336 [06:42<00:30,  1.24s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 312/336 [06:43<00:28,  1.20s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 313/336 [06:44<00:26,  1.14s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 314/336 [06:45<00:25,  1.14s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 315/336 [06:46<00:23,  1.10s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 316/336 [06:47<00:21,  1.07s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 317/336 [06:48<00:20,  1.05s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 318/336 [06:49<00:18,  1.05s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 319/336 [06:50<00:17,  1.03s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 320/336 [06:51<00:16,  1.03s/it]                                                  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 320/336 [06:51<00:16,  1.03s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 321/336 [06:52<00:15,  1.02s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 322/336 [06:53<00:14,  1.01s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 323/336 [06:54<00:13,  1.04s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 324/336 [06:55<00:12,  1.03s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 325/336 [06:57<00:12,  1.10s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 326/336 [06:58<00:11,  1.11s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 327/336 [06:59<00:10,  1.11s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 328/336 [07:00<00:08,  1.08s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 329/336 [07:01<00:07,  1.11s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 330/336 [07:02<00:06,  1.08s/it]                                                  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 330/336 [07:02<00:06,  1.08s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 331/336 [07:03<00:05,  1.13s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 332/336 [07:04<00:04,  1.13s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 333/336 [07:05<00:03,  1.09s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 334/336 [07:06<00:02,  1.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 335/336 [07:07<00:01,  1.07s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [07:08<00:00,  1.05s/it][INFO|trainer.py:2231] 2024-04-07 19:35:15,401 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2436] 2024-04-07 19:35:15,402 >> Loading best model from ../../saves/LLaMA2-7B/lora/03_sft/checkpoint-300 (score: 0.2556951642036438).
                                                 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [07:11<00:00,  1.05s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [07:11<00:00,  1.29s/it]
[INFO|trainer.py:3203] 2024-04-07 19:35:18,391 >> Saving model checkpoint to ../../saves/LLaMA2-7B/lora/03_sft
[INFO|configuration_utils.py:726] 2024-04-07 19:35:18,882 >> loading configuration file config.json from cache at /home/wenning1/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/8a0442e81540efaeb1a0fe3e95477b5e0edfd423/config.json
[INFO|configuration_utils.py:789] 2024-04-07 19:35:18,884 >> Model config LlamaConfig {
  "_name_or_path": "meta-llama/Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.39.3",
  "use_cache": true,
  "vocab_size": 32000
}

[INFO|tokenization_utils_base.py:2502] 2024-04-07 19:35:18,974 >> tokenizer config file saved in ../../saves/LLaMA2-7B/lora/03_sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2511] 2024-04-07 19:35:18,975 >> Special tokens file saved in ../../saves/LLaMA2-7B/lora/03_sft/special_tokens_map.json
[INFO|trainer.py:3512] 2024-04-07 19:35:33,078 >> ***** Running Evaluation *****
[INFO|trainer.py:3514] 2024-04-07 19:35:33,079 >>   Num examples = 100
[INFO|trainer.py:3517] 2024-04-07 19:35:33,079 >>   Batch size = 1
  0%|          | 0/25 [00:00<?, ?it/s]  8%|â–Š         | 2/25 [00:00<00:10,  2.28it/s] 12%|â–ˆâ–        | 3/25 [00:01<00:13,  1.64it/s] 16%|â–ˆâ–Œ        | 4/25 [00:02<00:15,  1.38it/s] 20%|â–ˆâ–ˆ        | 5/25 [00:03<00:15,  1.31it/s] 24%|â–ˆâ–ˆâ–       | 6/25 [00:04<00:15,  1.24it/s] 28%|â–ˆâ–ˆâ–Š       | 7/25 [00:05<00:15,  1.19it/s] 32%|â–ˆâ–ˆâ–ˆâ–      | 8/25 [00:06<00:14,  1.17it/s] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 9/25 [00:07<00:14,  1.14it/s] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 10/25 [00:08<00:13,  1.12it/s] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 11/25 [00:08<00:12,  1.12it/s] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 12/25 [00:09<00:12,  1.05it/s] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 13/25 [00:11<00:11,  1.01it/s] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 14/25 [00:11<00:10,  1.04it/s] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 15/25 [00:12<00:09,  1.04it/s] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 16/25 [00:13<00:08,  1.05it/s] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 17/25 [00:14<00:07,  1.07it/s] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 18/25 [00:15<00:06,  1.06it/s] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 19/25 [00:16<00:05,  1.07it/s] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 20/25 [00:17<00:04,  1.06it/s] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 21/25 [00:18<00:03,  1.04it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 22/25 [00:19<00:02,  1.06it/s] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 23/25 [00:20<00:01,  1.09it/s] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 24/25 [00:21<00:00,  1.12it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:22<00:00,  1.13it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:22<00:00,  1.13it/s]
[INFO|modelcard.py:450] 2024-04-07 19:35:55,919 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
