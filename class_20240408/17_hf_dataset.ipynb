{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8a193-cb5e-46b5-bfdc-f716a3167ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 安裝套件\n",
    "# google 翻譯 https://medium.com/@show780106/python-google-translate-%E5%BE%9Eexcel-%E6%AA%94%E6%A1%88%E8%AE%80%E5%8F%96%E8%B3%87%E6%96%99%E4%B8%A6%E6%89%B9%E6%AC%A1%E7%BF%BB%E8%AD%AF-77d44a8066cf\n",
    "!pip install --upgrade huggingface_hub -q\n",
    "!pip install hf_transfer -q\n",
    "!pip install pandas pyarrow -q\n",
    "!pip install openpyxl -q\n",
    "!pip install googletrans==4.0.0-rc1 -q\n",
    "!pip install opencc opencc-python-reimplemented datasets -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e6a63-ace1-45ce-91e2-5bcc9242d8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 請補上 hf token\n",
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541ef4f-bcfc-42f9-92dc-00be2de4d9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 下載 TLLM/ft-balance-mixed 並儲存在  taide_dataset 資料夾 (請自行更改)\n",
    "dataset=\"TLLM/ft-balance-mixed\"\n",
    "save_dir=\"taide_dataset\"\n",
    "\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download \\\n",
    "${dataset} \\\n",
    "--local-dir=${save_dir} \\\n",
    "--resume-download \\\n",
    "--repo-type=dataset \\\n",
    "--cache-dir=./cache \\\n",
    "--local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c783fe-ab06-4d91-a88b-efd1bbf124d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## 單一檔案程式碼 ##########\n",
    "# 將train-00000-of-00001.parquet 取出資料,  製作成為單一alpaca格式檔案\n",
    "# Library\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 資料集\n",
    "input_file=\"taide_dataset/b8.3patch2/train-00000-of-00001.parquet\"\n",
    "data = pd.read_parquet(input_file, engine='pyarrow')\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 提取所需欄位並建立新的字典列表\n",
    "source_arr=[\"all\"]\n",
    "variable_dict = {}\n",
    "for source in source_arr:  \n",
    "    variable_dict[source]=[]\n",
    "    print(source)\n",
    "    \n",
    "    \n",
    "# 分割檔案暫存字典列表\n",
    "#for index, row in zip(range(4), df.iterrows()):\n",
    "for row in df.iterrows():    \n",
    "    messages=row[1]['messages']\n",
    "    #source=row[1]['source']\n",
    "    source=\"all\"\n",
    "    check_key=\"0\"\n",
    "    data_tmp={}\n",
    "    for i in range(len(messages)-2, len(messages), 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']    \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            data_tmp['instruction']=user_content\n",
    "            data_tmp['output']=assistant_content\n",
    "            check_key=\"1\"\n",
    "  \n",
    "    history=[]\n",
    "    for i in range(0, len(messages)-2, 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']        \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            history_tmp=[user_content,assistant_content]\n",
    "            history.append(history_tmp)   \n",
    "\n",
    "    if (len(history)>0):\n",
    "        data_tmp['history']=history\n",
    "\n",
    "    if (check_key==\"1\"):\n",
    "        variable_dict[source].append(data_tmp)\n",
    "\n",
    "# 內容寫進檔案\n",
    "for source in source_arr: \n",
    "    file=\"data_\"+source+\".json\"\n",
    "    with open(file, \"wt\", encoding=\"UTF-8\") as fp:    \n",
    "    #with open(file, \"a\", encoding=\"UTF-8\") as fp:\n",
    "        json.dump(variable_dict[source], fp, ensure_ascii=False, indent=4) \n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3e0a90-aac0-495a-902d-fe12ab8f74c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "######## 分割檔案程式碼 ##########\n",
    "# 將train-00000-of-00001.parquet 取出資料,  依照source來源, 製作成為以source命名之alpaca格式檔案\n",
    "\n",
    "# Library\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 資料集\n",
    "input_file=\"taide_dataset/b8.3patch2/train-00000-of-00001.parquet\"\n",
    "data = pd.read_parquet(input_file, engine='pyarrow')\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 提取所需欄位並建立新的字典列表\n",
    "source_arr=df.source.unique()\n",
    "variable_dict = {}\n",
    "for source in source_arr:  \n",
    "    variable_dict[source]=[]\n",
    "\n",
    "# 分割檔案暫存字典列表\n",
    "#for index, row in zip(range(4), df.iterrows()):\n",
    "for row in df.iterrows():    \n",
    "    messages=row[1]['messages']\n",
    "    source=row[1]['source']\n",
    "    check_key=\"0\"\n",
    "    data_tmp={}\n",
    "    for i in range(len(messages)-2, len(messages), 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']    \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            data_tmp['instruction']=user_content\n",
    "            data_tmp['output']=assistant_content\n",
    "            check_key=\"1\"\n",
    "  \n",
    "    history=[]\n",
    "    for i in range(0, len(messages)-2, 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']        \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            history_tmp=[user_content,assistant_content]\n",
    "            history.append(history_tmp)   \n",
    "\n",
    "    if (len(history)>0):\n",
    "        data_tmp['history']=history\n",
    "\n",
    "    if (check_key==\"1\"):\n",
    "        variable_dict[source].append(data_tmp)\n",
    "\n",
    "# 內容寫進檔案\n",
    "for source in source_arr: \n",
    "    file=\"data_\"+source+\".json\"\n",
    "    with open(file, \"wt\", encoding=\"UTF-8\") as fp:    \n",
    "    #with open(file, \"a\", encoding=\"UTF-8\") as fp:\n",
    "        json.dump(variable_dict[source], fp, ensure_ascii=False, indent=4) \n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f234210-0caf-4870-afa9-fdd70c4d9df1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## 翻譯 ##########\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "# 把translations存成list\n",
    "translations = []\n",
    "\n",
    "text_to_translate = \"Your English text here\"\n",
    "translation = translator.translate(text_to_translate, dest='zh-tw')\n",
    "print(f\"原文: {text_to_translate}\")\n",
    "print(f\"翻譯: {translation.text}\")\n",
    "translations.append(translation.text)\n",
    "\n",
    "text_to_translate = \"What is federated learning\"\n",
    "translation = translator.translate(text_to_translate, dest='zh-tw')\n",
    "print(f\"原文: {text_to_translate}\")\n",
    "print(f\"翻譯: {translation.text}\")\n",
    "translations.append(translation.text)\n",
    "\n",
    "translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df769c6-efb9-4d62-957f-18820b809488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 取代字元\n",
    "NAME = \"c00cjz00\"\n",
    "AUTHOR = \"國網中心\"\n",
    "\n",
    "with open(\"LLaMA-Factory/data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  dataset = json.load(f)\n",
    "\n",
    "for sample in dataset:\n",
    "  sample[\"output\"] = sample[\"output\"].replace(\"NAME\", NAME).replace(\"AUTHOR\", AUTHOR)\n",
    "\n",
    "with open(\"./identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb507d2c-f5b1-4644-96f4-e7884a1b501c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# google 翻譯\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "# JSON資料集\n",
    "import pandas as pd\n",
    "import json\n",
    "input_file=\"./identity.json\"\n",
    "data = pd.read_json ( input_file )\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "extracted_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    translation = translator.translate(row['instruction'], dest='zh-tw')\n",
    "    instruction=translation.text\n",
    "    translation = translator.translate(row['output'], dest='zh-tw')\n",
    "    output=translation.text    \n",
    "    extracted_data = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\":  row['input'],\n",
    "        \"output\": output,\n",
    "        \"system\":  \"You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.\"                \n",
    "    }\n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcebb5-baae-427c-99d1-f1b32b7fd390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# opencc\n",
    "import json\n",
    "import opencc\n",
    "from datasets import load_dataset\n",
    "# s2t: 簡體到正體, s2twp:簡體到台灣正體\n",
    "op_cc=opencc.OpenCC('s2twp')\n",
    "\n",
    "# JSON資料集\n",
    "import pandas as pd\n",
    "import json\n",
    "input_file=\"./identity.json\"\n",
    "data = pd.read_json ( input_file )\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "extracted_dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    instruction = op_cc.convert(row['instruction'])\n",
    "    output = op_cc.convert(row['output']) \n",
    "    extracted_data = {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\":  row['input'],\n",
    "        \"output\": output,\n",
    "        \"system\":  \"You are a helpful AI assistant built by NCHC. The user you are helping speaks Traditional Chinese and comes from Taiwan.\"                \n",
    "    }\n",
    "    extracted_dataset.append(extracted_data)\n",
    "\n",
    "# 內容寫進檔案\n",
    "with open(\"data.json\", \"wt\", encoding=\"UTF-8\") as fp:\n",
    "    json.dump(extracted_dataset, fp, ensure_ascii=False, indent=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a729e63-4c7c-415c-b4d4-9cf9b1b526bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sha1sum data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e3aec-ab1c-4b2c-b0e4-985b6d8cf513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp LLaMA-Factory/data/identity.json 15a-SFT_identity.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e84561-3932-4abc-a7cb-156959d1d7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c1018-6495-4f9e-8a97-dbb1ecdae29c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Image_S-work-data_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel",
   "language": "python",
   "name": "s-work-data_c00cjz00_cuda11.8_pytorch_2.1.2-cuda11.8-cudnn8-devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
