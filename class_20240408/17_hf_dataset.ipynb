{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8a193-cb5e-46b5-bfdc-f716a3167ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 安裝套件\n",
    "!pip install --upgrade huggingface_hub -q\n",
    "!pip install hf_transfer -q\n",
    "!pip install pandas pyarrow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e6a63-ace1-45ce-91e2-5bcc9242d8d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 請補上 hf token\n",
    "!huggingface-cli login --token hf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4541ef4f-bcfc-42f9-92dc-00be2de4d9b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 下載 TLLM/ft-balance-mixed 並儲存在  taide_dataset 資料夾 (請自行更改)\n",
    "dataset=\"TLLM/ft-balance-mixed\"\n",
    "save_dir=\"taide_dataset\"\n",
    "\n",
    "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download \\\n",
    "${dataset} \\\n",
    "--local-dir=${save_dir} \\\n",
    "--resume-download \\\n",
    "--repo-type=dataset \\\n",
    "--cache-dir=./cache \\\n",
    "--local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20c783fe-ab06-4d91-a88b-efd1bbf124d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "data_all.json\n"
     ]
    }
   ],
   "source": [
    "######## 單一檔案程式碼 ##########\n",
    "# 將train-00000-of-00001.parquet 取出資料,  製作成為單一alpaca格式檔案\n",
    "# Library\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 資料集\n",
    "input_file=\"taide_dataset/b8.3patch2/train-00000-of-00001.parquet\"\n",
    "data = pd.read_parquet(input_file, engine='pyarrow')\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 提取所需欄位並建立新的字典列表\n",
    "source_arr=[\"all\"]\n",
    "variable_dict = {}\n",
    "for source in source_arr:  \n",
    "    variable_dict[source]=[]\n",
    "    print(source)\n",
    "    \n",
    "    \n",
    "# 分割檔案暫存字典列表\n",
    "#for index, row in zip(range(4), df.iterrows()):\n",
    "for row in df.iterrows():    \n",
    "    messages=row[1]['messages']\n",
    "    #source=row[1]['source']\n",
    "    source=\"all\"\n",
    "    check_key=\"0\"\n",
    "    data_tmp={}\n",
    "    for i in range(len(messages)-2, len(messages), 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']    \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            data_tmp['instruction']=user_content\n",
    "            data_tmp['output']=assistant_content\n",
    "            check_key=\"1\"\n",
    "  \n",
    "    history=[]\n",
    "    for i in range(0, len(messages)-2, 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']        \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            history_tmp=[user_content,assistant_content]\n",
    "            history.append(history_tmp)   \n",
    "\n",
    "    if (len(history)>0):\n",
    "        data_tmp['history']=history\n",
    "\n",
    "    if (check_key==\"1\"):\n",
    "        variable_dict[source].append(data_tmp)\n",
    "\n",
    "# 內容寫進檔案\n",
    "for source in source_arr: \n",
    "    file=\"data_\"+source+\".json\"\n",
    "    with open(file, \"wt\", encoding=\"UTF-8\") as fp:    \n",
    "    #with open(file, \"a\", encoding=\"UTF-8\") as fp:\n",
    "        json.dump(variable_dict[source], fp, ensure_ascii=False, indent=4) \n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3e0a90-aac0-495a-902d-fe12ab8f74c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_capybara.json\n",
      "data_taide_wizardlm.json\n",
      "data_ultrachat-MixtralTranslation.json\n",
      "data_ultrachat-CreationAndWriting.json\n",
      "data_ultrachat-tw-value.json\n",
      "data_ultrachat-GoogleTranslation.json\n",
      "data_ultrachat-Summary.json\n",
      "data_capybara-knowlogic.json\n",
      "data_dolphin-coder.json\n",
      "data_ultrachat-Extraction.json\n",
      "data_csqa-cot-zhtw-v2.json\n"
     ]
    }
   ],
   "source": [
    "######## 分割檔案程式碼 ##########\n",
    "# 將train-00000-of-00001.parquet 取出資料,  依照source來源, 製作成為以source命名之alpaca格式檔案\n",
    "\n",
    "# Library\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# 資料集\n",
    "input_file=\"taide_dataset/b8.3patch2/train-00000-of-00001.parquet\"\n",
    "data = pd.read_parquet(input_file, engine='pyarrow')\n",
    "df = pd.DataFrame(data) # 轉成 DataFrame\n",
    "\n",
    "# 提取所需欄位並建立新的字典列表\n",
    "source_arr=df.source.unique()\n",
    "variable_dict = {}\n",
    "for source in source_arr:  \n",
    "    variable_dict[source]=[]\n",
    "\n",
    "# 分割檔案暫存字典列表\n",
    "#for index, row in zip(range(4), df.iterrows()):\n",
    "for row in df.iterrows():    \n",
    "    messages=row[1]['messages']\n",
    "    source=row[1]['source']\n",
    "    check_key=\"0\"\n",
    "    data_tmp={}\n",
    "    for i in range(len(messages)-2, len(messages), 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']    \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            data_tmp['instruction']=user_content\n",
    "            data_tmp['output']=assistant_content\n",
    "            check_key=\"1\"\n",
    "  \n",
    "    history=[]\n",
    "    for i in range(0, len(messages)-2, 2):\n",
    "        user=messages[i]['role']\n",
    "        user_content=messages[i]['content']\n",
    "        assistant=messages[i+1]['role']\n",
    "        assistant_content=messages[i+1]['content']        \n",
    "        if ((user==\"user\") and (assistant==\"assistant\")):\n",
    "            history_tmp=[user_content,assistant_content]\n",
    "            history.append(history_tmp)   \n",
    "\n",
    "    if (len(history)>0):\n",
    "        data_tmp['history']=history\n",
    "\n",
    "    if (check_key==\"1\"):\n",
    "        variable_dict[source].append(data_tmp)\n",
    "\n",
    "# 內容寫進檔案\n",
    "for source in source_arr: \n",
    "    file=\"data_\"+source+\".json\"\n",
    "    with open(file, \"wt\", encoding=\"UTF-8\") as fp:    \n",
    "    #with open(file, \"a\", encoding=\"UTF-8\") as fp:\n",
    "        json.dump(variable_dict[source], fp, ensure_ascii=False, indent=4) \n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82942c-c45b-4ed9-a9a8-8dc5d609d352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
